
@article{xamboMusicInformationRetrieval2019,
	title = {Music {Information} {Retrieval} in {Live} {Coding}: {A} {Theoretical} {Framework}},
	volume = {42},
	number = {4},
	journal = {Computer Music Journal},
	author = {Xambó, Anna and Lerch, Alexander and Freeman, Jason},
	year = {2019},
	keywords = {On paper},
	pages = {9--25},
	annote = {ISBN: 0148-9267},
	annote = {ISBN: 0148-9267},
}

@article{serafinVirtualRealityMusical2016,
	title = {Virtual {Reality} {Musical} {Instruments}: {State} of the {Art}, {Design} {Principles}, and {Future} {Directions}},
	volume = {40},
	number = {3},
	journal = {Computer Music Journal},
	author = {Serafin, Stefania and Erkut, Cumhur and Kojs, Juraj and Nilsson, Niels C and Nordahl, Rolf},
	year = {2016},
	keywords = {On paper},
	pages = {22--40},
	annote = {ISBN: 0148-9267},
	annote = {ISBN: 0148-9267},
}

@misc{radfordLanguageModelsAre2019,
	title = {Language {Models} {Are} {Unsupervised} {Multitask} {Learners}},
	journal = {OpenAI Blog},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	year = {2019},
	file = {Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:/Users/eleanorrow/Zotero/storage/MR2SAIP2/Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:application/pdf},
}

@article{shneidermanCreativitySupportTools2007,
	title = {Creativity {Support} {Tools}: {Accelerating} {Discovery} and {Innovation}},
	volume = {50},
	number = {12},
	journal = {Communications of the ACM},
	author = {Shneiderman, Ben},
	year = {2007},
	keywords = {On paper},
	pages = {20--32},
	annote = {ISBN: 0001-0782},
	annote = {ISBN: 0001-0782},
}

@article{richterRadiosBiocomputersInterview2018,
	title = {From {Radios} to {Biocomputers}: {An} {Interview} with {Eduardo} {Reck} {Miranda}},
	volume = {42},
	number = {1},
	journal = {Computer Music Journal},
	author = {Richter, Miriam},
	year = {2018},
	keywords = {On paper},
	pages = {10--22},
	annote = {ISBN: 0148-9267},
	annote = {ISBN: 0148-9267},
}

@misc{pachetAssistedMusicCreation2020,
	title = {Assisted {Music} {Creation} with {Flow} {Machines}: {Towards} {New} {Categories} of {New}},
	author = {Pachet, François and Roy, Pierre and Carré, Benoit},
	year = {2020},
	annote = {arXiv: 2006.09232},
	annote = {arXiv: 2006.09232},
}

@inproceedings{narayananQuantumComputingBeginners1999,
	title = {Quantum {Computing} for {Beginners}},
	volume = {3},
	isbn = {0-7803-5536-9},
	booktitle = {Proceedings of the 1999 {Congress} on {Evolutionary} {Computation}-{CEC99} ({Cat}. {No}. {99TH8406})},
	publisher = {IEEE},
	author = {Narayanan, Ajit},
	year = {1999},
	pages = {2231--2238},
}

@book{mumfordParticipativeApproachComputer1978,
	address = {USA},
	title = {Participative {Approach} to {Computer} {Systems} {Design}: {A} {Case} {Study} of the {Introduction} of a {New} {Computer} {System}},
	isbn = {978-0-470-26581-9},
	shorttitle = {Participative {Approach} to {Computer} {Systems} {Design}},
	publisher = {Halsted Press},
	author = {Mumford, Enid and Henshall, Don},
	year = {1978},
	keywords = {Participatory Design},
}

@article{minasVisualBackgroundMusic2019,
	title = {Visual {Background} {Music}: {Creativity} {Support} {Systems} with {Priming}},
	volume = {36},
	number = {1},
	journal = {Journal of Management Information Systems},
	author = {Minas, Randall K and Dennis, Alan R},
	year = {2019},
	keywords = {On paper},
	pages = {230--258},
	annote = {ISBN: 0742-1222},
	annote = {ISBN: 0742-1222},
}

@misc{payneMuseNet2019,
	title = {{MuseNet}},
	journal = {OpenAI Blog},
	author = {Payne, Christine},
	year = {2019},
}

@incollection{parracancinoTimbreNetworksApproach2013,
	title = {Timbre {Networks}: {An} {Approach} to {Composition} and {Performance} in {Computer} {Music}},
	booktitle = {Creativity and the {Agile} {Mind}: {A} {Multi}-{Disciplinary} {Study} of a {Multi}-{Faceted} {Phenomenon}},
	publisher = {De Gruyter},
	author = {Parra Cancino, Juan},
	year = {2013},
	keywords = {On paper},
}

@article{mirandaComputerMusicMeets2009,
	title = {Computer {Music} {Meets} {Unconventional} {Computing}: {Towards} {Sound} {Synthesis} with in {Vitro} {Neuronal} {Networks}},
	volume = {33},
	number = {1},
	journal = {Computer Music Journal},
	author = {Miranda, Eduardo R and Bull, Larry and Gueguen, François and Uroukov, Ivan S},
	year = {2009},
	keywords = {On paper},
	pages = {9--18},
	annote = {ISBN: 0148-9267},
	annote = {ISBN: 0148-9267},
}

@book{marksAaronMarksComplete2017,
	address = {Boca Raton},
	edition = {Third edition},
	title = {Aaron {Marks}' {Complete} {Guide} to {Game} {Audio}: {For} {Composers}, {Sound} {Designers}, {Musicians}, and {Game} {Developers}},
	isbn = {978-1-138-79538-9},
	shorttitle = {Aaron {Marks}' {Complete} {Guide} to {Game} {Audio}},
	publisher = {CRC Press, Taylor \& Francis Group},
	author = {Marks, Aaron},
	year = {2017},
	keywords = {Computer games, Programming, Recording and reproducing Digital techniques, Sound},
}

@article{lubartHowCanComputers2005,
	title = {How {Can} {Computers} {Be} {Partners} in the {Creative} {Process}: {Classification} and {Commentary} on the {Special} {Issue}},
	volume = {63},
	number = {4-5},
	journal = {International Journal of Human-Computer Studies},
	author = {Lubart, Todd},
	year = {2005},
	keywords = {On paper},
	pages = {365--369},
	annote = {ISBN: 1071-5819},
	annote = {ISBN: 1071-5819},
}

@inproceedings{marupakaConnectivityCreativitySemantic2011,
	title = {Connectivity and {Creativity} in {Semantic} {Neural} {Networks}},
	isbn = {1-4244-9637-3},
	booktitle = {The 2011 {International} {Joint} {Conference} on {Neural} {Networks}},
	publisher = {IEEE},
	author = {Marupaka, Nagendra and Minai, Ali A},
	year = {2011},
	keywords = {On paper},
	pages = {3127--3133},
}

@misc{makridakisForthcomingArtificialIntelligence2017,
	title = {The {Forthcoming} {Artificial} {Intelligence} ({AI}) {Revolution}: {Its} {Impact} on {Society} and {Firms}},
	author = {Makridakis, Spyros},
	year = {2017},
	keywords = {On paper},
	annote = {ISBN: 0016-3287},
	annote = {ISBN: 0016-3287},
}

@book{liArtificialIntelligenceUncertainty2007,
	title = {Artificial {Intelligence} with {Uncertainty}},
	isbn = {1-58488-999-3},
	publisher = {CRC press},
	author = {Li, Deyi and Du, Yi},
	year = {2007},
	keywords = {On paper},
}

@article{knillQuantumComputing2010,
	title = {Quantum {Computing}},
	volume = {463},
	number = {7280},
	journal = {Nature},
	author = {Knill, Emanuel},
	year = {2010},
	keywords = {On paper},
	pages = {441--443},
	annote = {ISBN: 1476-4687},
	annote = {ISBN: 1476-4687},
}

@incollection{indurkhyaRoleComputersCreativitySupport2016,
	title = {On the {Role} of {Computers} in {Creativity}-{Support} {Systems}},
	booktitle = {Knowledge, {Information} and {Creativity} {Support} {Systems}: {Recent} {Trends}, {Advances} and {Solutions}},
	publisher = {Springer},
	author = {Indurkhya, Bipin},
	year = {2016},
	keywords = {On paper},
	pages = {213--227},
}

@inproceedings{higuchiCreativityImprovementIdeaMarathon2012,
	title = {Creativity {Improvement} by {Idea}-{Marathon} {Training}, {Measured} by {Torrance} {Tests} of {Creative} {Thinking} ({TTCT}) and {Its} {Applications} to {Laboratories}},
	isbn = {1-4673-4564-4},
	booktitle = {2012 {Seventh} {International} {Conference} on {Knowledge}, {Information} and {Creativity} {Support} {Systems}},
	publisher = {IEEE},
	author = {Higuchi, Takeo and Miyata, Kazunori and Yuizono, Takaya},
	year = {2012},
	keywords = {On paper},
	pages = {66--72},
}

@article{gutmannNoiseContrastiveEstimationNew2010,
	title = {Noise-{Contrastive} {Estimation}: {A} {New} {Estimation} {Principle} for {Unnormalized} {Statistical} {Models}},
	volume = {9},
	issn = {15324435},
	abstract = {We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise, using the model log-density function in the regression nonlinearity. We show that this leads to a consistent (convergent) estimator of the parameters, and analyze the asymptotic variance. In particular, the method is shown to directly work for unnormalized models, i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parameter. For a tractable ICA model, we compare the method with other estimation methods that can be used to learn unnormalized models, including score matching, contrastive divergence, and maximum-likelihood where the normalization constant is estimated with importance sampling. Simulations show that noise-contrastive estimation offers the best trade-off between computational and statistical efficiency. The method is then applied to the modeling of natural images: We show that the method can successfully estimate a large-scale two-layer model and a Markov random field. Copyright 2010 by the authors.},
	journal = {Journal of Machine Learning Research},
	author = {Gutmann, Michael and Hyvärinen, Aapo},
	year = {2010},
	keywords = {Contrastive Learning, Loss Function, NCE, NCE Loss},
	pages = {297--304},
}

@article{keesingTheoriesCulture1974,
	title = {Theories of {Culture}},
	volume = {3},
	journal = {Annual review of anthropology},
	author = {Keesing, Roger M},
	year = {1974},
	keywords = {On paper},
	pages = {73--97},
	annote = {ISBN: 0084-6570},
	annote = {ISBN: 0084-6570},
}

@inproceedings{hyunButterflyModelSupporting2012,
	title = {The {Butterfly} {Model} for {Supporting} {Creative} {Problem} {Solving}},
	isbn = {1-4673-4564-4},
	booktitle = {2012 {Seventh} {International} {Conference} on {Knowledge}, {Information} and {Creativity} {Support} {Systems}},
	publisher = {IEEE},
	author = {Hyun, Jung Suk and Park, Chan Jung},
	year = {2012},
	keywords = {On paper},
	pages = {28--34},
}

@book{howatDebussyProportionMusical1983,
	title = {Debussy in {Proportion} : {A} {Musical} {Analysis}},
	isbn = {0-521-23282-1 978-0-521-23282-1 0-521-31145-4 978-0-521-31145-8},
	abstract = {This is the first paperback edition of Roy Howat's stimulating and provocative study of Debussy's unorthodox use of musical form. Throught detailed analyses of the piano pieces Reflets dans l'eau and L'isle joyeuse and the symphonic poem La mer, Dr Howat shows how the pieces are built precisely and intricately around the two ratios of Golden Section and bisection so that the music is organised in various geometrical patterns which contribute substantially to its expansive and dramatic impact. The final chapter traces evidence of whether the proportional systems were designed consciously, and if so, where and why Debussy might have learnt such techniques. The evidence includes his involvement in the Symbolist movement and in esotericism.},
	author = {Howat, Roy},
	year = {1983},
}

@article{henaffDataEfficientImageRecognition2019,
	title = {Data-{Efficient} {Image} {Recognition} with {Contrastive} {Predictive} {Coding}},
	issn = {23318422},
	abstract = {Large scale deep learning excels when labeled images are abundant, yet data-efficient learning remains a longstanding challenge. While biological vision is thought to leverage vast amounts of unlabeled data to solve classification problems with limited supervision, computer vision has so far not succeeded in this 'semi-supervised' regime. Our work tackles this challenge with Contrastive Predictive Coding, an unsupervised objective which extracts stable structure from still images. The result is a representation which, equipped with a simple linear classifier, separates ImageNet categories better than all competing methods, and surpasses the performance of a fully-supervised AlexNet model. When given a small number of labeled images (as few as 13 per class), this representation retains a strong classification performance, outperforming state-of-the-art semi-supervised methods by 10\% Top-5 accuracy and supervised methods by 20\%. Finally, we find our unsupervised representation to serve as a useful substrate for image detection on the PASCAL-VOC 2007 dataset, approaching the performance of representations trained with a fully annotated ImageNet dataset. We expect these results to open the door to pipelines that use scalable unsupervised representations as a drop-in replacement for supervised ones for real-world vision tasks where labels are scarce.},
	number = {2018},
	journal = {arXiv},
	author = {Hénaff, Olivier J. and Razavi, Ali and Doersch, Carl and Ali Eslami, S. M. and Van Den Oord, Aaron},
	year = {2019},
	keywords = {Contrastive Predictive Coding},
}

@article{fernandezAIMethodsAlgorithmic2013,
	title = {{AI} {Methods} in {Algorithmic} {Composition}: {A} {Comprehensive} {Survey}},
	volume = {48},
	journal = {Journal of Artificial Intelligence Research},
	author = {Fernández, Jose D and Vico, Francisco},
	year = {2013},
	pages = {513--582},
	annote = {ISBN: 1076-9757},
	annote = {ISBN: 1076-9757},
}

@book{ennsUnderstandingGameScoring2021,
	address = {Abingdon, Oxon ; New York},
	edition = {1.},
	series = {Perspectives on {Music} {Production}},
	title = {Understanding {Game} {Scoring}: {The} {Evolution} of {Compositional} {Practice} for and through {Gaming}},
	isbn = {978-1-00-047364-3 978-1-00-304546-5},
	shorttitle = {Understanding {Game} {Scoring}},
	abstract = {"Understanding Game Scoring explores the unique collaboration between gameplay and composition that defines musical scoring for video games. Using an array of case studies reaching back into the canon of classic video games, this book illuminates the musical flexibility, user interactivity and sound programming that make game scoring so different from traditional modes of composition. Mack Enns explores the collaboration between game scorers and players to produce the final score for a game, through case studies of the Nintendo Entertainment System sound hardware configuration, and game scores, including the canonic scores for Super Mario Bros. (1985) and The Legend of Zelda (1986). This book is recommended reading for students and researchers interested in the composition and production of video game scores, as well as those interested in ludo-musicology"–},
	publisher = {Routledge},
	author = {Enns, Mackenzie},
	year = {2021},
	keywords = {Collaboration, Composition (Music), Instruction and study, Production and direction, Video game music},
}

@article{dipaolaInformingArtificialIntelligence2018,
	title = {Informing {Artificial} {Intelligence} {Generative} {Techniques} {Using} {Cognitive} {Theories} of {Human} {Creativity}},
	volume = {145},
	journal = {Procedia computer science},
	author = {DiPaola, Steve and Gabora, Liane and McCaig, Graeme},
	year = {2018},
	keywords = {On paper},
	pages = {158--168},
	annote = {ISBN: 1877-0509},
	annote = {ISBN: 1877-0509},
}

@article{dehaasAutomaticFunctionalHarmonic2013,
	title = {Automatic {Functional} {Harmonic} {Analysis}},
	volume = {37},
	number = {4},
	journal = {Computer Music Journal},
	author = {De Haas, W Bas and Magalhães, José Pedro and Wiering, Frans and C. Veltkamp, Remco},
	year = {2013},
	keywords = {On paper},
	pages = {37--53},
	annote = {ISBN: 0148-9267},
	annote = {ISBN: 0148-9267},
}

@article{fascianiVocalControlSound2018,
	title = {Vocal {Control} of {Sound} {Synthesis} {Personalized} by {Unsupervised} {Machine} {Listening} and {Learning}},
	volume = {42},
	number = {1},
	journal = {Computer Music Journal},
	author = {Fasciani, Stefano and Wyse, Lonce},
	year = {2018},
	keywords = {On paper},
	pages = {37--59},
	annote = {ISBN: 0148-9267},
	annote = {ISBN: 0148-9267},
}

@book{donnellyMusicVideoGames2014,
	address = {New York ; London},
	series = {Routledge {Music} and {Screen} {Media} {Series}},
	title = {Music in {Video} {Games}: {Studying} {Play}},
	isbn = {978-0-415-63443-4 978-0-415-63444-1},
	shorttitle = {Music in {Video} {Games}},
	publisher = {Routledge},
	editor = {Donnelly, K. J. and Gibbons, William and Lerner, Neil William},
	year = {2014},
	keywords = {Analysis, appreciation, History and criticism, Video game music},
}

@article{dixon-woodsSynthesisingQualitativeQuantitative2005,
	title = {Synthesising {Qualitative} and {Quantitative} {Evidence}: {A} {Review} of {Possible} {Methods}},
	volume = {10},
	issn = {1355-8196},
	shorttitle = {Synthesising {Qualitative} and {Quantitative} {Evidence}},
	url = {https://doi.org/10.1177/135581960501000110},
	doi = {10.1177/135581960501000110},
	abstract = {BackgroundThe limitations of traditional forms of systematic review in making optimal use of all forms of evidence are increasingly evident, especially for policy-makers and practitioners. There is an urgent need for robust ways of incorporating qualitative evidence into systematic reviews.ObjectivesIn this paper we provide a brief overview and critique of a selection of strategies for synthesising qualitative and quantitative evidence, ranging from techniques that are largely qualitative and interpretive through to techniques that are largely quantitative and integrative.ResultsA range of methods is available for synthesising diverse forms of evidence. These include narrative summary, thematic analysis, grounded theory, meta-ethnography, meta-study, realist synthesis, Miles and Huberman's data analysis techniques, content analysis, case survey, qualitative comparative analysis and Bayesian meta-analysis. Methods vary in their strengths and weaknesses, ability to deal with qualitative and quantitative forms of evidence, and type of question for which they are most suitable.ConclusionsWe identify a number of procedural, conceptual and theoretical issues that need to be addressed in moving forward with this area, and emphasise the need for existing techniques to be evaluated and modified, rather than inventing new approaches.},
	number = {1},
	urldate = {2022-11-20},
	journal = {Journal of Health Services Research \& Policy},
	author = {Dixon-Woods, Mary and Agarwal, Shona and Jones, David and Young, Bridget and Sutton, Alex},
	month = jan,
	year = {2005},
	pages = {45--53},
	annote = {Publisher: SAGE Publications},
	annote = {Publisher: SAGE Publications},
}

@book{crabtreeDoingDesignEthnography2012,
	address = {London},
	series = {Human-{Computer} {Interaction} {Series}},
	title = {Doing {Design} {Ethnography}},
	isbn = {978-1-4471-2726-0},
	abstract = {'Doing Design Ethnography' elaborates the ethnomethodological perspective on ethnography, a distinctive approach that provides canonical 'studies of work' in and for design. It provides an extensive treatment of the approach. Ethnography is now a fundamental feature of design practice, taught in universities worldwide and practiced widely in commerce. Despite its rise to prominence a great many competing perspectives exist and there are few practical texts to support the development of competence. Doing Design Ethnography elaborates the ethnomethodological perspective on ethnography, a distinctive approach that provides canonical 'studies of work' in and for design. It provides an extensive treatment of the approach, with a particular slant on providing a pedagogical text that will support the development of competence for students, career researchers and design practitioners. It is organised around a complementary series of self-contained chapters, each of which address key features of doing the job of ethnography for purposes of system design. The book will be of broad appeal to students and practitioners in HCI, CSCW and software engineering, providing valuable insights as to how to conduct ethnography and relate it to design},
	publisher = {Springer},
	author = {Crabtree, Andrew and Rouncefield, Mary and Tolmie, Peter},
	year = {2012},
	keywords = {Data processing, Ethnology, Methodology, System design},
	file = {Crabtree et al. - 2012 - Doing Design Ethnography.pdf:/Users/eleanorrow/Zotero/storage/X99CDVKN/Crabtree et al. - 2012 - Doing Design Ethnography.pdf:application/pdf},
}

@article{brownTechniquesGenerativeMelodies2015,
	title = {Techniques for {Generative} {Melodies} {Inspired} by {Music} {Cognition}},
	volume = {39},
	number = {1},
	journal = {Computer Music Journal},
	author = {Brown, Andrew R and Gifford, Toby and Davidson, Robert},
	year = {2015},
	keywords = {On paper},
	pages = {11--26},
	annote = {ISBN: 0148-9267},
	annote = {ISBN: 0148-9267},
}

@book{coessensAgileMusicalMind2013,
	title = {The {Agile} {Musical} {Mind}: {Mapping} {The}},
	volume = {21},
	author = {Coessens, Kathleen},
	year = {2013},
	keywords = {On paper},
	annote = {ISBN: 3110295296},
	annote = {ISBN: 3110295296},
}

@article{carterUsingArtificialIntelligence2017,
	title = {Using {Artificial} {Intelligence} to {Augment} {Human} {Intelligence}},
	volume = {2},
	number = {12},
	journal = {Distill},
	author = {Carter, Shan and Nielsen, Michael},
	year = {2017},
	keywords = {On paper},
	pages = {e9},
	annote = {ISBN: 2476-0757},
	annote = {ISBN: 2476-0757},
}

@book{bryan-kinnsReImaginingCrossculturalCoCreation2020,
	title = {{ReImagining}: {Cross}-cultural {Co}-{Creation} of a {Chinese} {Traditional} {Musical} {Instrument} with {Digital} {Technologies}},
	shorttitle = {{ReImagining}},
	abstract = {There are many studies of Digital Musical Instrument (DMI) design, but there is little research on the cross-cultural co-creation of DMIs drawing on traditional musical instruments. We present a study of cross-cultural co-creation inspired by the Duxianqin-a traditional Chinese Jing ethnic minority single stringed musical instrument. We report on how we structured the co-creation with European and Chi-nese participants ranging from DMI designers to composers and performers. We discuss how we identified the 'essence' of the Duxianqin and used this to drive co-creation of three Duxianqin reimagined through digital technologies. Music was specially composed for these reimagined Duxianqin and performed in public as the culmination of the design process. We reflect on our co-creation process and how others could use such an approach to identify the essence of traditional instruments and reimagine them in the digital age.},
	author = {Bryan-Kinns, Nick and Zijin, Li},
	month = jun,
	year = {2020},
}

@article{bornComputerSoftwareMedium1997,
	title = {Computer {Software} as a {Medium}: {Textuality}, {Orality} and {Sociality} in an {Artificial} {Intelligence} {Research} {Culture}},
	journal = {Rethinking visual anthropology},
	author = {Born, Georgina},
	year = {1997},
	keywords = {On Paper},
	pages = {139--169},
}

@misc{AnnotatedDiffusionModel,
	title = {The {Annotated} {Diffusion} {Model}},
	url = {https://huggingface.co/blog/annotated-diffusion},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2022-09-30},
}

@article{adamsArtificialIntelligenceCulture1986,
	title = {Artificial {Intelligence}, {Culture}, and {Individual} {Responsibility}},
	volume = {8},
	number = {4},
	journal = {Technology in Society},
	author = {Adams, Stephen T},
	year = {1986},
	keywords = {On paper},
	pages = {251--257},
	annote = {ISBN: 0160-791X},
	annote = {ISBN: 0160-791X},
}

@misc{belSTRUCTURINGMUSICMEANS2017,
	title = {{STRUCTURING} {MUSIC} {BY} {MEANS} {OF} {AUDIO} {CLUSTERING} {AND} {GRAPH} {SEARCH} {ALGORITHMS}},
	author = {Le Bel, Frédéric},
	year = {2017},
	note = {To cite this version : HAL Id : hal-01577224 STRUCTURING MUSIC BY MEANS OF},
	keywords = {Audio, Clustering, Models, Search Algorithms},
}

@article{chenComputationalCognitiveModel2015,
	title = {A computational cognitive model of user applying creativity technique in creativity support systems},
	volume = {55},
	journal = {Procedia computer science},
	author = {Chen, Zhibin and Jia, Xiping and Xiao, Zhenghong},
	year = {2015},
	keywords = {On paper},
	pages = {818--824},
	annote = {ISBN: 1877-0509},
	annote = {ISBN: 1877-0509},
}

@article{gabrielCreativitySupportSystems2016,
	title = {Creativity support systems: {A} systematic mapping study},
	volume = {21},
	journal = {Thinking Skills and Creativity},
	author = {Gabriel, Alex and Monticolo, Davy and Camargo, Mauricio and Bourgault, Mario},
	year = {2016},
	keywords = {On paper},
	pages = {109--122},
	annote = {ISBN: 1871-1871},
	annote = {ISBN: 1871-1871},
}

@techreport{eckFirstLookMusic2002,
	title = {A {First} {Look} at {Music} {Composition} using {LSTM} {Recurrent} {Neural} {Networks}},
	institution = {Istituto Dalle Molle Di Studi Sull Intelligenza Artificiale},
	author = {Eck, Douglas and Schmidhuber, Juergen},
	year = {2002},
	keywords = {On paper},
}

@inproceedings{hunterSoundscapeParticipatoryDesign2019,
	address = {New York, NY, USA},
	series = {{AM}'19},
	title = {Soundscape: {Participatory} {Design} of an {Interface} for {Musical} {Expression}},
	isbn = {978-1-4503-7297-8},
	shorttitle = {Soundscape},
	url = {https://doi.org/10.1145/3356590.3356640},
	doi = {10.1145/3356590.3356640},
	abstract = {The design of New Musical Interfaces (NMIs) has been informed by principles and methods core to Human-computer Interaction. Largely, the field of NMIs remains focused on usability as it evaluates and designs interfaces. Both fields, however, recognize but still wrestle with the need to adopt a more humanistic approach to both design and evaluation. In this project a design process informed by participatory design principles was followed. Through the discussions and evaluations within that process, the details of musicians' needs and values emerged indicating a need to step beyond usability. Soundscape, the resulting NMI, has value as a physical embodiment of the range of needs and values of those musicians in its design as an interface to support creativity. Further, it is hoped that through demonstration to the NMI design community critical discourse around humanistic design considerations will be supported.},
	urldate = {2022-01-08},
	booktitle = {Proceedings of the 14th {International} {Audio} {Mostly} {Conference}: {A} {Journey} in {Sound}},
	publisher = {Association for Computing Machinery},
	author = {Hunter, Trevor and Worthy, Peter and Matthews, Ben and Viller, Stephen},
	month = sep,
	year = {2019},
	keywords = {Design Considerations, Human-computer Interaction, New Musical Interfaces, Participatory Design, Theory, Usability},
	pages = {292--296},
}

@inproceedings{geigerParticipatoryDesignEvaluation2008,
	title = {Towards {Participatory} {Design} and {Evaluation} of {Theremin}-based {Musical} {Interfaces}},
	abstract = {Being one of the earliest electronic instruments the basic principles of the Theremin have often been used to design new musical interfaces. We present the structured design and evaluation of a set of 3D interfaces for a virtual Theremin, the VRemin. The variants differ in the size of the interaction space, the interface complexity, and the applied IO devices. We conducted a formal evaluation based on the well-known AttrakDiff questionnaire for evaluating the hedonic and pragmatic quality of interactive products. The presented work is a first approach towards a participatory design process for musical interfaces that includes user evaluation at early design phases.},
	author = {Geiger, Christian and Reckter, H and Paschke, D and Schulz, F and Pöpel, Cornelius},
	month = jun,
	year = {2008},
	keywords = {Participatory Design},
}

@inproceedings{grahamProbesParticipation2008,
	address = {USA},
	series = {{PDC} '08},
	title = {Probes and {Participation}},
	isbn = {978-0-9818561-0-0},
	abstract = {This exploratory paper reflects on the relationship between methodological techniques and forms of user participation. Specifically our concern is to document and describe our experiences with different kinds of participation that different sorts of 'Probes' - 'Cultural Probes', 'Technology Probes' etc - elicit, encourage and provoke. Analysis of the different kinds of participation invoked by Probes - imaginative, investigative, emotional, discursive, reactive, disruptive, reflective, and playful - may prove useful as heuristic devices guiding the selection and deployment of these methodological and design tools. Whilst there are further opportunities for new forms of participation through 'Probing', new concerns, challenges and risks also emerge.},
	booktitle = {Proceedings of the {Tenth} {Anniversary} {Conference} on {Participatory} {Design} 2008},
	publisher = {Indiana University},
	author = {Graham, Connor and Rouncefield, Mark},
	month = oct,
	year = {2008},
	keywords = {design, participation, Participatory Design, probes, Probes, Theory},
	pages = {194--197},
}

@book{schulerParticipatoryDesignPrinciples1993,
	title = {Participatory {Design}: {Principles} and {Practices}},
	isbn = {978-0-8058-0951-0},
	shorttitle = {Participatory {Design}},
	abstract = {The voices in this collection are primarily those of researchers and developers concerned with bringing knowledge of technological possibilities to bear on informed and effective system design. Their efforts are distinguished from many previous writings on system development by their central and abiding reliance on direct and continuous interaction with those who are the ultimate arbiters of system adequacy; namely, those who will use the technology in their everyday lives and work. A key issue throughout is the question of who does what to whom: whose interests are at stake, who initiates action and for what reason, who defines the problem and who decides that there is one. The papers presented follow in the footsteps of a small but growing international community of scholars and practitioners of participatory systems design. Many of the original European perspectives are represented here as well as some new and distinctively American approaches. The collection is characterized by a rich and diverse set of perspectives and experiences that, despite their differences, share a distinctive spirit and direction – a more humane, creative, and effective relationship between those involved in technology's design and use, and between technology and the human activities that motivate the technology.},
	publisher = {CRC Press},
	author = {Schuler, Douglas and Namioka, Aki},
	month = mar,
	year = {1993},
	keywords = {Design / Product, Education / Home Schooling, Participatory Design, Psychology / Cognitive Psychology, Psychology / Cognitive Psychology \& Cognition, Technology \& Engineering / Industrial Health \& Safety, Theory},
}

@book{thomasComposingMusicGames2016,
	address = {Boca Raton, FL},
	title = {Composing music for games: the art, technology and business of video game scoring},
	isbn = {978-1-138-02141-9},
	shorttitle = {Composing music for games},
	publisher = {CRC Press, Taylor \& Francis Group},
	author = {Thomas, Chance},
	year = {2016},
	keywords = {Instruction and study, Video game music},
}

@book{vogtSelectingRightAnalyses2014,
	title = {Selecting the {Right} {Analyses} for {Your} {Data}: {Quantitative}, {Qualitative}, and {Mixed} {Methods}},
	isbn = {978-1-4625-1602-5},
	shorttitle = {Selecting the {Right} {Analyses} for {Your} {Data}},
	abstract = {\&quot;What are the most effective methods to code and analyze data for a particular study? This thoughtful and engaging book reviews the selection criteria for coding and analyzing any set of data–whether qualitative, quantitative, mixed, or visual. The authors systematically explain when to use verbal, numerical, graphic, or combined codes, and when to use qualitative, quantitative, graphic, or mixed-methods modes of analysis. Chapters on each topic are organized so that researchers can read them sequentially or can easily \&quot;flip and find\&quot; answers to specific questions. Nontechnical discussions of cutting-edge approaches–illustrated with real-world examples–emphasize how to choose (rather than how to implement) the various analyses. The book shows how using the right analysis methods leads to more justifiable conclusions and more persuasive presentations of research results. Useful features for teaching or self-study: *Chapter-opening preview boxes that highlight useful topics addressed. *End-of-chapter summary tables recapping the \&\#39;dos and don\&\#39;ts\&\#39; and advantages and disadvantages of each analytic technique. *Annotated suggestions for further reading and technical resources on each topic. Subject Areas/Keywords: analyses, coding, combined methods, data analysis, data collection, dissertation, graphical, interpretation, mixed methods, qualitative, quantitative, research analysis, research designs, research methods, social sciences, thesis, visual Audience: Researchers, instructors, and graduate students in a range of disciplines, including psychology, education, social work, sociology, health, and management; administrators and managers who need to make data-driven decisions\&quot;–},
	language = {en},
	publisher = {Guilford Publications},
	author = {Vogt, W. Paul and Gardner, Dianne C. and Haeffele, Lynne M. and Vogt, Elaine R.},
	month = jun,
	year = {2014},
	keywords = {Education / Research, Medical / Nursing / Research \& Theory, Psychology / Research \& Methodology, Social Science / Methodology},
	annote = {Google-Books-ID: 9911AwAAQBAJ},
	annote = {Google-Books-ID: 9911AwAAQBAJ},
}

@book{phillipsComposerGuideGame2017,
	address = {Cambridge, Massachusetts},
	edition = {First MIT Press paperback edition},
	title = {A composer's guide to game music},
	isbn = {978-0-262-53449-9},
	publisher = {The MIT Press},
	author = {Phillips, Winifred},
	year = {2017},
	keywords = {Computer game music, Handbooks and manuals, Instruction and study, Video game music, Writing and publishing},
	annote = {OCLC: ocn978679103},
	annote = {OCLC: ocn978679103},
}

@book{collinsGameSoundIntroduction2008,
	address = {Cambridge, Mass},
	title = {Game sound: an introduction to the history, theory, and practice of video game music and sound design},
	isbn = {978-0-262-53777-3 978-0-262-03378-7},
	shorttitle = {Game sound},
	language = {eng},
	publisher = {MIT Press},
	author = {Collins, Karen},
	year = {2008},
}

@article{coltonComputationalCreativityFinal2012,
	title = {Computational {Creativity}: {The} {Final} {Frontier}?},
	author = {Colton, Simon and Wiggins, Geraint A},
	year = {2012},
	keywords = {curation coefficient, On paper},
	annote = {mentions curation coefficient},
	annote = {mentions curation coefficient},
}

@misc{GoldsmithsMusicalSophistication,
	title = {The {Goldsmiths} {Musical} {Sophistication} {Index} ({Gold}-{MSI})},
	url = {https://www.gold.ac.uk/music-mind-brain/gold-msi/},
	abstract = {The Gold-MSI is a new self-report inventory for individual differences in musical sophistication. It measures the ability to engage with music in a flexible, effective and nuanced way.},
	language = {en},
	urldate = {2022-07-05},
}

@article{heinAbletonLive112021,
	title = {Ableton {Live} 11},
	volume = {74},
	issn = {0003-0139},
	url = {https://online.ucpress.edu/jams/article/74/1/214/116877/Ableton-Live-11},
	doi = {10.1525/jams.2021.74.1.214},
	abstract = {Ableton Live is a widely used digital audio workstation (DAW).1 The program’s name refers to its intended use as a performance instrument for electronic musicians, a way to literally play the studio as an instrument in real time. This practice aligns with Brian Eno’s concept of “the studio as compositional tool.”2 Ableton Live’s main user base consists of electronic dance music and hip-hop producers, and it is also becoming common in experimental and academic circles. For example, the composer Morton Subotnick uses Live on stage to loop and process the output of his modular synthesizer. At the New School in New York where I teach, Live is the default DAW among students and faculty alike. This review evaluates Live’s utility for research and teaching in music studies.Some of Live’s useful features are generic to all DAWs: visualizing and playing back audio and MIDI data; segmenting, annotating, and color-coding that data; mixing and editing; and applying audio effects. Live has some additional distinctive features and affordances. Users can quickly tempo-align different pieces of music, or different performances of the same music, which facilitates comparative listening. Users can create and display chords and scales via MIDI, which is useful for music theory teaching. Finally, users can perform audio and MIDI clips nonlinearly like an instrument, making it possible to play back and loop audio examples in classrooms and presentations in a DJ-like manner.Live’s most obvious scholarly use is in examining and illuminating the musics it was designed to produce—dance music, hip-hop, ambient, and related genres. The relatively low price and accessibility of DAWs like Live has made studio creativity accessible to a greater number of amateurs, giving rise to numerous electronic music styles, increasingly created in bedrooms rather than formal studios.3 A DAW is not a neutral intermediary between the user and the music: it guides and shapes the final product. Software interfaces do a great deal of implicit teaching, especially for novice musicians.4 The methods of electronic music production can be opaque even to expert listeners, and innumerable online tutorial videos for Live and other DAWs therefore serve a purpose beyond teaching software functionality. Adrian Renzo and Steve Collins discuss a producer who makes videos in which he painstakingly recreates dance tracks by Prodigy using Live. Such reconstructions show “the blurred lines between compositional, performance, and production processes.”5 By disaggregating the layers of sound and chains of effects in a track, tutorial videos can also make producers’ creative work more legible. Dance music production is an understudied form of musical creativity, and insight into its processes constitutes valuable research data.DAWs can be useful for the explanation of any kind of music, not just dance music. Live’s MIDI functionality is broadly useful for teaching and demonstrating music theory concepts, especially to liberal arts and popular music students who do not read notation. I have used it extensively for this purpose while teaching a course on the fundamentals of Western music at the New School’s Eugene Lang College. For example, Live includes a MIDI effect called the Scale device, which alters incoming MIDI notes to output a wide variety of scales and modes. This makes it possible to play any scale in any key using only the white notes on the keyboard. The Scale device therefore enables novice musicians to experiment with unfamiliar scales by ear, and to create music with them. Such experimentation can motivate students through the hard work of learning scales’ names and intervallic construction.Live also includes a Chord device that adds notes at specified intervals above or below incoming MIDI notes. By default, the resulting chords shift up and down in strict parallel, and do not fit into any particular key. In order to create diatonic chords, it is necessary to filter the output of the Chord device using the Scale device. The concept of parallel vs. diatonic chords is abstract and can be challenging to understand, but by activating and deactivating the Scale device, beginner students are able to hear the difference immediately. The Chord device also creates the opportunity for students to explore advanced harmonic concepts through playful exploration, including exotic chord voicings like clusters and fourths chords.Live’s Version 11, released in February 2021, has added the ability to show “key signatures” in the MIDI piano roll, highlighting the notes that belong to a given key. These are not limited to the standard diatonic scales. Figure 1 shows a screenshot of a MIDI clip set to E Lydian dominant mode. Notes within the scale are highlighted. Clicking the Scale button hides all the rows in the piano roll that are outside the scale, and clicking the Fold button shows only those rows that are occupied by notes. Live 11 has also added a MIDI Meter device that shows the names of incoming MIDI notes, their location on the piano keyboard, and chord symbols as applicable. Students enter music theory classrooms with a wide variety of prior music making experiences beyond piano; growing numbers have had their formative learning experiences on guitar, or in the MIDI piano roll. Having a “Rosetta stone” for translating different representations of theoretical concepts is invaluable for assisting such students.Live is extensible via Max for Live, an API for the Max visual programming language. An active community of Max developers creates specialized devices and utilities using this API. One noteworthy example for music educators is Samuel Halligan’s “Pop Up Piano.”6 It goes further than Ableton’s own MIDI monitor device, showing incoming MIDI notes on the grand staff. Halligan created this device to assist users of Ableton’s Push controller, but it is a useful music theory resource for users of conventional piano-style MIDI controllers as well. Students who struggle with notation can benefit from seeing and hearing notes on the grand staff in a real-time, responsive context paired with auditory reinforcement.Beyond straightforward documentation or explication of technical processes and music theory concepts, Live is also useful for illuminating recorded music’s broader cultural contexts and connections. Live’s affordances are conducive to DJ-style remixing, which is a useful method for assisting listeners in hearing similarities and drawing comparisons. DJs use remix methods to make musical arguments through critical interpretation and repurposing7—for example, by using juxtaposition and combination to point out connections between songs.8 Repeating samples over a groove focuses listeners’ attention on nuances they might miss in a single hearing, and digital effects can act as “audio highlighting,” heightening or exaggerating certain sonic features while concealing or obscuring others.The “technomusicologist” Wayne Marshall supplements an article on a rhythm he calls “American clave” with a DJ-style mix of examples created using Live.9 The rhythmic pattern, a 2+3+3+3+3+2 sequence of beats, recurs throughout the past hundred years of American popular music. Together with his textual analysis, Marshall includes a seamless mix of more than one hundred examples of the rhythm drawn from a century of ragtime, country, jazz, rock, dance, pop, and rap recordings. He expresses his hope that the mix “can tell the story more directly, eloquently, and heteroglossically” than his article alone.10 Such technomusicological practice is an example of arts-based research, discussed in greater depth below.11All DAWs are useful for visualizing and annotating audio as waveforms on a timeline, and for showing MIDI as rectangular clips. Most DAWs also support labeling and color-coding of audio and MIDI data. It is particularly helpful to be able to zoom in and out to view and edit this data at any time scale from milliseconds to hours. In notation, it is difficult to see large-scale structure or combinations of many instruments simultaneously, but in a DAW editor window, it is effortless.Figure 2 shows my own annotation of “Jóga” by Björk (1997) using Live. Sections of the audio file are labeled to show song structure, with labeled MIDI clips indicating key centers/modes and chords. “Jóga” was recorded at a metronomic tempo, as is typical for contemporary popular music. Once tempo is determined, it is not difficult to align such a song with a DAW bars-and-beats grid. Live is unusual in that it also makes it relatively easy to tempo-align music that is not metronomically steady, which opens up analysis and remixing possibilities far beyond beat-driven pop and dance styles.It is possible to timestretch audio to align it to the tempo grid using any DAW. Doing so, however, is usually a tedious exercise. Ableton has developed an unusually intuitive workflow and interface for timestretching, which they call “warping.”12 Using this functionality, it is reasonably easy to align even performances that employ extreme rubato to the grid. This is invaluable for analysis of Western classical recordings. Scores can be used to determine the correct metrical placement of notes in a performance on the bars-and-beats grid, but it is often more expedient to import a MIDI file of the piece and align the audio recording to it by ear.Once audio clips are warped correctly, then Live becomes a valuable tool for transcribing, since it is easy to loop metrically logical segments and slow them down. Live’s audio-to-MIDI conversion feature is also of value for transcription purposes after the audio has been warped to the tempo grid. Like all audio-to-MIDI algorithms, those included in Live work better on some sources than on others. They are highly effective for single-instrument recordings with clear, distinct note onsets and transparent timbres. Conversion of chords will frequently read harmonics of individual notes as distinct pitches, but these can be edited out. Live is generally less successful at extracting melodies, chords, and rhythms from densely mixed tracks or noisy sources.Live stands out among DAWs for the ease with which warped audio clips can be combined and transformed. Once recordings are warped to the grid, then numerous interesting pedagogical possibilities open up. For example, it is quite easy to align multiple performances of the same piece of music, to hear them simultaneously, and to switch back and forth rapidly between them. Users can also superimpose Live’s metronome or MIDI drum loops aligned with the tempo map. Clips will automatically play back at the main session tempo, so aligning and layering clips from different sources is largely effortless. (This is how Marshall was able to mix one hundred examples of “American clave” seamlessly.)Live users can also automatically quantize MIDI to the rhythmic groove of audio clips.13 Warped audio behaves much like MIDI on the rhythmic axis: users can quantize imperfect performances to the tempo grid, shift the metrical position of particular notes or beats, and change a clip’s overall rhythmic groove—for example, by adding swing to a recording performed in straight time, or by straightening a swung performance. This can be especially helpful when studying jazz, funk, and related musics, as it is possible to show the microtiming variations of drum and note onsets from the underlying grid. Concepts like swing, groove, and pocket need no longer be mysterious or ineffable; it is possible to demonstrate them visually and aurally.Perhaps the most analytically valuable warping function in Live is the ability to set any audio clip to be the “tempo leader,” which makes all other audio and MIDI in the session follow the clip’s organically performed tempo. Setting a tempo leader automatically generates a tempo map of the performance in the session’s Master track, and MIDI playback follows this tempo map. This has made it possible for me to create richly interactive structural and harmonic visualizations of classical works that are accessible to students who cannot read scores. Figure 3 shows my annotation of Mstislav Rostropovich’s 1995 recording of the Prelude of J. S. Bach’s Cello Suite no. 1 in G Major (BWV 1007).14 The MIDI data follows Rostropovich’s expressive timing; the small tick marks in the timeline at the bottom show each of his shifts in tempo. The main obstacle to creating grid-aligned visualizations in Live is Ableton’s presumption that users will be working in 4/4 time. While Live does support other time signatures, the dynamic grid overlay always increments bars and beats in powers of two. Grid lines therefore do not behave as expected in triple or odd meters. This does not affect the software’s functionality, but it can be aesthetically awkward.Live’s warping and tempo mapping offer new possibilities for explaining and analyzing the rhythms of Western classical music and exploring canonical classical works. Bruce Berr distinguishes between “prosaic” rhythm, the timing of events represented in notation, and “poetic” rhythm, the expressive timing that is actually performed.15 When performers use extreme rubato, it can be difficult for novice listeners to decode the underlying prosaic rhythms. By quantizing performances that use rubato, it is possible to hear the prosaic rhythms without needing to be able to follow a score.Prosaic rhythm is especially salient in the context of Baroque dances such as the Chaconne of J. S. Bach’s Partita for Solo Violin in D Minor (BWV 1004). Contemporary performances of the piece use very free rubato, and the connection with the chaconne as a dance form is opaque unless the listener can follow the score. I became curious to learn what the Chaconne would sound like as more literal dance music, at a steady tempo over a beat. A sufficiently skilled classical performer would be able to simply play it and hear it that way. Not being sufficiently skilled, or having a sufficiently skilled violinist at my disposal, I was nevertheless able to warp out performances of the Chaconne by Viktoria Mullova (2009), Christoph Poppen (2001), and Hopkinson Smith (2000).16 I was then able to quantize these recordings to a steady tempo, and even to overlay breakbeats and Afro-Cuban percussion to hear how they might sound as contemporary dance music.17 My students have found the Chaconne remix highly enlightening.In his rhythmic analysis of Chopin’s Étude op. 10, no. 3, John Rink argues that the piece makes sense only if the rhythms are played as Chopin wrote them.18 Rink complains that when performers apply their customary extreme rubato, they place emphases where Chopin did not intend them, diminishing the piece’s impact. I was able to demonstrate Rink’s analysis by quantizing Maurizio Pollini’s 1990 recording of the Étude to a steady tempo and listening to it over a breakbeat that complements its rhythms.19 All timestretching algorithms produce audible artifacting when pushed beyond their intended limits. In the instance of remixing Pollini’s recording, Live’s timestretching artifacts had an unexpected benefit. The further Pollini departs from metronomic time, the more extremely the audio must be stretched to align it to the grid, and the more prominent the resulting stuttery-sounding artifacting becomes. In the remix, the more artifacted a passage is, the more rubato there is in Pollini’s playing in that passage. Shifting this expressive aspect of the music from the tempo to the timbre has been a strange and fascinating listening experience.Quantizing music flattens out some of a performer’s expression, naturally, but that can be a positive for pedagogical purposes. Listening to a quantized version of a classical work is like “listening” to an audio version of the score. While a similar experience can be derived from listening to MIDI files played back by software instruments, quantized human performances are more satisfying, because they have real instrument sounds and nuanced expression. Preliminary and informal tests have shown that young listeners find the quantized versions easy to memorize and sing along with. It would be worthwhile to further explore the potential of quantizing classical works to support aural and participatory learning.Rap and dance music producers frequently sample copyrighted material, and often do so without permission. This practice gives rise to debates about intellectual property, both in legal contexts and in informal discourse among artists and fans.20 These debates have existed since the advent of digital samplers in the 1980s, but they assume greater urgency in a world where DAWs are so ubiquitous and accessible. Sampling is no longer practiced solely by underground producers; it is one of the dominant creative strategies of popular culture as a whole. Live’s sampling affordances bring the legal and ethical quandaries of sampling into particularly stark relief. For example, it is effortless to map slices of an audio sample to keys or pads on a controller and to play the slices back as an “instrument.” If the sample comes from a copyrighted source, how are we to understand who owns the resulting sampler performance? Does it matter whether the end result bears any auditory resemblance to its source material? As a matter of law, this is an unsettled issue, and there is no broad cultural consensus either. This material therefore makes for rich classroom discussion.Further complexities arise when we examine Live’s randomizing and algorithmic composition functions. For example, the Follow Actions function plays back sets of audio or MIDI clips in predictable, random, or semi-random sequences. Live’s arpeggiators and probabilistic MIDI generation functions can similarly be set to generate patterns with greater or lesser degrees of user control. The question of whether computer-generated music constitutes creativity or musicianship becomes more concrete when students hear Live generating convincing-sounding dance tracks without any user intervention.Recorded music is both familiar and strange: familiar in its ubiquity, and strange in its disconnection from human performers. The remixing and manipulation made possible by Ableton Live and software like it can be an avenue for Maxine Greene’s practice of noticing, being attentive to what is “ordinarily obscured by the familiar,” and, in so doing, defamiliarizing familiar things, “to make them strange.”21 Just as music scholars use music notation to analyze notated musics, so it makes sense to study produced musics using the tools of producers. Analytical audio manipulation and remixing is a form of arts-based research, a set of methods that can communicate immanent experience “without the limiting constraints of discursive communication.”22 Arts practice gives insight into musical experience that other forms of research may not, and the product of such practice can sometimes communicate musical concepts more clearly than verbal discourse.Why remix recordings rather than simply letting audio speak for itself? I personally prefer to make my editing and recontextualizing conspicuous at the level of listener awareness. Such overt editing highlights the innate artificiality of audio recording, rather than trying to convey the “real” original experience to the listener. Marc Weidenbaum observes that a recording of an experience never sounds like what he heard in the moment.23 The heightened listening experience of the remix is closer to the heightened attention that remixers hope to elicit from their listeners.Live is a commercial DAW, and as such, its affordances are different from tools intended for scholarly research. It is instructive to compare Live with Sonic Visualiser, a tool developed by and for Music Information Retrieval researchers. Sonic Visualiser extracts pitch data from audio recordings and displays spectrograms and other data visualizations. It can be scripted via Python to automatically analyze large data sets, and to produce quantitative data in various machine-readable formats. Live works on one audio file at a time, and it requires extensive user intervention to perform analysis. It is not useful for large-scale quantitative studies, but serves rather for smaller-scale qualitative analysis, and for presenting the results of that analysis in visually intuitive and musically appealing ways. To put it in a nutshell, Sonic Visualiser is for scientists, while Live is for artists. Live is useful to scholars to the extent that any arts medium is; it depends on the research subject and method.},
	language = {en},
	number = {1},
	urldate = {2022-07-12},
	journal = {Journal of the American Musicological Society},
	author = {Hein, Ethan},
	month = apr,
	year = {2021},
	pages = {214--225},
	annote = {Publisher: University of California Press},
	annote = {Publisher: University of California Press},
}

@misc{farrandFinale1988,
	title = {Finale},
	publisher = {MakeMusic},
	author = {Farrand, Phil},
	year = {1988},
}

@book{sarahpinkDesignEthnographyResearch2022,
	address = {New York, NY},
	title = {Design {Ethnography} : {Research}, {Responsibilities}, and {Futures}},
	isbn = {978-0-367-53904-7},
	shorttitle = {Design {Ethnography}},
	abstract = {This book advances the practice and theory of design ethnography. It presents a methodologically adventurous and conceptually robust approach to interventional and ethical research design, practice and engagement. The authors, specialising in design ethnography across the fields of anthropology, sociology, human geography, pedagogy and design research, draw on their extensive international experience of collaborating with engineers, designers, creative practitioners and specialists from other fields. They call for, and demonstrate the benefits of, ethnographic and conceptual attention to design as part of our personal and public everyday lives, society, institutions and activism. Design Ethnography is essential reading for researchers, scholars and students seeking to reshape the way we research, live and design ethically and responsibly into yet unknown futures.},
	language = {English},
	publisher = {Routledge},
	author = {{Sarah Pink} and {Vaike Fors} and {Debora Lanzeni} and {Melisa Duque} and {Shanti Sumartojo} and {Yolande Strengers}},
	year = {2022},
	keywords = {Design–Anthropological aspects, Ethnology–Methodology, Research–Moral and ethical aspects, SOCIAL SCIENCE / Anthropology / Cultural \& Social},
}

@article{guzhovAudioCLIPExtendingCLIP2021,
	title = {{AudioCLIP}: {Extending} {CLIP} to {Image}, {Text} and {Audio}},
	abstract = {In the past, the rapidly evolving field of sound classification greatly benefited from the application of methods from other domains. Today, we observe the trend to fuse domain-specific tasks and approaches together, which provides the community with new outstanding models. In this work, we present an extension of the CLIP model that handles audio in addition to text and images. Our proposed model incorporates the ESResNeXt audio-model into the CLIP framework using the AudioSet dataset. Such a combination enables the proposed model to perform bimodal and unimodal classification and querying, while keeping CLIP's ability to generalize to unseen datasets in a zero-shot inference fashion. AudioCLIP achieves new state-of-the-art results in the Environmental Sound Classification (ESC) task, out-performing other approaches by reaching accuracies of 90.07\% on the UrbanSound8K and 97.15\% on the ESC-50 datasets. Further it sets new baselines in the zero-shot ESC-task on the same datasets 68.78\% and 69.40\%, respectively). Finally, we also assess the cross-modal querying performance of the proposed model as well as the influence of full and partial training on the results. For the sake of reproducibility, our code is published.},
	author = {Guzhov, Andrey and Raue, Federico and Hees, Jörn and Dengel, Andreas},
	year = {2021},
	keywords = {audio classification, multimodal learning, zero-shot in-},
	pages = {1--14},
	annote = {\_eprint: 2106.13043},
	annote = {\_eprint: 2106.13043},
}

@article{GOTOH1982705,
	title = {An improved algorithm for matching biological sequences},
	volume = {162},
	issn = {0022-2836},
	url = {https://www.sciencedirect.com/science/article/pii/0022283682903989},
	doi = {https://doi.org/10.1016/0022-2836(82)90398-9},
	abstract = {The algorithm of Waterman et al. (1976) for matching biological sequences was modified under some limitations to be accomplished in essentially MN steps, instead of the M2N steps necessary in the original algorithm. The limitations do not seriously reduce the generality of the original method, and the present method is available for most practical uses. The algorithm can be executed on a small computer with a limited capacity of core memory.},
	number = {3},
	journal = {Journal of Molecular Biology},
	author = {Gotoh, Osamu},
	year = {1982},
	pages = {705--708},
}

@article{mukherjeeComposeinstyleMusicComposition2022,
	title = {Composeinstyle: {Music} composition with and without style transfer},
	volume = {191},
	abstract = {Every music composition has a composer at the core of its building block, molding it into a style of their own. The creative compositional style of a composer varies dynamically with every composer which is perishable and inimitable but cannot be preserved. This paper proposes a hybrid style transfer model which is an end-to-end approach to transfer style, compose as well as evaluate the style transfer accuracy in the domain of the target music composer. The paper focuses on 3 composer maestros Liszt, Chopin and Schubert taken from the Maestro dataset. The main step of music composer style transfer is accomplished in 3 systematic steps, training composer classifiers according to feature sets, generating a model of a composition in a particular composer’s style from noise, and finally style transfer and its evaluation. The 3 steps are interconnected in a way that each step is the building block for the next step. The GAN architecture of the style transfer step is built out of the GAN architecture of the second step. The compositions generated from each architecture is finally evaluated by the common pre-trained classifier of the first step. The highest accuracy obtained is 80\% for composer classification using the maestro dataset, 77.27\% for the classification of the generated style transferred version of a composition into the target composer class using the pre-trained classifiers.},
	journal = {Expert Systems with Applications},
	author = {Mukherjee, Sreetama and Mulimani, Manjunath},
	year = {2022},
	pages = {116195},
}

@inproceedings{Wu2021StyleFormerRA,
	title = {{StyleFormer}: {Real}-time arbitrary style transfer via parametric style composition},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Wu, Xiaolei and Hu, Zhihao and Sheng, Lu and Xu, Dong},
	year = {2021},
	pages = {14598--14607},
}

@inproceedings{9747817,
	title = {Music phrase inpainting using long-term representation and contrastive loss},
	doi = {10.1109/ICASSP43922.2022.9747817},
	booktitle = {{ICASSP} 2022 - 2022 {IEEE} international conference on acoustics, speech and signal processing ({ICASSP})},
	author = {Wei, Shiqi and Xia, Gus and Zhang, Yixiao and Lin, Liwei and Gao, Weiguo},
	year = {2022},
	pages = {186--190},
}

@inproceedings{Wu2020TheJT,
	title = {The jazz transformer on the front line: {Exploring} the shortcomings of {AI}-composed music through quantitative measures},
	booktitle = {International society for music information retrieval conference},
	author = {Wu, Shih-Lun and Yang, Yi-Hsuan},
	year = {2020},
}

@inproceedings{7471641,
	title = {Optimizing {DTW}-based audio-to-{MIDI} alignment and matching},
	doi = {10.1109/ICASSP.2016.7471641},
	booktitle = {2016 {IEEE} international conference on acoustics, speech and signal processing ({ICASSP})},
	author = {Raffel, Colin and Ellis, Daniel P. W.},
	year = {2016},
	pages = {81--85},
}

@book{Pfleiderer:2017:BOOK,
	title = {Inside the {Jazzomat} - {New} {Perspectives} for {Jazz} {Research}},
	publisher = {Schott Campus},
	editor = {Pfleiderer, Martin and Frieler, Klaus and Abeßer, Jakob and Zaddach, Wolf-Georg and Burkhart, Benjamin},
	year = {2017},
}

@article{Manilow2019CuttingMS,
	title = {Cutting music source separation some slakh: {A} dataset to study the impact of training data quality and quantity},
	journal = {2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
	author = {Manilow, Ethan and Wichern, Gordon and Seetharaman, Prem and Roux, Jonathan Le},
	year = {2019},
	pages = {45--49},
}

@article{10.1109/TASLP.2021.3121991,
	title = {High-resolution {Piano} {Transcription} with {Pedals} by {Regressing} {Onset} and {Offset} {Times}},
	volume = {29},
	issn = {2329-9290},
	url = {https://doi.org/10.1109/TASLP.2021.3121991},
	doi = {10.1109/TASLP.2021.3121991},
	abstract = {Automatic music transcription (AMT) is the task of transcribing audio recordings into symbolic representations. Recently, neural network-based methods have been applied to AMT, and have achieved state-of-the-art results. However, many previous systems only detect the onset and offset of notes frame-wise, so the transcription resolution is limited to the frame hop size. There is a lack of research on using different strategies to encode onset and offset targets for training. In addition, previous AMT systems are sensitive to the misaligned onset and offset labels of audio recordings. Furthermore, there are limited researches on sustain pedal transcription on large-scale datasets. In this article, we propose a high-resolution AMT system trained by regressing precise onset and offset times of piano notes. At inference, we propose an algorithm to analytically calculate the precise onset and offset times of piano notes and pedal events. We show that our AMT system is robust to the misaligned onset and offset labels compared to previous systems. Our proposed system achieves an onset F1 of 96.72\% on the MAESTRO dataset, outperforming previous onsets and frames system of 94.80\%. Our system achieves a pedal onset F1 score of 91.86\%, which is the first benchmark result on the MAESTRO dataset. We have released the source code and checkpoints of our work at ¡uri¿https://github.com/bytedance/pianoₜranscription¡/uri¿},
	journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
	author = {Kong, Qiuqiang and Li, Bochen and Song, Xuchen and Wan, Yuan and Wang, Yuxuan},
	month = oct,
	year = {2021},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {3707--3717},
	annote = {arXiv:2010.01815 [cs, eess]},
	annote = {Comment: 12 pages},
	annote = {Comment: 12 pages},
	annote = {Number of pages: 11 Publisher: IEEE Press tex.issue\_date: 2021},
	annote = {Number of pages: 11 Publisher: IEEE Press tex.issue\_date: 2021},
	file = {Kong et al. - 2021 - High-resolution Piano Transcription with Pedals by.pdf:/Users/eleanorrow/Zotero/storage/ZYUKEPTE/Kong et al. - 2021 - High-resolution Piano Transcription with Pedals by.pdf:application/pdf},
}

@inproceedings{10.1145/3394171.3413671,
	address = {New York, NY, USA},
	series = {{MM} '20},
	title = {Pop music transformer: {Beat}-based modeling and generation of expressive pop piano compositions},
	isbn = {978-1-4503-7988-5},
	url = {https://doi.org/10.1145/3394171.3413671},
	doi = {10.1145/3394171.3413671},
	abstract = {A great number of deep learning based models have been recently proposed for automatic music composition. Among these models, the Transformer stands out as a prominent approach for generating expressive classical piano performance with a coherent structure of up to one minute. The model is powerful in that it learns abstractions of data on its own, without much human-imposed domain knowledge or constraints. In contrast with this general approach, this paper shows that Transformers can do even better for music modeling, when we improve the way a musical score is converted into the data fed to a Transformer model. In particular, we seek to impose a metrical structure in the input data, so that Transformers can be more easily aware of the beat-bar-phrase hierarchical structure in music. The new data representation maintains the flexibility of local tempo changes, and provides hurdles to control the rhythmic and harmonic structure of music. With this approach, we build a Pop Music Transformer that composes Pop piano music with better rhythmic structure than existing Transformer models.},
	booktitle = {Proceedings of the 28th {ACM} international conference on multimedia},
	publisher = {Association for Computing Machinery},
	author = {Huang, Yu-Siang and Yang, Yi-Hsuan},
	year = {2020},
	keywords = {automatic music composition, Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, neural sequence model, Statistics - Machine Learning, transformer},
	pages = {1180--1188},
	annote = {Comment: Accepted at ACM Multimedia 2020},
	annote = {Issue: arXiv:2002.00212 arXiv: 2002.00212},
	annote = {Number of pages: 9 Place: Seattle, WA, USA},
	annote = {Number of pages: 9 Place: Seattle, WA, USA},
	file = {Huang and Yang - 2020 - Pop Music Transformer Beat-based Modeling and Gen.pdf:/Users/eleanorrow/Zotero/storage/8E9EYGX8/Huang and Yang - 2020 - Pop Music Transformer Beat-based Modeling and Gen.pdf:application/pdf},
}

@inproceedings{Hawthorne2021SequencetoSequencePT,
	title = {Sequence-to-sequence piano transcription with transformers},
	booktitle = {International {Society} for {Music} {Information} {Retrieval} conference},
	author = {Hawthorne, Curtis and Simon, Ian and Swavely, Rigel and Manilow, Ethan and Engel, Jesse},
	year = {2021},
}

@article{briotArtificialNeuralNetworks2021,
	title = {From artificial neural networks to deep learning for music generation: history, concepts and trends},
	volume = {33},
	issn = {1433-3058},
	url = {https://doi.org/10.1007/s00521-020-05399-0},
	doi = {10.1007/s00521-020-05399-0},
	abstract = {The current wave of deep learning (the hyper-vitamined return of artificial neural networks) applies not only to traditional statistical machine learning tasks: prediction and classification (e.g., for weather prediction and pattern recognition), but has already conquered other areas, such as translation. A growing area of application is the generation of creative content, notably the case of music, the topic of this article. The motivation is in using the capacity of modern deep learning techniques to automatically learn musical styles from arbitrary musical corpora and then to generate musical samples from the estimated distribution, with some degree of control over the generation. This article provides a tutorial on music generation based on deep learning techniques. After a short introduction to the topic illustrated by a recent example, the article analyzes some early works from the late 1980s using artificial neural networks for music generation and how their pioneering contributions foreshadowed current techniques. Then, we introduce some conceptual framework to analyze various concepts and dimensions involved. Various examples of recent systems are introduced and analyzed to illustrate the variety of concerns and of techniques.},
	number = {1},
	journal = {Neural Computing and Applications},
	author = {Briot, Jean-Pierre},
	month = jan,
	year = {2021},
	pages = {39--65},
	file = {Full Text:/Users/eleanorrow/Zotero/storage/36C69UVG/Briot - 2021 - From artificial neural networks to deep learning f.pdf:application/pdf},
}

@inproceedings{huang2018music,
	title = {Music transformer},
	url = {https://openreview.net/forum?id=rJe4ShAcF7},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Huang, Cheng-Zhi Anna and Vaswani, Ashish and Uszkoreit, Jakob and Simon, Ian and Hawthorne, Curtis and Shazeer, Noam and Dai, Andrew M. and Hoffman, Matthew D. and Dinculescu, Monica and Eck, Douglas},
	year = {2019},
	file = {Full Text:/Users/eleanorrow/Zotero/storage/A6P8E3NP/Huang et al. - 2019 - Music transformer.pdf:application/pdf},
}

@inproceedings{hawthorne-2018-onsets,
	title = {Onsets and frames: {Dual}-objective piano transcription},
	url = {http://ismir2018.ircam.fr/doc/pdfs/19_Paper.pdf},
	booktitle = {Proceedings of the 19th international society for music information retrieval conference, {ISMIR} 2018, paris, france, september 23-27, 2018},
	author = {Hawthorne, Curtis and Elsen, Erich and Song, Jialin and Roberts, Adam and Simon, Ian and Raffel, Colin and Engel, Jesse H. and Oore, Sageev and Eck, Douglas},
	editor = {Gómez, Emilia and Hu, Xiao and Humphrey, Eric and Benetos, Emmanouil},
	year = {2018},
	pages = {50--57},
	annote = {tex.bibsource: dblp computer science bibliography, https://dblp.org tex.biburl: https://dblp.org/rec/conf/ismir/HawthorneESRSRE18.bib tex.timestamp: Thu, 12 Mar 2020 11:33:21 +0100},
	annote = {tex.bibsource: dblp computer science bibliography, https://dblp.org tex.biburl: https://dblp.org/rec/conf/ismir/HawthorneESRSRE18.bib tex.timestamp: Thu, 12 Mar 2020 11:33:21 +0100},
	file = {Full Text:/Users/eleanorrow/Zotero/storage/KDTKIXN2/Hawthorne et al. - 2018 - Onsets and frames Dual-objective piano transcript.pdf:application/pdf},
}

@article{hochreiterVanishingGradientProblem1998,
	title = {The {Vanishing} {Gradient} {Problem} {During} {Learning} {Recurrent} {Neural} {Nets} and {Problem} {Solutions}},
	volume = {06},
	issn = {0218-4885},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0218488598000094},
	doi = {10.1142/S0218488598000094},
	abstract = {Recurrent nets are in principle capable to store past inputs to produce the currently desired output. Because of this property recurrent nets are used in time series prediction and process control. Practical applications involve temporal dependencies spanning many time steps, e.g. between relevant inputs and desired outputs. In this case, however, gradient based learning methods take too much time. The extremely increased learning time arises because the error vanishes as it gets propagated back. In this article the de-caying error flow is theoretically analyzed. Then methods trying to overcome vanishing gradients are briefly discussed. Finally, experiments comparing conventional algorithms and alternative methods are presented. With advanced methods long time lag problems can be solved in reasonable time.},
	number = {02},
	urldate = {2022-12-09},
	journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
	author = {Hochreiter, Sepp},
	month = apr,
	year = {1998},
	keywords = {long short-term memory, long-term dependencies, Recurrent neural nets, vanishing gradient},
	pages = {107--116},
	annote = {Publisher: World Scientific Publishing Co.},
	annote = {Publisher: World Scientific Publishing Co.},
}

@inproceedings{liuEfficientThemeNontrivial1999,
	title = {Efficient theme and non-trivial repeating pattern discovering in music databases},
	doi = {10.1109/ICDE.1999.754893},
	abstract = {Proposes an approach for the fast discovery of all non-trivial repeating patterns in music objects. A repeating pattern is a sequence of notes which appears more than once in a music object. The longest repeating patterns in music objects are typically their themes. The themes and other non-trivial repeating patterns are important musical features which can be used both for content-based retrieval of music data and for music data analysis. We present a data structure called an RP-tree (repeating pattern tree) and its associated algorithms for the fast extraction of all non-trivial repeating patterns in a music object. Experiments are performed to compare this method with related approaches. The results are further analysed to show the efficiency and effectiveness of our approach.},
	booktitle = {Proceedings 15th {International} {Conference} on {Data} {Engineering} ({Cat}. {No}.{99CB36337})},
	author = {Liu, Chih-Chin and Hsu, Jia-Lien and Chen, A.L.P.},
	month = mar,
	year = {1999},
	keywords = {Content based retrieval, Data mining, Data structures, Humans, Image retrieval, Information retrieval, Multimedia databases, Multiple signal classification, Music information retrieval, Rhythm},
	pages = {14--21},
	annote = {ISSN: 1063-6382},
	annote = {ISSN: 1063-6382},
}

@inproceedings{zhangATEPPDatasetAutomatically2022,
	title = {{ATEPP}: {A} {Dataset} of {Automatically} {Transcribed} {Expressive} {Piano} {Performance}},
	shorttitle = {{ATEPP}},
	url = {https://ismir2022program.ismir.net/poster_70.html},
	abstract = {Computational models of expressive piano performance rely on attributes like tempo, timing, dynamics and pedalling. Despite some promising models for performance assessment and performance rendering, results are limited by the scale, breadth and uniformity of existing datasets. In this paper, we present ATEPP, a dataset that contains 1000 hours of performances of standard piano repertoire by 49 world-renowned pianists, organized and aligned by compositions and movements for comparative studies. Scores in MusicXML format are also available for around half of the tracks. We first evaluate and verify the use of transcribed MIDI for representing expressive performance with a listening evaluation that involves recent transcription models. Then, the process of sourcing and curating the dataset is outlined, including composition entity resolution and a pipeline for audio matching and solo filtering. Finally, we conduct baseline experiments for performer identification and performance rendering on our datasets, demonstrating its potential in generalizing expressive features of individual performing style.{\textless}br{\textgreater}{\textless}br{\textgreater} \textbf{{\textless}p align="center"{\textgreater}[Direct link to video](https://drive.google.com/open?id=1jtKPOklu0VSGfQkXaO-yKqbHxI2ixNx2)}},
	language = {en},
	urldate = {2023-02-20},
	booktitle = {Proceedings of the {First} {MiniCon} {Conference}},
	author = {Zhang, Huan* and Tang, Jingjing and Rafee, Syed RM and Dixon, Simon and Fazekas, George and Wiggins, Geraint A.},
	month = dec,
	year = {2022},
	file = {Zhang et al. - 2022 - ATEPP A DATASET OF AUTOMATICALLY TRANSCRIBED EXPR.pdf:/Users/eleanorrow/Zotero/storage/K5J7FP7E/Zhang et al. - 2022 - ATEPP A DATASET OF AUTOMATICALLY TRANSCRIBED EXPR.pdf:application/pdf},
}

@book{normanThingsThatMake1993,
	address = {USA},
	title = {Things that make us smart: defending human attributes in the age of the machine},
	isbn = {978-0-201-62695-7},
	shorttitle = {Things that make us smart},
	publisher = {Addison-Wesley Longman Publishing Co., Inc.},
	author = {Norman, Donald A.},
	year = {1993},
}

@incollection{rappAutoethnographyHumanComputerInteraction2018a,
	address = {Cham},
	series = {Human–{Computer} {Interaction} {Series}},
	title = {Autoethnography in {Human}-{Computer} {Interaction}: {Theory} and {Practice}},
	isbn = {978-3-319-73374-6},
	shorttitle = {Autoethnography in {Human}-{Computer} {Interaction}},
	url = {https://doi.org/10.1007/978-3-319-73374-6_3},
	abstract = {Autoethnography is an ethnographic method in which a fieldworker’s experience is investigated together with the experience of other observed social actors. Over the years, Human-Computer Interaction (HCI) research almost exclusively produced “objective ethnographies”, attempting to generate accurate descriptions of the “world” and the individuals inhabiting it. However, recently HCI community started exploring different forms of observing and describing reality, making the ethnographer regain visibility, and produce reflexive first-person recounts of her work. Autoethnography might be precisely inscribed in this movement, whereby it explicitly attempts to recount the fieldwork from the fieldworker’s point of view, situating the ethnographer as the protagonist of the ethnographic narration. In this chapter, I will outline the anthropological roots of the autoethnographic method, and describe its potential implications for HCI research.},
	language = {en},
	urldate = {2022-12-08},
	booktitle = {New {Directions} in {Third} {Wave} {Human}-{Computer} {Interaction}: {Volume} 2 - {Methodologies}},
	publisher = {Springer International Publishing},
	author = {Rapp, Amon},
	editor = {Filimowicz, Michael and Tzankova, Veronika},
	year = {2018},
	doi = {10.1007/978-3-319-73374-6_3},
	pages = {25--42},
}

@article{valenzuela-moguillanskyAnalysisProcedureMicroPhenomenological2019a,
	title = {An {Analysis} {Procedure} for the {Micro}-{Phenomenological} {Interview}},
	volume = {14},
	issn = {1782-348X},
	url = {https://constructivist.info/14/2/123},
	abstract = {Context: The advent of the embodied approach to cognition produced a paradigm shift giving experience a primary place in the different fields of inquiry. This gave rise to the need to develop …},
	language = {en},
	number = {2},
	urldate = {2022-12-08},
	journal = {Constructivist Foundations},
	author = {Valenzuela-Moguillansky, Camila and Vásquez-Rosati, Alejandra},
	year = {2019},
	note = {tex.copyright: 2019 Constructivist Foundations},
	pages = {123--145},
	annote = {Number: 2 Publisher: Alexander Riegler},
	annote = {Number: 2 Publisher: Alexander Riegler},
}

@article{poulsenEmbodiedDesignThinking2011,
	title = {Embodied design thinking: a phenomenological perspective},
	volume = {7},
	issn = {1571-0882},
	shorttitle = {Embodied design thinking},
	url = {https://doi.org/10.1080/15710882.2011.563313},
	doi = {10.1080/15710882.2011.563313},
	abstract = {The aim of the article is to demonstrate, discuss and substantiate the embodiment of design thinking: what role does the body play in relation to engaging in design interaction and the generating of ideas? In order to discuss this question, we draw on the phenomenological philosophy of Merleau-Ponty and his concept of the lived body. The phenomenological perspective is related to a single case study in which three designers collaborate during a workshop as they discuss ethnographic video material with the aim of generating new ideas. Through an analysis of their interaction it is argued that the embodied engagement of the designers plays a fundamental role both in understanding the problem at hand and in opening up new ideas leading to a new design solution. The verbal interaction constantly finds its meaning in reference to a tacit level of embodiment, which remains unspoken. The verbal interaction is also integrated into the designer's tacit use of items in the surroundings. Consequently, the paper concludes that design thinking cannot be understood if we are only attentive to verbalised interaction, but design thinking relies on a more complex and multidimensional interaction, which is based on the pre-linguistic engaged perspective of the lived body.},
	number = {1},
	urldate = {2022-12-08},
	journal = {CoDesign : international journal of cocreation in design and the arts},
	author = {Poulsen, Søren Bolvig and Thøgersen, Ulla},
	month = mar,
	year = {2011},
	keywords = {design interaction, design thinking, embodiment, Merleau-Ponty, phenomenology, Video Card Game},
	pages = {29--44},
	annote = {Publisher: Taylor \& Francis \_eprint: https://doi.org/10.1080/15710882.2011.563313},
	annote = {Publisher: Taylor \& Francis \_eprint: https://doi.org/10.1080/15710882.2011.563313},
}

@book{goodmanReconceptionsPhilosophyOther1988,
	title = {Reconceptions in {Philosophy} and {Other} {Arts} and {Sciences}},
	isbn = {978-0-87220-052-4},
	abstract = {\&quot;The authors argue against certain philosophical distinctions between art and science; between verbal and nonverbal meaning; and between the affective and the cognitive. The book continues Goodman\&\#39;s argument against one traditional mode of philosophizing which privileges the notions of \&\#39;truth\&\#39; and \&\#39;knowledge\&\#39;. Hence, the book is in a broadly pragmatic tradition. It also deals in detail with such topics as meaning in architecture and the concept of \&\#39;variation\&\#39; in art, and contains a superb critique of some important views in contemporary epistemology. This work will be savored even by those who will not accept all aspects of Goodman and Elgin\&\#39;s approach. Essential for all undergraduate philosophy collections.\&quot; –Stanley Bates, Choice},
	language = {en},
	publisher = {Hackett Publishing},
	author = {Goodman, Nelson and Elgin, Catherine Z.},
	month = jan,
	year = {1988},
	keywords = {Philosophy / History \& Surveys / General},
	annote = {Google-Books-ID: jEsYBM6ef1IC},
	annote = {Google-Books-ID: jEsYBM6ef1IC},
}

@book{sismanHaydnClassicalVariation1993,
	title = {Haydn and the {Classical} {Variation}},
	isbn = {978-0-674-38315-9},
	abstract = {In this first full-scale examination of the theme-and-variations form in the Classical era, Elaine Sisman demonstrates persuasively that it was Haydn's prophetic innovationsplacing the variation in every position of a multi-movement cycle, broadening its array of theme types, and transforming its larger shapethat truly created the Classical variation. She elucidates the concept and technique of variation, traces Haydn's development and use of the form in symphonies, chamber music, and keyboard works, and then shows how Mozart and Beethoven in their individual ways built on his contributions. Throughout, Sisman's analysis reflects both musical thinking of the Classical period and today's critical interests. She discusses ornamentation and musical figures, explores the pervasive eighteenth-century notion of music as rhetoric, and relates the style of the variation to that of the other dominant form in this period: sonata form. Her book offers a revaluation of the nature of the variation form and a new approach to the music of Haydn. Haydn and the Classical Variation is addressed to students and scholars of music, but the author's unaffected style makes it accessible to nonprofessional music lovers as well.},
	language = {en},
	publisher = {Harvard University Press},
	author = {Sisman, Elaine Rochelle},
	year = {1993},
	keywords = {Music / History \& Criticism, Music / Instruction \& Study / General},
	annote = {Google-Books-ID: kL3Xnuzye9QC},
	annote = {Google-Books-ID: kL3Xnuzye9QC},
}

@book{fauvelMusicMathematicsPythagoras2006,
	title = {Music and {Mathematics}: {From} {Pythagoras} to {Fractals}},
	isbn = {978-0-19-929893-8},
	shorttitle = {Music and {Mathematics}},
	abstract = {From Ancient Greek times, music has been seen as a mathematical art, and the relationship between mathematics and music has fascinated generations. This collection of wide ranging, comprehensive and fully-illustrated papers, authorized by leading scholars, presents the link between these two subjects in a lucid manner that is suitable for students of both subjects, as well as the general reader with an interest in music. Physical, theoretical, physiological, acoustic, compositional and analytical relationships between mathematics and music are unfolded and explored with focus on tuning and temperament, the mathematics of sound, bell-ringing and modern compositional techniques.},
	language = {en},
	publisher = {Oxford University Press},
	author = {Fauvel, John and Flood, Raymond and Wilson, Robin J.},
	year = {2006},
	keywords = {Mathematics / General, Mathematics / History \& Philosophy},
	annote = {Google-Books-ID: yLBykq9cjp0C},
	annote = {Google-Books-ID: yLBykq9cjp0C},
}

@article{dahlstedtMusicalAgentsComputerAided2006,
	title = {Musical {Agents}: {Toward} {Computer}-{Aided} {Music} {Composition} {Using} {Autonomous} {Software} {Agents}},
	volume = {39},
	issn = {0024-094X},
	shorttitle = {Musical {Agents}},
	url = {https://doi.org/10.1162/leon.2006.39.5.469},
	doi = {10.1162/leon.2006.39.5.469},
	abstract = {The authors, a composer and a computer scientist, discuss their collaborative research on the use of multiagent systems and their applicability to music and musical composition. They describe the development of software and techniques for the composition of generative music.},
	number = {5},
	urldate = {2022-12-06},
	journal = {Leonardo},
	author = {Dahlstedt, Palle and McBurney, Peter},
	month = oct,
	year = {2006},
	pages = {469--470},
}

@article{vigliensoniCreatingLatentSpaces2020,
	title = {Creating {Latent} {Spaces} for {Modern} {Music} {Genre} {Rhythms} {Using} {Minimal} {Training} {Data}},
	author = {Vigliensoni, Gabriel and McCallum, Louis and Fiebrink, Rebecca},
	year = {2020},
	annote = {Publisher: Goldsmiths, University of London},
	annote = {Publisher: Goldsmiths, University of London},
}

@inproceedings{achilleEmergenceInvarianceDisentanglement2018,
	address = {San Diego, CA},
	title = {Emergence of {Invariance} and {Disentanglement} in {Deep} {Representations}},
	volume = {19},
	isbn = {978-1-72810-124-8},
	url = {https://ieeexplore.ieee.org/document/8503149/},
	doi = {10.1109/ITA.2018.8503149},
	abstract = {Using established principles from Statistics and Information Theory, we show that invariance to nuisance factors in a deep neural network is equivalent to information minimality of the learned representation, and that stacking layers and injecting noise during training naturally bias the network towards learning invariant representations. We then decompose the cross-entropy loss used during training and highlight the presence of an inherent overﬁtting term. We propose regularizing the loss by bounding such a term in two equivalent ways: One with a Kullbach-Leibler term, which relates to a PAC-Bayes perspective; the other using the information in the weights as a measure of complexity of a learned model, yielding a novel Information Bottleneck for the weights. Finally, we show that invariance and independence of the components of the representation learned by the network are bounded above and below by the information in the weights, and therefore are implicitly optimized during training. The theory enables us to quantify and predict sharp phase transitions between underﬁtting and overﬁtting of random labels when using our regularized loss, which we verify in experiments, and sheds light on the relation between the geometry of the loss function, invariance properties of the learned representation, and generalization error.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {2018 {Information} {Theory} and {Applications} {Workshop} ({ITA})},
	publisher = {IEEE},
	author = {Achille, Alessandro and Soatto, Stefano},
	month = feb,
	year = {2018},
	keywords = {Deep learning, Flat minima, Generalization, Information bottleneck, Information complexity, Minimality, Neural network, Overfitting, PAC-Bayes, Regularization, Representation, Sensitivity, Stochastic gradient descent, Sufficiency, Total correlation},
	pages = {1--9},
	annote = {ISBN: 9781728101248},
	file = {Achille and Soatto - 2018 - Emergence of Invariance and Disentanglement in Dee.pdf:/Users/eleanorrow/Zotero/storage/NHSIDNJE/Achille and Soatto - 2018 - Emergence of Invariance and Disentanglement in Dee.pdf:application/pdf},
}

@inproceedings{adegbijaJazznetDatasetFundamental2023,
	address = {Rhodes Island, Greece},
	title = {Jazznet: {A} {Dataset} of {Fundamental} {Piano} {Patterns} for {Music} {Audio} {Machine} {Learning} {Research}},
	isbn = {978-1-72816-327-7},
	shorttitle = {Jazznet},
	url = {https://ieeexplore.ieee.org/document/10096620/},
	doi = {10.1109/ICASSP49357.2023.10096620},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {{ICASSP} 2023 - 2023 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Adegbija, Tosiron},
	month = jun,
	year = {2023},
	pages = {1--5},
	file = {Adegbija - 2023 - Jazznet A Dataset of Fundamental Piano Patterns f.pdf:/Users/eleanorrow/Zotero/storage/Y6324E22/Adegbija - 2023 - Jazznet A Dataset of Fundamental Piano Patterns f.pdf:application/pdf},
}

@article{ahonenCompressionbasedSimilarityMeasures2011,
	title = {Compression-based similarity measures in symbolic, polyphonic music},
	abstract = {We present a novel compression-based method for measuring similarity between sequences of symbolic, polyphonic music. The method is based on mapping the values of binary chromagrams extracted from MIDI ﬁles to tonal centroids, then quantizing the tonal centroid representation values to sequences, and ﬁnally measuring the similarity between the quantized sequences using Normalized Compression Distance (NCD). The method is comprehensively evaluated with a test set of classical music variations, and the highest achieved precision and recall values suggest that the proposed method can be applied for similarity measuring. Also, we analyze the performance of the method and discuss what should be taken into consideration when applying the method for measurement tasks.},
	language = {en},
	journal = {Poster Session},
	author = {Ahonen, Teppo E and Lemstrom, Kjell and Linkola, Simo},
	year = {2011},
	pages = {6},
	file = {Ahonen et al. - 2011 - COMPRESSION-BASED SIMILARITY MEASURES IN SYMBOLIC,.pdf:/Users/eleanorrow/Zotero/storage/SDFDVDBI/Ahonen et al. - 2011 - COMPRESSION-BASED SIMILARITY MEASURES IN SYMBOLIC,.pdf:application/pdf},
}

@inproceedings{aktenLearningSeeYou2019,
	address = {Los Angeles California},
	title = {Learning to see: you are what you see},
	isbn = {978-1-4503-6311-2},
	shorttitle = {Learning to see},
	url = {https://dl.acm.org/doi/10.1145/3306211.3320143},
	doi = {10.1145/3306211.3320143},
	abstract = {The authors present a visual instrument developed as part of the creation of the artwork Learning to See. The artwork explores bias in artificial neural networks and provides mechanisms for the manipulation of specifically trained–for real-world representations. The exploration of these representations acts as a metaphor for the process of developing a visual understanding and/or visual vocabulary of the world. These representations can be explored and manipulated in real time, and have been produced in such a way so as to reflect specific creative perspectives that call into question the relationship between how both artificial neural networks and humans may construct meaning.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {{ACM} {SIGGRAPH} 2019 {Art} {Gallery}},
	publisher = {ACM},
	author = {Akten, Memo and Fiebrink, Rebecca and Grierson, Mick},
	month = jul,
	year = {2019},
	pages = {1--6},
	annote = {ISBN: 9781450363112},
	file = {Akten et al. - 2019 - Learning to see you are what you see.pdf:/Users/eleanorrow/Zotero/storage/W6WF3K9E/Akten et al. - 2019 - Learning to see you are what you see.pdf:application/pdf},
}

@misc{albanieOriginSpeciesSelfSupervised2021,
	title = {On the {Origin} of {Species} of {Self}-{Supervised} {Learning}},
	url = {http://arxiv.org/abs/2103.17143},
	abstract = {In the quiet backwaters of cs.CV, cs.LG and stat.ML, a cornucopia of new learning systems is emerging from a primordial soup of mathematics—learning systems with no need for external supervision. To date, little thought has been given to how these self-supervised learners have sprung into being or the principles that govern their continuing diversiﬁcation. After a period of deliberate study and dispassionate judgement during which each author set their Zoom virtual background to a separate Gala´pagos island, we now entertain no doubt that each of these learning machines are lineal descendants of some older and generally extinct species.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Albanie, Samuel and Lu, Erika and Henriques, Joao F.},
	month = mar,
	year = {2021},
	note = {arXiv:2103.17143 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: SIGBOVIK 2021},
	annote = {\_eprint: 2103.17143},
	file = {Albanie et al. - 2021 - On the Origin of Species of Self-Supervised Learni.pdf:/Users/eleanorrow/Zotero/storage/XTXZ4KBX/Albanie et al. - 2021 - On the Origin of Species of Self-Supervised Learni.pdf:application/pdf},
}

@article{amesMarkovProcessCompositional1989a,
	title = {The {Markov} {Process} as a {Compositional} {Model}: {A} {Survey} and {Tutorial}},
	volume = {22},
	issn = {0024094X},
	shorttitle = {The {Markov} {Process} as a {Compositional} {Model}},
	url = {https://www.jstor.org/stable/1575226?origin=crossref},
	doi = {10.2307/1575226},
	abstract = {The author combines a survey of Markov-based efforts in automated composition with a tutorial demonstrating how various theoretical properties associated with Markov processes can be put to practical use. The historical background is traced from A. A. Markov's original formulation through to the present. A digression into Markov-chain theory introduces 'waiting counts' and 'stationary probabilities'. The author's "Demonstration 4" for solo clarinet illustrates how these properties affect the behavior of a melody composed using Markov chains. This simple example becomes a point of departure for increasingly general interpretations of the Markov process. The interpretation of 'states' is reevaluated in the light of recent musical efforts that employ Markov chains of higher-level objects and in the light of other efforts that incorporate relative attributes into the possible interpretations. Other efforts expand Markov's original definition to embrace 'Nth-order' transitions, evolving transition matrices and chains of chains. The remainder of this article contrasts Markov processes with alternative compositional strategies.},
	language = {en},
	number = {2},
	urldate = {2023-12-30},
	journal = {Leonardo},
	author = {Ames, Charles},
	year = {1989},
	pages = {175},
	annote = {Publisher: The MIT Press},
	file = {Ames - 1989 - The Markov Process as a Compositional Model A Sur.pdf:/Users/eleanorrow/Zotero/storage/FLPF9DBF/Ames - 1989 - The Markov Process as a Compositional Model A Sur.pdf:application/pdf},
}

@misc{arjovskyWassersteinGAN2017,
	title = {Wasserstein {GAN}},
	url = {http://arxiv.org/abs/1701.07875},
	abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
	month = dec,
	year = {2017},
	note = {arXiv:1701.07875 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {\_eprint: 1701.07875},
	file = {Arjovsky et al. - 2017 - Wasserstein GAN.pdf:/Users/eleanorrow/Zotero/storage/5NZWWMWW/Arjovsky et al. - 2017 - Wasserstein GAN.pdf:application/pdf},
}

@misc{austinStructuredDenoisingDiffusion2023,
	title = {Structured {Denoising} {Diffusion} {Models} in {Discrete} {State}-{Spaces}},
	url = {http://arxiv.org/abs/2107.03006},
	abstract = {Denoising diffusion probabilistic models (DDPMs) [19] have shown impressive results on image and waveform generation in continuous state spaces. Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs), diffusionlike generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al. [20], by going beyond corruption processes with uniform transition probabilities. This includes corruption with transition matrices that mimic Gaussian kernels in continuous space, matrices based on nearest neighbors in embedding space, and matrices that introduce absorbing states. The third allows us to draw a connection between diffusion models and autoregressive and mask-based generative models. We show that the choice of transition matrix is an important design decision that leads to improved results in image and text domains. We also introduce a new loss function that combines the variational lower bound with an auxiliary cross entropy loss. For text, this model class achieves strong results on character-level text generation while scaling to large vocabularies on LM1B. On the image dataset CIFAR-10, our models approach the sample quality and exceed the log-likelihood of the continuous-space DDPM model.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Austin, Jacob and Johnson, Daniel D. and Ho, Jonathan and Tarlow, Daniel and van den Berg, Rianne},
	month = feb,
	year = {2023},
	note = {arXiv:2107.03006 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: 10 pages plus references and appendices. First two authors contributed equally},
	file = {Austin et al. - 2023 - Structured Denoising Diffusion Models in Discrete .pdf:/Users/eleanorrow/Zotero/storage/ZG2QZ7BI/Austin et al. - 2023 - Structured Denoising Diffusion Models in Discrete .pdf:application/pdf},
}

@article{avdeeffArtificialIntelligencePopular2019,
	title = {Artificial {Intelligence} \& {Popular} {Music}: {SKYGGE}, {Flow} {Machines}, and the {Audio} {Uncanny} {Valley}},
	volume = {8},
	issn = {2076-0752},
	shorttitle = {Artificial {Intelligence} \& {Popular} {Music}},
	url = {https://www.mdpi.com/2076-0752/8/4/130},
	doi = {10.3390/arts8040130},
	abstract = {This article presents an overview of the ﬁrst AI-human collaborated album, Hello World, by SKYGGE, which utilizes Sony’s Flow Machines technologies. This case study is situated within a review of current and emerging uses of AI in popular music production, and connects those uses with myths and fears that have circulated in discourses concerning the use of AI in general, and how these fears connect to the idea of an audio uncanny valley. By proposing the concept of an audio uncanny valley in relation to AIPM (artiﬁcial intelligence popular music), this article oﬀers a lens through which to examine the more novel and unusual melodies and harmonization made possible through AI music generation, and questions how this content relates to wider speculations about posthumanism, sincerity, and authenticity in both popular music, and broader assumptions of anthropocentric creativity. In its documentation of the emergence of a new era of popular music, the AI era, this article surveys: (1) The current landscape of artiﬁcial intelligence popular music focusing on the use of Markov models for generative purposes; (2) posthumanist creativity and the potential for an audio uncanny valley; and (3) issues of perceived authenticity in the technologically mediated “voice”.},
	language = {en},
	number = {4},
	urldate = {2023-12-30},
	journal = {Arts},
	author = {Avdeeff, Melissa},
	month = oct,
	year = {2019},
	keywords = {a new phase of, a new society, artificial intelligence, being created, brought about, by this or that, creativity, Flow Machines, Generative Music, history, in the same way, Inpainting, it is often said, Models, most of us know, new, new technology, our world, people speak of a, popular music, posthuman, Sony, that television has altered, the atomic bomb, the automobile, the steam engine, uncanny valley, what, world},
	pages = {130},
	file = {Avdeeff - 2019 - Artificial Intelligence & Popular Music SKYGGE, F.pdf:/Users/eleanorrow/Zotero/storage/2ZDHR3RX/Avdeeff - 2019 - Artificial Intelligence & Popular Music SKYGGE, F.pdf:application/pdf},
}

@inproceedings{bartonMelodicStringMatching2012,
	address = {Berlin, Heidelberg},
	title = {Melodic {String} {Matching} via {Interval} {Consolidation} and {Fragmentation}},
	volume = {382},
	isbn = {978-3-642-33411-5 978-3-642-33412-2},
	url = {http://link.springer.com/10.1007/978-3-642-33412-2_47},
	abstract = {In this paper, we address the problem of melodic string matching that enables identification of varied (ornamented) instances of a given melodic pattern. To this aim, a new set of edit distance operations adequate for pitch interval strings is introduced. Insertion, deletion and replacement operations are abolished as irrelevant. Consolidation and fragmentation are retained, but adapted to the pitch interval domain, i.e., two or more intervals of one string may be matched to an interval from a second string through consolidation or fragmentation. The melodic interval string matching problem consists of finding all occurrences of a given pattern in a melodic sequence that takes into account exact matches, consolidations and fragmentations of intervals in both the sequence and the pattern. We show some properties of the problem and an algorithm that solves this problem is proposed.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {Artificial {Intelligence} {Applications} and {Innovations}},
	publisher = {Springer Berlin Heidelberg},
	author = {Barton, Carl and Cambouropoulos, Emilios and Iliopoulos, Costas S. and Lipták, Zsuzsanna},
	editor = {Iliadis, Lazaros and Maglogiannis, Ilias and Papadopoulos, Harris and Karatzas, Kostas and Sioutas, Spyros},
	year = {2012},
	doi = {10.1007/978-3-642-33412-2_47},
	note = {Series Title: IFIP Advances in Information and Communication Technology},
	pages = {460--469},
	annote = {Series Title: IFIP Advances in Information and Communication Technology},
	file = {Barton et al. - 2012 - Melodic String Matching via Interval Consolidation.pdf:/Users/eleanorrow/Zotero/storage/GRVIF8ST/Barton et al. - 2012 - Melodic String Matching via Interval Consolidation.pdf:application/pdf},
}

@inproceedings{barzDeepLearningSmall2020,
	address = {Snowmass Village, CO, USA},
	title = {Deep {Learning} on {Small} {Datasets} without {Pre}-{Training} using {Cosine} {Loss}},
	isbn = {978-1-72816-553-0},
	url = {https://ieeexplore.ieee.org/document/9093286/},
	doi = {10.1109/WACV45572.2020.9093286},
	abstract = {Two things seem to be indisputable in the contemporary deep learning discourse: 1. The categorical cross-entropy loss after softmax activation is the method of choice for classiﬁcation. 2. Training a CNN classiﬁer from scratch on small datasets does not work well.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {2020 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE},
	author = {Barz, Bjorn and Denzler, Joachim},
	month = mar,
	year = {2020},
	pages = {1360--1369},
	file = {Barz and Denzler - 2020 - Deep Learning on Small Datasets without Pre-Traini.pdf:/Users/eleanorrow/Zotero/storage/RMV7UUQR/Barz and Denzler - 2020 - Deep Learning on Small Datasets without Pre-Traini.pdf:application/pdf},
}

@misc{bazinSpectrogramInpaintingInteractive2021,
	title = {Spectrogram {Inpainting} for {Interactive} {Generation} of {Instrument} {Sounds}},
	url = {http://arxiv.org/abs/2104.07519},
	abstract = {Modern approaches to sound synthesis using deep neural networks are hard to control, especially when ﬁne-grained conditioning information is not available, hindering their adoption by musicians.},
	language = {en},
	urldate = {2023-12-30},
	author = {Bazin, Théis and Hadjeres, Gaëtan and Esling, Philippe and Malt, Mikhail},
	year = {2021},
	doi = {10.30746/978-91-519-5560-5},
	note = {arXiv:2104.07519 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, inpainting, interactive interfaces, nsynth, sound synthesis, transformers, vector-quantization, vq-vae-2, web interfaces},
	annote = {Comment: 8 pages + references + appendices. 4 figures. Published as a conference paper at the The 2020 Joint Conference on AI Music Creativity, October 19-23, 2020, organized and hosted virtually by the Royal Institute of Technology (KTH), Stockholm, Sweden},
	annote = {\_eprint: 2104.07519},
	file = {Bazin et al. - 2021 - Spectrogram Inpainting for Interactive Generation .pdf:/Users/eleanorrow/Zotero/storage/BKPEVXNZ/Bazin et al. - 2021 - Spectrogram Inpainting for Interactive Generation .pdf:application/pdf},
}

@article{beerTrainingDeepQuantum2020,
	title = {Training deep quantum neural networks},
	volume = {11},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-020-14454-2},
	doi = {10.1038/s41467-020-14454-2},
	abstract = {Abstract
            Neural networks enjoy widespread success in both research and industry and, with the advent of quantum technology, it is a crucial challenge to design quantum neural networks for fully quantum learning tasks. Here we propose a truly quantum analogue of classical neurons, which form quantum feedforward neural networks capable of universal quantum computation. We describe the efficient training of these networks using the fidelity as a cost function, providing both classical and efficient quantum implementations. Our method allows for fast optimisation with reduced memory requirements: the number of qudits required scales with only the width, allowing deep-network optimisation. We benchmark our proposal for the quantum task of learning an unknown unitary and find remarkable generalisation behaviour and a striking robustness to noisy training data.},
	language = {en},
	number = {1},
	urldate = {2023-12-30},
	journal = {Nature Communications},
	author = {Beer, Kerstin and Bondarenko, Dmytro and Farrelly, Terry and Osborne, Tobias J. and Salzmann, Robert and Scheiermann, Daniel and Wolf, Ramona},
	month = feb,
	year = {2020},
	pages = {1--6},
	annote = {ISBN: 2041-1723},
	file = {Beer et al. - 2020 - Training deep quantum neural networks.pdf:/Users/eleanorrow/Zotero/storage/KA3YWCXW/Beer et al. - 2020 - Training deep quantum neural networks.pdf:application/pdf;Beer et al. - 2020 - Training deep quantum neural networks.pdf:/Users/eleanorrow/Zotero/storage/4MVA78Y9/Beer et al. - 2020 - Training deep quantum neural networks.pdf:application/pdf},
}

@article{ben-talHowMusicAI2021,
	title = {How {Music} {AI} {Is} {Useful}: \textit{{Engagements} with {Composers}, {Performers} and {Audiences}}},
	volume = {54},
	issn = {0024-094X, 1530-9282},
	shorttitle = {How {Music} {AI} {Is} {Useful}},
	url = {https://direct.mit.edu/leon/article/54/5/510/97278/How-Music-AI-Is-Useful-Engagements-with-Composers},
	doi = {10.1162/leon_a_01959},
	abstract = {Critical but often overlooked questions of research in artificial intelligence (AI) applied to music involve the impact of the resulting models for music. How and to what extent does such research contribute to the domain of music? How are the resulting models useful for music practitioners? In this article, we describe how we are addressing such questions by engaging composers, musicians, and audiences with our research. We describe two websites we have created that make our AI models accessible to a wide audience. We describe a professionally recorded album that we released to gauge the plausibility of material generated by our models and reviewers’ comments on the music. Finally, we describe the use of our AI models as a tool for co-creation. Evaluating AI music models in these ways illuminate their impact on music making in a range of styles and practices.},
	language = {en},
	number = {5},
	urldate = {2023-12-30},
	journal = {Leonardo},
	author = {Ben-Tal, Oded and Harris, Matthew Tobias and Sturm, Bob L.T.},
	month = oct,
	year = {2021},
	keywords = {Human-AI Partnerships},
	pages = {510--516},
	file = {Ben-Tal et al. - 2021 - How Music AI Is Useful Engagements with Compos.pdf:/Users/eleanorrow/Zotero/storage/RXBH5KD7/Ben-Tal et al. - 2021 - How Music AI Is Useful Engagements with Compos.pdf:application/pdf},
}

@article{bernardoDesigningEvaluatingUsability2020,
	title = {Designing and {Evaluating} the {Usability} of a {Machine} {Learning} {API} for {Rapid} {Prototyping} {Music} {Technology}},
	volume = {3},
	issn = {2624-8212},
	url = {https://www.frontiersin.org/article/10.3389/frai.2020.00013/full},
	doi = {10.3389/frai.2020.00013},
	abstract = {To better support creative software developers and music technologists’ needs, and to empower them as machine learning users and innovators, the usability of and developer experience with machine learning tools must be considered and better understood. We review background research on the design and evaluation of application programming interfaces (APIs), with a focus on the domain of machine learning for music technology software development. We present the design rationale for the RAPID-MIX API, an easy-to-use API for rapid prototyping with interactive machine learning, and a usability evaluation study with software developers of music technology. A cognitive dimensions questionnaire was designed and delivered to a group of 12 participants who used the RAPID-MIX API in their software projects, including people who developed systems for personal use and professionals developing software products for music and creative technology companies. The results from questionnaire indicate that participants found the RAPID-MIX API a machine learning API which is easy to learn and use, fun, and good for rapid prototyping with interactive machine learning. Based on these ﬁndings, we present an analysis and characterization of the RAPID-MIX API based on the cognitive dimensions framework, and discuss its design trade-offs and usability issues. We use these insights and our design experience to provide design recommendations for ML APIs for rapid prototyping of music technology. We conclude with a summary of the main insights, a discussion of the merits and challenges of the application of the CDs framework to the evaluation of machine learning APIs, and directions to future work which our research deems valuable.},
	language = {en},
	urldate = {2023-12-30},
	journal = {Frontiers in Artificial Intelligence},
	author = {Bernardo, Francisco and Zbyszyński, Michael and Grierson, Mick and Fiebrink, Rebecca},
	month = apr,
	year = {2020},
	pages = {13},
	file = {Bernardo et al. - 2020 - Designing and Evaluating the Usability of a Machin.pdf:/Users/eleanorrow/Zotero/storage/Z9BTNP6W/Bernardo et al. - 2020 - Designing and Evaluating the Usability of a Machin.pdf:application/pdf},
}

@inproceedings{berndtSurveyVariationTechniques2012,
	address = {Corfu Greece},
	title = {A survey of variation techniques for repetitive games music},
	isbn = {978-1-4503-1569-2},
	url = {https://dl.acm.org/doi/10.1145/2371456.2371466},
	doi = {10.1145/2371456.2371466},
	abstract = {How much time will a player spend in an interactive scene? For the majority of game scenarios this is impossible to predict. Therefore, their musical accompaniment is usually disposed to continuously loop until player interaction triggers a change. This approach involves an existential danger: Sooner or later the player becomes aware of the repetitive character of the ambience design; the game scenario emerges as a mere mechanical arrangement and loses much of its integrity.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {Proceedings of the 7th {Audio} {Mostly} {Conference}: {A} {Conference} on {Interaction} with {Sound}},
	publisher = {ACM},
	author = {Berndt, Axel and Dachselt, Raimund and Groh, Rainer},
	month = sep,
	year = {2012},
	keywords = {Games music, Games Music, Generative music, Models},
	pages = {61--67},
	annote = {ISBN: 9781450315692},
	file = {Berndt et al. - 2012 - A survey of variation techniques for repetitive ga.pdf:/Users/eleanorrow/Zotero/storage/8D79NQKP/Berndt et al. - 2012 - A survey of variation techniques for repetitive ga.pdf:application/pdf},
}

@book{beyerContextualDesignDefining1997,
	address = {San Francisco, Calif},
	title = {Contextual {Design}: {Defining} {Customer}-{Centered} {Systems}},
	isbn = {978-1-55860-411-7},
	shorttitle = {Contextual design},
	abstract = {This book introduces a customer-centered approach to business by showing how data gathered from people while they work can drive the definition of a product or process while supporting the needs of teams and their organizations. This is a practical, hands-on guide for anyone trying to design systems that reflect the way customers want to do their work. The authors developed Contextual Design, the method discussed here, through their work with teams struggling to design products and internal systems. In this book, you'll find the underlying principles of the method and how to apply them to different problems, constraints, and organizational situations. Contextual Design enables you to + gather detailed data about how people work and use systems + develop a coherent picture of a whole customer population + generate systems designs from a knowledge of customer work + diagram a set of existing systems, showing their relationships, inconsistencies, redundancies, and omissions Table of Contents Chapter 1 Introduction Chapter 2 Gathering Customer Data Chapter 3 Principles of Contextual Inquiry Chapter 4 Contextual Inquiry in Practice Chapter 5 A Language of Work Chapter 6 Work Models Chapter 7 The Interpretation Session Chapter 8 Consolidation Chapter 9 Creating One View of the Customer Chapter 10 Communicating to the Organization Chapter 11 Work Redesign Chapter 12 Using Data to Drive Design Chapter 13 Design from Data Chapter 14 System Design Chapter 15 The User Environment Design Chapter 16 Project Planning and Strategy Chapter 17 Prototyping as a Design Tool Chapter 18 From Structure to User Interface Chapter 19 Iterating with a Prototype Chapter 20 Putting It into Practice},
	language = {en},
	publisher = {Morgan Kaufmann},
	author = {Beyer, Hugh and Holtzblatt, Karen},
	year = {1997},
	keywords = {System analysis, System design},
	file = {Beyer and Holtzblatt - 1998 - Contextual design defining customer-centered syst.pdf:/Users/eleanorrow/Zotero/storage/Y8L7RRGV/Beyer and Holtzblatt - 1998 - Contextual design defining customer-centered syst.pdf:application/pdf},
}

@inproceedings{bodkerWhenSecondWave2006a,
	address = {Oslo Norway},
	series = {{NordiCHI} '06},
	title = {When second wave {HCI} meets third wave challenges},
	isbn = {978-1-59593-325-6},
	url = {https://dl.acm.org/doi/10.1145/1182475.1182476},
	doi = {10.1145/1182475.1182476},
	abstract = {This paper surveys the current status of second generation HCI theory, faced with the challenges brought to HCI by the so-called third wave. In the third wave, the use context and application types are broadened, and intermixed, relative to the focus of the second wave on work. Technology spreads from the workplace to our homes and everyday lives and culture. Using these challenges the paper specifically addresses the topics of multiplicity, context, boundaries, experience and participation in order to discuss where second wave theory and conceptions can still be positioned to make a contribution as part of the maturing of our handling of the challenges brought on by the third wave.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {Proceedings of the 4th {Nordic} {Conference} on {Human}-computer {Interaction}: {Changing} {Roles}},
	publisher = {Association for Computing Machinery},
	author = {Bødker, Susanne},
	month = oct,
	year = {2006},
	keywords = {boundaries, context, experience, multiplicity, participation},
	pages = {1--8},
	annote = {event-place: New York, NY, USA},
	file = {Bødker - 2006 - When second wave HCI meets third wave challenges.pdf:/Users/eleanorrow/Zotero/storage/4ZLWNCSR/Bødker - 2006 - When second wave HCI meets third wave challenges.pdf:application/pdf},
}

@article{bond-taylorDeepGenerativeModelling2022,
	title = {Deep {Generative} {Modelling}: {A} {Comparative} {Review} of {VAEs}, {GANs}, {Normalizing} {Flows}, {Energy}-{Based} and {Autoregressive} {Models}},
	volume = {44},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {Deep {Generative} {Modelling}},
	url = {http://arxiv.org/abs/2103.04922},
	doi = {10.1109/TPAMI.2021.3116668},
	abstract = {Deep generative modelling is a class of techniques that train deep neural networks to model the distribution of training samples. Research has fragmented into various interconnected approaches, each of which making trade-offs including run-time, diversity, and architectural restrictions. In particular, this compendium covers energy-based models, variational autoencoders, generative adversarial networks, autoregressive models, normalizing ﬂows, in addition to numerous hybrid approaches. These techniques are drawn under a single cohesive framework, comparing and contrasting to explain the premises behind each, while reviewing current state-of-the-art advances and implementations.},
	language = {en},
	number = {11},
	urldate = {2023-12-30},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Bond-Taylor, Sam and Leach, Adam and Long, Yang and Willcocks, Chris G.},
	month = nov,
	year = {2022},
	note = {arXiv:2103.04922 [cs, stat]},
	keywords = {68T01 (Primary), 68T07 (Secondary), Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, G.3, I.4.0, I.5.0, Statistics - Machine Learning},
	pages = {7327--7347},
	annote = {Comment: 20 pages, 9 figures, will appear in IEEE Transactions on Pattern Analysis and Machine Intelligence},
	file = {Bond-Taylor et al. - 2022 - Deep Generative Modelling A Comparative Review of.pdf:/Users/eleanorrow/Zotero/storage/ML6WG2CG/Bond-Taylor et al. - 2022 - Deep Generative Modelling A Comparative Review of.pdf:application/pdf},
}

@article{braunReflectingReflexiveThematic2019,
	title = {Reflecting on reflexive thematic analysis},
	volume = {11},
	issn = {2159-676X, 2159-6778},
	url = {https://www.tandfonline.com/doi/full/10.1080/2159676X.2019.1628806},
	doi = {10.1080/2159676X.2019.1628806},
	abstract = {Since initially writing on thematic analysis in 2006, the popularity of the method we outlined has exploded, the variety of TA approaches have expanded, and, not least, our thinking has developed and shifted. In this reﬂexive commentary, we look back at some of the unspoken assumptions that informed how we wrote our 2006 paper. We connect some of these un-identiﬁed assumptions, and developments in the method over the years, with some conceptual mismatches and confusions we see in published TA studies. In order to facilitate better TA practice, we reﬂect on how our thinking has evolved – and in some cases sedimented –since the publication of our 2006 paper, and clarify and revise some of the ways we phrased or conceptualised TA, and the elements of, and processes around, a method we now prefer to call reﬂexive TA.},
	language = {en},
	number = {4},
	urldate = {2023-12-30},
	journal = {Qualitative Research in Sport, Exercise and Health},
	author = {Braun, Virginia and Clarke, Victoria},
	month = aug,
	year = {2019},
	keywords = {Big Q qualitative, coding, data domains, epistemology, ontology, paradigm, post-positivism, reflexivity, small Q qualitative, themes},
	pages = {589--597},
	annote = {Publisher: Routledge \_eprint: https://doi.org/10.1080/2159676X.2019.1628806},
	file = {Braun and Clarke - 2019 - Reflecting on reflexive thematic analysis.pdf:/Users/eleanorrow/Zotero/storage/69UX6TXY/Braun and Clarke - 2019 - Reflecting on reflexive thematic analysis.pdf:application/pdf},
}

@article{braunOneSizeFits2021,
	title = {One size fits all? {What} counts as quality practice in (reflexive) thematic analysis?},
	volume = {18},
	issn = {1478-0887, 1478-0895},
	shorttitle = {One size fits all?},
	url = {https://www.tandfonline.com/doi/full/10.1080/14780887.2020.1769238},
	doi = {10.1080/14780887.2020.1769238},
	abstract = {Developing a universal quality standard for thematic analysis (TA) is complicated by the existence of numerous iterations of TA that differ paradigmatically, philosophically and procedurally. This plur­ ality in TA is often not recognised by editors, reviewers or authors, who promote ‘coding reliability measures’ as universal require­ ments of quality TA. Focusing particularly on our reflexive TA approach, we discuss quality in TA with reference to ten common problems we have identified in published TA research that cites or claims to follow our guidance. Many of the common problems are underpinned by an assumption of homogeneity in TA. We end by outlining guidelines for reviewers and editors – in the form of twenty critical questions – to support them in promoting high(er) standards in TA research, and more deliberative and reflexive engagement with TA as method and practice.},
	language = {en},
	number = {3},
	urldate = {2023-12-30},
	journal = {Qualitative Research in Psychology},
	author = {Braun, Virginia and Clarke, Victoria},
	month = jul,
	year = {2021},
	keywords = {Codebook, coding frame, coding reliability, consensus coding, framework analysis, inter-rater reliability, qualitative paradigm, reflexivity, template analysis, theme},
	pages = {328--352},
	annote = {Publisher: Routledge \_eprint: https://doi.org/10.1080/14780887.2020.1769238},
	file = {Braun and Clarke - 2021 - One size fits all What counts as quality practice.pdf:/Users/eleanorrow/Zotero/storage/BQMJZJUX/Braun and Clarke - 2021 - One size fits all What counts as quality practice.pdf:application/pdf},
}

@article{braunConceptualDesignThinking2022,
	title = {Conceptual and {Design} {Thinking} for {Thematic} {Analysis}},
	volume = {9},
	issn = {2326-3598, 2326-3601},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/qup0000196},
	doi = {10.1037/qup0000196},
	abstract = {Thematic analysis (TA) is widely used in qualitative psychology. In using TA, researchers must choose between a diverse range of approaches that can differ considerably in their underlying (but often implicit) conceptualizations of qualitative research, meaningful knowledge production, and key constructs such as themes, as well as analytic procedures. This diversity within the method of TA is typically poorly understood and rarely acknowledged, resulting in the frequent publication of research lacking in design coherence. Furthermore, because TA offers researchers something closer to a method (a transtheoretical tool or technique) rather than a methodology (a theoretically informed framework for research), one with considerable theoretical and design ﬂexibility, researchers need to engage in careful conceptual and design thinking to produce TA research with methodological integrity. In this article, we support researchers in their conceptual and design thinking for TA, and particularly for the reﬂexive approach we have developed, by guiding them through the conceptual underpinnings of different approaches to TA, and key design considerations. We outline our typology of three main “schools” of TA—coding reliability, codebook, and reﬂexive—and consider how these differ in their conceptual underpinnings, with a particular focus on the distinct characteristics of our reﬂexive approach. We discuss key areas of design—research questions, data collection, participant/data item selection strategy and criteria, ethics, and quality standards and practices—and end with guidance on reporting standards for reﬂexive TA.},
	language = {en},
	number = {1},
	urldate = {2023-12-30},
	journal = {Qualitative Psychology},
	author = {Braun, Virginia and Clarke, Victoria},
	month = feb,
	year = {2022},
	keywords = {Data Collection, Ethics, Integrity, Methodology, Procedural Knowledge, Qualitative Methods, Reporting Standards, Thematic Analysis},
	pages = {3--26},
	annote = {Place: US Publisher: Educational Publishing Foundation},
	file = {Braun and Clarke - 2022 - Conceptual and design thinking for thematic analys.pdf:/Users/eleanorrow/Zotero/storage/99QPN4ZE/Braun and Clarke - 2022 - Conceptual and design thinking for thematic analys.pdf:application/pdf},
}

@article{bretanUnitSelectionMethodologya,
	title = {A {Unit} {Selection} {Methodology} for {Music} {Generation} using {Deep} {Neural} {Networks}},
	abstract = {Several methods exist for a computer to generate music based on data including Markov chains, recurrent neural networks, recombinancy, and grammars. We explore the use of unit selection and concatenation as a means of generating music using a procedure based on ranking, where, we consider a unit to be a variable length number of measures of music. We ﬁrst examine whether a unit selection method, that is restricted to a ﬁnite size unit library, can be sufﬁcient for encompassing a wide spectrum of music. This is done by developing a deep autoencoder that encodes a musical input and reconstructs the input by selecting from the library. We then describe a generative model that combines a deep structured semantic model (DSSM) with an LSTM to predict the next unit, where units consist of four, two, and one measures of music. We evaluate the generative model using objective metrics including mean rank and accuracy and with a subjective listening test in which expert musicians are asked to complete a forcedchoiced ranking task. Our system is compared to a note-level generative baseline model that consists of a stacked LSTM trained to predict forward by one note.},
	language = {en},
	author = {Bretan, Mason and Weinberg, Gil and Heck, Larry},
	pages = {8},
	file = {Bretan et al. - A Unit Selection Methodology for Music Generation .pdf:/Users/eleanorrow/Zotero/storage/FA64KAAS/Bretan et al. - A Unit Selection Methodology for Music Generation .pdf:application/pdf},
}

@article{???summersPlayingTuneVideo2011,
	title = {Playing the {Tune}: {Video} {Game} {Music}, {Gamers}, and {Genre}},
	abstract = {This article proposes a particular approach to video game music by advocating a genre-based enquiry. Two generic levels are active in video game music: “interactive genre” (the type of game/interactive mechanism) and “environmental genre” (the “setting” of the game). The interaction between these levels produces the game’s music. By examining games within the same interactive genre, even if the environmental genre is markedly different, we can begin to uncover similar concerns, functions and methodologies of game music. Three interactive genres are briefly examined (survival horror games, strategy games, fighting games), in order to demonstrate how musicalstrategic similarities can be seen to weave through game genres.},
	language = {en},
	author = {??? Summers, Tim},
	year = {2011},
	file = {Summers - Playing the Tune Video Game Music, Gamers, and Ge.pdf:/Users/eleanorrow/Zotero/storage/WRJAAXVH/Summers - Playing the Tune Video Game Music, Gamers, and Ge.pdf:application/pdf},
}

@article{brownStimulatingCreativePartnerships2016,
	title = {Stimulating {Creative} {Partnerships} in {Human}-{Agent} {Musical} {Interaction}},
	volume = {14},
	issn = {1544-3574},
	url = {https://dl.acm.org/doi/10.1145/2991146},
	doi = {10.1145/2991146},
	abstract = {Musical duets are a type of creative partnership with a long history of artistic practice. What can they tell us about creative partnerships between a human and a computer? To explore this question, we implemented an activity-based model of duet interaction in software designed to support musical metacreation and investigated the experience of performing with it. The activity-based model allowed for the application of reflexive interactive processes, previously used in dialogic interaction, to a synchronous musical performance context. The experience of improvising with the computational agent was evaluated by expert musicians, who reported that interactions were fun, engaging, and challenging, despite some obvious limitations in the musical sophistication of the software. These findings reinforce the idea that even simple metacreative systems can stimulate creative partnerships and, further, that creative human-machine duet partnerships may well produce, like human-human duet partnerships, more than the sum of their parts.},
	language = {en},
	number = {2},
	urldate = {2023-12-30},
	journal = {Computers in Entertainment},
	author = {Brown, Andrew R. and Gifford, Toby and Voltz, Bradley},
	month = dec,
	year = {2016},
	keywords = {computational creativity, Human-AI Partnerships, human-computer interaction., interactive music systems, Musical metacreation},
	pages = {1--17},
	file = {Brown et al. - 2016 - Stimulating Creative Partnerships in Human-Agent M.pdf:/Users/eleanorrow/Zotero/storage/VVCYKCB4/Brown et al. - 2016 - Stimulating Creative Partnerships in Human-Agent M.pdf:application/pdf},
}

@article{buttonEthnographicTraditionDesign2000,
	title = {The {Ethnographic} {Tradition} and {Design}},
	volume = {21},
	issn = {0142694X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0142694X00000053},
	doi = {10.1016/S0142-694X(00)00005-3},
	abstract = {This paper reflects upon the emerging uses of ethnography in engineering and systems design. Although ethnography is often equated simply with fieldwork, a driving force in the early development of classical ethnography was to provide accounts for what is observed in terms of a priori anthropological and sociological theories. Recent studies of collaborative work systems informed by ethnomethodology have exposed the shortcomings of classical ethnography, and have shifted the emphasis of fieldwork towards describing the accountable practices through which those in work constitute and organise their joint activity. However, a second wave of fieldwork-based studies of work systems and design is now gaining force that threatens to dilute this analytic emphasis. Using examples drawn from the production printing industry, I argue that recent examples of scenic fieldwork—fieldwork that merely describes and codifies what relevant persons do in the workplace—may well be missing out on the constitutive practices of how they do what they do, the `interactional what' of their activities. Rather than ethnography, or even fieldwork itself, it is the explication of members' knowledge—what people have to know to do work, and how that knowledge is deployed in the ordering and organisation of work—that provides the key to understanding the contribution of sociology to engineering and design.},
	language = {en},
	number = {4},
	urldate = {2023-12-30},
	journal = {Design Studies},
	author = {Button, Graham},
	month = jul,
	year = {2000},
	keywords = {collaborative design, ethnography, research methods, systems design},
	pages = {319--332},
	file = {Button - 2000 - The ethnographic tradition and design.pdf:/Users/eleanorrow/Zotero/storage/MGPN6HVV/Button - 2000 - The ethnographic tradition and design.pdf:application/pdf},
}

@article{cadizCreativityGenerativeMusical2021,
	title = {Creativity in {Generative} {Musical} {Networks}: {Evidence} {From} {Two} {Case} {Studies}},
	volume = {8},
	issn = {2296-9144},
	shorttitle = {Creativity in {Generative} {Musical} {Networks}},
	url = {https://www.frontiersin.org/articles/10.3389/frobt.2021.680586/full},
	doi = {10.3389/frobt.2021.680586},
	abstract = {Deep learning, one of the fastest-growing branches of artiﬁcial intelligence, has become one of the most relevant research and development areas of the last years, especially since 2012, when a neural network surpassed the most advanced image classiﬁcation techniques of the time. This spectacular development has not been alien to the world of the arts, as recent advances in generative networks have made possible the artiﬁcial creation of high-quality content such as images, movies or music. We believe that these novel generative models propose a great challenge to our current understanding of computational creativity. If a robot can now create music that an expert cannot distinguish from music composed by a human, or create novel musical entities that were not known at training time, or exhibit conceptual leaps, does it mean that the machine is then creative? We believe that the emergence of these generative models clearly signals that much more research needs to be done in this area. We would like to contribute to this debate with two case studies of our own: TimbreNet, a variational auto-encoder network trained to generate audio-based musical chords, and StyleGAN Pianorolls, a generative adversarial network capable of creating short musical excerpts, despite the fact that it was trained with images and not musical data. We discuss and assess these generative models in terms of their creativity and we show that they are in practice capable of learning musical concepts that are not obvious based on the training data, and we hypothesize that these deep models, based on our current understanding of creativity in robots and machines, can be considered, in fact, creative.},
	language = {en},
	number = {August},
	urldate = {2023-12-30},
	journal = {Frontiers in Robotics and AI},
	author = {Cádiz, Rodrigo F. and Macaya, Agustín and Cartagena, Manuel and Parra, Denis},
	month = aug,
	year = {2021},
	keywords = {creativity, deep learning - artificial neural network (DL-ANN), GAN (generative adversarial network), generative models, music, VAE (variational AutoEncoder)},
	pages = {680586},
	file = {Cádiz et al. - 2021 - Creativity in Generative Musical Networks Evidenc.pdf:/Users/eleanorrow/Zotero/storage/MWKH85BJ/Cádiz et al. - 2021 - Creativity in Generative Musical Networks Evidenc.pdf:application/pdf},
}

@article{caramiauxAdaptiveGestureRecognition2015,
	title = {Adaptive {Gesture} {Recognition} with {Variation} {Estimation} for {Interactive} {Systems}},
	volume = {4},
	issn = {2160-6455, 2160-6463},
	url = {https://dl.acm.org/doi/10.1145/2643204},
	doi = {10.1145/2643204},
	abstract = {This article presents a gesture recognition/adaptation system for human--computer interaction applications that goes beyond activity classification and that, as a complement to gesture labeling, characterizes the movement execution. We describe a template-based recognition method that simultaneously aligns the input gesture to the templates using a Sequential Monte Carlo inference technique. Contrary to standard template-based methods based on dynamic programming, such as Dynamic Time Warping, the algorithm has an adaptation process that tracks gesture variation in real time. The method continuously updates, during execution of the gesture, the estimated parameters and recognition results, which offers key advantages for continuous human--machine interaction. The technique is evaluated in several different ways: Recognition and early recognition are evaluated on 2D onscreen pen gestures; adaptation is assessed on synthetic data; and both early recognition and adaptation are evaluated in a user study involving 3D free-space gestures. The method is robust to noise, and successfully adapts to parameter variation. Moreover, it performs recognition as well as or better than nonadapting offline template-based methods.},
	language = {en},
	number = {4},
	urldate = {2023-12-30},
	journal = {ACM Transactions on Interactive Intelligent Systems},
	author = {Caramiaux, Baptiste and Montecchio, Nicola and Tanaka, Atau and Bevilacqua, Frédéric},
	month = jan,
	year = {2015},
	keywords = {adaptive decoding, continuous gesture modeling, gesture analysis, Gesture recognition, Human-AI Partnerships, particle filtering, real time},
	pages = {1--34},
	file = {Caramiaux et al. - 2015 - Adaptive Gesture Recognition with Variation Estima.pdf:/Users/eleanorrow/Zotero/storage/FYMHB95T/Caramiaux et al. - 2015 - Adaptive Gesture Recognition with Variation Estima.pdf:application/pdf},
}

@article{carlessMuchLouderAm2021,
	title = {Much {Louder} {Than} {I} {Am} : {A} {Musical} {Collaborative} {Autoethnography}},
	volume = {14},
	issn = {1940-8447, 1940-8455},
	shorttitle = {\textit{{Much} {Louder} {Than} {I} {Am}}},
	url = {http://journals.sagepub.com/doi/10.1177/1940844720978754},
	doi = {10.1177/1940844720978754},
	abstract = {One challenge of performative research is that a performance is a one-t­ime unique event. It cannot be preserved or returned to in its own form. Here, we offer a more durable artifact to preserve some aspects of the collaborative performance autoethnography we performed at the International Congress of Qualitative Inquiry (ICQI) in 2018. We write to communicate not only what we performed during the session but also our sentiments concerning singing and playing music as autoethnography. Because so often in our work we use songs, songwriting, music, and performance; we propose rhythm, melody, and harmony as alternative acts of autoethnographic collaboration. In this way of doing autoethnography, it may be that no words are spoken. But the burden of work is shared. This is the kind of collaboration we seek … in the here and now.},
	language = {en},
	number = {2},
	urldate = {2023-12-30},
	journal = {International Review of Qualitative Research},
	author = {Carless, David and Douglas, Kitrina},
	month = aug,
	year = {2021},
	pages = {276--284},
	file = {Carless and Douglas - 2021 - Much Louder Than I Am  A Musical Collabora.pdf:/Users/eleanorrow/Zotero/storage/FDGKPJSP/Carless and Douglas - 2021 - Much Louder Than I Am  A Musical Collabora.pdf:application/pdf},
}

@inproceedings{carnovaliniOpenChallengesMusical2019,
	address = {Valencia Spain},
	title = {Open {Challenges} in {Musical} {Metacreation}},
	isbn = {978-1-4503-6261-0},
	url = {https://dl.acm.org/doi/10.1145/3342428.3342678},
	doi = {10.1145/3342428.3342678},
	abstract = {Musical Metacreation tries to obtain creative behaviors from computers algorithms composing music. In this paper I briefly analyze how this field evolved from algorithmic composition to be focused on the search for creativity, and I point out some issues in pursuing this goal. Finally, I argue that hybridization of algorithms can be a useful direction for research.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {Proceedings of the 5th {EAI} {International} {Conference} on {Smart} {Objects} and {Technologies} for {Social} {Good}},
	publisher = {ACM},
	author = {Carnovalini, Filippo},
	month = sep,
	year = {2019},
	keywords = {algorithmic composition, music, musical metacreation},
	pages = {124--125},
	file = {Carnovalini - 2019 - Open Challenges in Musical Metacreation.pdf:/Users/eleanorrow/Zotero/storage/AYG6Q3A6/Carnovalini - 2019 - Open Challenges in Musical Metacreation.pdf:application/pdf},
}

@misc{changVariableLengthMusicScore2021,
	title = {Variable-{Length} {Music} {Score} {Infilling} via {XLNet} and {Musically} {Specialized} {Positional} {Encoding}},
	url = {http://arxiv.org/abs/2108.05064},
	abstract = {This paper proposes a new self-attention based model for music score inﬁlling, i.e., to generate a polyphonic music sequence that ﬁlls in the gap between given past and future contexts. While existing approaches can only ﬁll in a short segment with a ﬁxed number of notes, or a ﬁxed time span between the past and future contexts, our model can inﬁll a variable number of notes (up to 128) for different time spans. We achieve so with three major technical contributions. First, we adapt XLNet, an autoregressive model originally proposed for unsupervised model pre-training, to music score inﬁlling. Second, we propose a new, musically specialized positional encoding called relative bar encoding that better informs the model of notes’ position within the past and future context. Third, to capitalize relative bar encoding, we perform look-ahead onset prediction to predict the onset of a note one time step before predicting the other attributes of the note. We compare our proposed model with two strong baselines and show that our model is superior in both objective and subjective analyses.},
	language = {en},
	author = {Chang, Chin-Jui and Lee, Chun-Yi and Yang, Yi-Hsuan},
	year = {2021},
	annote = {\_eprint: 2108.05064},
	file = {Chang et al. - 2021 - VARIABLE-LENGTH MUSIC SCORE INFILLING VIA XLNET AN.pdf:/Users/eleanorrow/Zotero/storage/YDF9UDNW/Chang et al. - 2021 - VARIABLE-LENGTH MUSIC SCORE INFILLING VIA XLNET AN.pdf:application/pdf},
}

@inproceedings{chenCreatingMelodiesEvolving2001,
	address = {Washington, DC, USA},
	title = {Creating melodies with evolving recurrent neural networks},
	volume = {3},
	isbn = {978-0-7803-7044-9},
	url = {http://ieeexplore.ieee.org/document/938515/},
	doi = {10.1109/IJCNN.2001.938515},
	abstract = {Music composition is a domain well-suitedf o r evolutionary reinforcement learning. Instead of applying explicit composition rules, a neural network is used to generate melodies. An evolutionary algorithm is used to find a neural network that maximizes the chance of generating good melodies. Composition rules on tonality and rhythm are used as a fitness function f o r the evolution. We observe that the model learns to generate melodies according to these rules with interesting variations.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {{IJCNN}'01. {International} {Joint} {Conference} on {Neural} {Networks}. {Proceedings} ({Cat}. {No}.{01CH37222})},
	publisher = {IEEE},
	author = {Chen, C.-C.J. and Miikkulainen, R.},
	year = {2001},
	keywords = {On paper},
	pages = {2241--2246},
	file = {Chen and Miikkulainen - 2001 - Creating melodies with evolving recurrent neural n.pdf:/Users/eleanorrow/Zotero/storage/4CR296YC/Chen and Miikkulainen - 2001 - Creating melodies with evolving recurrent neural n.pdf:application/pdf},
}

@inproceedings{chenEffectExplicitStructure2019,
	title = {The {Effect} of {Explicit} {Structure} {Encoding} of {Deep} {Neural} {Networks} for {Symbolic} {Music} {Generation}},
	isbn = {1-72811-649-X},
	url = {http://arxiv.org/abs/1811.08380},
	doi = {10.1109/MMRP.2019.00022},
	abstract = {With recent breakthroughs in artiﬁcial neural networks, deep generative models have become one of the leading techniques for computational creativity. Despite very promising progress on image and short sequence generation, symbolic music generation remains a challenging problem since the structure of compositions are usually complicated. In this study, we attempt to solve the melody generation problem constrained by the given chord progression. In particular, we explore the effect of explicit architectural encoding of musical structure via comparing two sequential generative models: LSTM (a type of RNN) and WaveNet (dilated temporal-CNN). As far as we know, this is the ﬁrst study of applying WaveNet to symbolic music generation, as well as the ﬁrst systematic comparison between temporalCNN and RNN for music generation. We conduct a survey for evaluation in our generations and implemented Variable Markov Oracle in music pattern discovery. Experimental results show that to encode structure more explicitly using a stack of dilated convolution layers improved the performance signiﬁcantly, and a global encoding of underlying chord progression into the generation procedure gains even more.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {2019 {International} {Workshop} on {Multilayer} {Music} {Representation} and {Processing} ({MMRP})},
	publisher = {IEEE},
	author = {Chen, Ke and Zhang, Weilin and Dubnov, Shlomo and Xia, Gus and Li, Wei},
	month = jan,
	year = {2019},
	note = {arXiv:1811.08380 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {77--84},
	annote = {Comment: 8 pages, 13 figures},
	annote = {Comment: 8 pages, 13 figures},
	file = {Chen et al. - 2019 - The Effect of Explicit Structure Encoding of Deep .pdf:/Users/eleanorrow/Zotero/storage/YR6UJJ26/Chen et al. - 2019 - The Effect of Explicit Structure Encoding of Deep .pdf:application/pdf},
}

@misc{chenSimpleFrameworkContrastive2020,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {http://arxiv.org/abs/2002.05709},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = jun,
	year = {2020},
	note = {arXiv:2002.05709 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, SimCLR, Statistics - Machine Learning},
	annote = {Comment: ICML'2020. Code and pretrained models at https://github.com/google-research/simclr},
	annote = {\_eprint: 2002.05709},
	file = {Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf:/Users/eleanorrow/Zotero/storage/Y6CPXKJS/Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf:application/pdf},
}

@article{cherryQuantifyingCreativitySupport2014,
	title = {Quantifying the {Creativity} {Support} of {Digital} {Tools} through the {Creativity} {Support} {Index}},
	volume = {21},
	issn = {1073-0516, 1557-7325},
	url = {https://dl.acm.org/doi/10.1145/2617588},
	doi = {10.1145/2617588},
	abstract = {Creativity support tools help people engage creatively with the world, but measuring how well a tool supports creativity is challenging since creativity is ill-defined. To this end, we developed the Creativity Support Index (CSI), which is a psychometric survey designed for evaluating the ability of a creativity support tool to assist a user engaged in creative work. The CSI measures six dimensions of creativity support: Exploration, Expressiveness, Immersion, Enjoyment, Results Worth Effort, and Collaboration. The CSI allows researchers to understand not just how well a tool supports creative work overall, but what aspects of creativity support may need attention. In this article, we present the CSI, along with scenarios for how it can be deployed in a variety of HCI research settings and how the CSI scores can help target design improvements. We also present the iterative, rigorous development and validation process used to create the CSI.},
	language = {en},
	number = {4},
	urldate = {2023-12-30},
	journal = {ACM Transactions on Computer-Human Interaction},
	author = {Cherry, Erin and Latulipe, Celine},
	month = aug,
	year = {2014},
	keywords = {Creativity support tools, evaluation, psychometrics, surveys},
	pages = {1--25},
	file = {Cherry and Latulipe - 2014 - Quantifying the Creativity Support of Digital Tool.pdf:/Users/eleanorrow/Zotero/storage/9R4WJMS7/Cherry and Latulipe - 2014 - Quantifying the Creativity Support of Digital Tool.pdf:application/pdf},
}

@misc{choiEncodingMusicalStyle2020,
	title = {Encoding {Musical} {Style} with {Transformer} {Autoencoders}},
	url = {http://arxiv.org/abs/1912.05537},
	abstract = {We consider the problem of learning high-level controls over the global structure of generated sequences, particularly in the context of symbolic music generation with complex language models. In this work, we present the Transformer autoencoder, which aggregates encodings of the input data across time to obtain a global representation of style from a given performance. We show it is possible to combine this global representation with other temporally distributed embeddings, enabling improved control over the separate aspects of performance style and melody. Empirically, we demonstrate the effectiveness of our method on various music generation tasks on the MAESTRO dataset and a YouTube dataset with 10,000+ hours of piano performances, where we achieve improvements in terms of log-likelihood and mean listening scores as compared to baselines.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Choi, Kristy and Hawthorne, Curtis and Simon, Ian and Dinculescu, Monica and Engel, Jesse},
	month = jun,
	year = {2020},
	note = {arXiv:1912.05537 [cs, eess, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Generative Music, Models, Overpainting, Statistics - Machine Learning, Style, Transformers},
	annote = {\_eprint: 1912.05537},
	file = {Choi et al. - 2020 - Encoding Musical Style with Transformer Autoencode.pdf:/Users/eleanorrow/Zotero/storage/XBCSCZKI/Choi et al. - 2020 - Encoding Musical Style with Transformer Autoencode.pdf:application/pdf},
}

@misc{chouMidiBERTPianoLargescalePretraining2021,
	title = {{MidiBERT}-{Piano}: {Large}-scale {Pre}-training for {Symbolic} {Music} {Understanding}},
	shorttitle = {{MidiBERT}-{Piano}},
	url = {http://arxiv.org/abs/2107.05223},
	abstract = {This paper presents an attempt to employ the mask language modeling approach of BERT to pre-train a 12-layer Transformer model over 4,166 pieces of polyphonic piano MIDI ﬁles for tackling a number of symbolic-domain discriminative music understanding tasks. These include two note-level classiﬁcation tasks, i.e., melody extraction and velocity prediction, as well as two sequence-level classiﬁcation tasks, i.e., composer classiﬁcation and emotion classiﬁcation. We ﬁnd that, given a pretrained Transformer, our models outperform recurrent neural network based baselines with less than 10 epochs of ﬁne-tuning. Ablation studies show that the pre-training remains effective even if none of the MIDI data of the downstream tasks are seen at the pre-training stage, and that freezing the self-attention layers of the Transformer at the ﬁne-tuning stage slightly degrades performance. All the ﬁve datasets employed in this work are publicly available, as well as checkpoints of our pre-trained and ﬁne-tuned models. As such, our research can be taken as a benchmark for symbolic-domain music understanding.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Chou, Yi-Hui and Chen, I.-Chun and Chang, Chin-Jui and Ching, Joann and Yang, Yi-Hsuan},
	month = jul,
	year = {2021},
	note = {arXiv:2107.05223 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Multimedia, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {arXiv:2107.05223 [cs, eess]},
	file = {Chou et al. - 2021 - MidiBERT-Piano Large-scale Pre-training for Symbo.pdf:/Users/eleanorrow/Zotero/storage/XBM5ILTF/Chou et al. - 2021 - MidiBERT-Piano Large-scale Pre-training for Symbo.pdf:application/pdf},
}

@inproceedings{chuanModelingTemporalTonal2018,
	title = {Modeling {Temporal} {Tonal} {Relations} in {Polyphonic} {Music} through {Deep} {Networks} with a {Novel} {Image}-{Based} {Representation}},
	volume = {32},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11880},
	doi = {10.1609/aaai.v32i1.11880},
	abstract = {We propose an end-to-end approach for modeling polyphonic music with a novel graphical representation, based on music theory, in a deep neural network. Despite the success of deep learning in various applications, it remains a challenge to incorporate existing domain knowledge in a network without affecting its training routines. In this paper we present a novel approach for predictive music modeling and music generation that incorporates domain knowledge in its representation. In this work, music is transformed into a 2D representation, inspired by tonnetz from music theory, which graphically encodes musical relationships between pitches. This representation is incorporated in a deep network structure consisting of multilayered convolutional neural networks (CNN, for learning an efﬁcient abstract encoding of the representation) and recurrent neural networks with long short-term memory cells (LSTM, for capturing temporal dependencies in music sequences). We empirically evaluate the nature and the effectiveness of the network by using a dataset of classical music from various composers. We investigate the effect of parameters including the number of convolution feature maps, pooling strategies, and three conﬁgurations of the network: LSTM without CNN, LSTM with CNN (pre-trained vs. not pre-trained). Visualizations of the feature maps and ﬁlters in the CNN are explored, and a comparison is made between the proposed tonnetz-inspired representation and pianoroll, a commonly used representation of music in computational systems. Experimental results show that the tonnetz representation produces musical sequences that are more tonally stable and contain more repeated patterns than sequences generated by pianoroll-based models, a ﬁnding that is directly useful for tackling current challenges in music and AI such as smart music generation.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Chuan, Ching-Hua and Herremans, Dorien},
	month = apr,
	year = {2018},
	pages = {8},
	file = {Chuan and Herremans - 2018 - Modeling Temporal Tonal Relations in Polyphonic Mu.pdf:/Users/eleanorrow/Zotero/storage/AKKJJX7Z/Chuan and Herremans - 2018 - Modeling Temporal Tonal Relations in Polyphonic Mu.pdf:application/pdf},
}

@article{civitSystematicReviewArtificial2022,
	title = {A {Systematic} {Review} of {Artificial} {Intelligence}-{Based} {Music} {Generation}: {Scope}, {Applications}, and {Future} {Trends}},
	volume = {209},
	issn = {09574174},
	shorttitle = {A systematic review of artificial intelligence-based music generation},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422013537},
	doi = {10.1016/j.eswa.2022.118190},
	abstract = {Currently available reviews in the area of artificial intelligence-based music generation do not provide a wide range of publications and are usually centered around comparing very specific topics between a very limited range of solutions. Best surveys available in the field are bibliography sections of some papers and books which lack a systematic approach and limit their scope to only handpicked examples In this work, we analyze the scope and trends of the research on artificial intelligence-based music generation by performing a systematic review of the available publications in the field using the Prisma methodology. Furthermore, we discuss the possible implementations and accessibility of a set of currently available AI solutions, as aids to musical composition. Our research shows how publications are being distributed globally according to many characteristics, which provides a clear picture of the situation of this technology.},
	language = {en},
	urldate = {2023-12-30},
	journal = {Expert Systems with Applications},
	author = {Civit, Miguel and Civit-Masot, Javier and Cuadrado, Francisco and Escalona, Maria J.},
	month = dec,
	year = {2022},
	keywords = {Artificial intelligence, Assisted music composition, Automatic music generation, Human–machine co-creation, Scoping review},
	pages = {118190},
	file = {Civit et al. - 2022 - A systematic review of artificial intelligence-bas.pdf:/Users/eleanorrow/Zotero/storage/KQRB7S4T/Civit et al. - 2022 - A systematic review of artificial intelligence-bas.pdf:application/pdf},
}

@article{coatesGraphAlgorithmsFeature2018a,
	title = {Graph {Algorithms} for {Feature} {Extraction} in {Music} {Data}},
	abstract = {Music Information Retrieval (MIR) is a broad ﬁeld with various goals such as genre classiﬁcaiton, artist classiﬁcation, and music generation. A common challenge in MIR is how to represent and analyze music digitally. Common representations include digital audio formats, like .mp3 and .wav, and MIDI, which contains more precise note information but lacks information about timbre. Both formats include a large amount of information; machine learning tasks in MIR seek to reduce this information and extract important features. A common approach to condensing the information in audio formats is mel-frequency ceptstrum coeﬃciencs (MFCC). This paper presents a new alternative: a way of modeling MIDI information with a directed graph. This paper demonstrates that eight features extracted from this graph alone are able to achieve .98 accuracy in a binary genre classiﬁcation task and .92 accuracy on a binary artist classiﬁcation task. Furthermore, this paper discusses potential alterations and improvements to the model as well as potential applications to music generation.},
	language = {en},
	author = {Coates, Sebastian},
	year = {2018},
	pages = {1--18},
	file = {Coates - Graph Algorithms for Feature Extraction in Music D.pdf:/Users/eleanorrow/Zotero/storage/8CYPD9TC/Coates - Graph Algorithms for Feature Extraction in Music D.pdf:application/pdf},
}

@article{colomboAlgorithmicCompositionMelodies2016,
	title = {Algorithmic {Composition} of {Melodies} with {Deep} {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1606.07251},
	doi = {10.13140/RG.2.1.2436.5683},
	abstract = {A big challenge in algorithmic composition is to devise a model that is both easily trainable and able to reproduce the long-range temporal dependencies typical of music. Here we investigate how artiﬁcial neural networks can be trained on a large corpus of melodies and turned into automated music composers able to generate new melodies coherent with the style they have been trained on. We employ gated recurrent unit networks that have been shown to be particularly eﬃcient in learning complex sequential activations with arbitrary long time lags. Our model processes rhythm and melody in parallel while modeling the relation between these two features. Using such an approach, we were able to generate interesting complete melodies or suggest possible continuations of a melody fragment that is coherent with the characteristics of the fragment itself.},
	language = {en},
	urldate = {2023-12-30},
	author = {Colombo, Florian and Muscinelli, Samuel P. and Seeholzer, Alexander and Brea, Johanni and Gerstner, Wulfram},
	year = {2016},
	note = {arXiv:1606.07251 [cs, stat]},
	keywords = {algorithmic composition, chine learning, Composition, Computer Science - Machine Learning, deep recurrent neural networks, generative model of music, Generative Music, ma-, Models, Neural Nets, Statistics - Machine Learning},
	annote = {Comment: Proceeding of the 1st Conference on Computer Simulation of Musical Creativity, Huddersfield University},
	annote = {\_eprint: 1606.07251},
	file = {Colombo et al. - 2016 - Algorithmic Composition of Melodies with Deep Recu.pdf:/Users/eleanorrow/Zotero/storage/SSZG3SSR/Colombo et al. - 2016 - Algorithmic Composition of Melodies with Deep Recu.pdf:application/pdf},
}

@article{cookANGELINAVideogameDesign2017,
	title = {The {ANGELINA} {Videogame} {Design} {System}—{Part} {II}},
	volume = {9},
	issn = {1943-068X, 1943-0698},
	url = {https://ieeexplore.ieee.org/document/7442549/},
	doi = {10.1109/TCIAIG.2016.2520305},
	abstract = {Procedural content generation is generally viewed as a means to an end – a tool employed by designers to overcome technical problems or achieve a particular design goal. When we move from generating single parts of games to automating the entirety of their design, however, we ﬁnd ourselves facing a far wider and more interesting set of problems than mere generation. When the designer of a game is a piece of software, we face questions about what it means to be a designer, about Computational Creativity, and about how to assess the growth of these automated game designers and the value of their output. Answering these questions can lead to new ideas in how to generate content procedurally, and produce systems that can further the cutting edge of game design.},
	language = {en},
	number = {3},
	urldate = {2023-12-30},
	journal = {IEEE Transactions on Computational Intelligence and AI in Games},
	author = {Cook, Michael and Colton, Simon and Gow, Jeremy},
	month = sep,
	year = {2017},
	keywords = {curation coefficient},
	pages = {254--266},
	annote = {gives a good description of curation coefficient},
	file = {Cook et al. - 2017 - The ANGELINA Videogame Design System—Part II.pdf:/Users/eleanorrow/Zotero/storage/FE7EQ3CX/Cook et al. - 2017 - The ANGELINA Videogame Design System—Part II.pdf:application/pdf},
}

@article{cooperInterestedAutoethnographyHow2022a,
	title = {I’m {Interested} in {Autoethnography}, but {How} {Do} {I} {Do} {It}?},
	issn = {21603715},
	url = {https://nsuworks.nova.edu/tqr/vol27/iss1/14/},
	doi = {10.46743/2160-3715/2022.5288},
	abstract = {Autoethnography is one of the qualitative research methodologies that remains somewhat mysterious to many scholars. While the use of autoethnography has expanded across numerous fields, it can be difficult to find much guidance about the procedures involved in conducting an autoethnography. Recognizing both the flexibility and creativity inherent in autoethnography, as well as the need for rigor in achieving meaningful research results, we offer in this article some suggestions and reflections regarding the process of conducting an autoethnography – from developing the research question to reporting the findings. These recommendations draw from both narrative and ethnographic research methodologies, as well as descriptive and arts-based approaches. This discussion may serve as a resource for those interested in teaching and conducting autoethnography.},
	language = {en},
	urldate = {2023-12-30},
	journal = {The Qualitative Report},
	author = {Cooper, Robin and Lilyea, Bruce},
	year = {2022},
	file = {Cooper and Lilyea - 2022 - I’m Interested in Autoethnography, but How Do I Do.pdf:/Users/eleanorrow/Zotero/storage/LBRH5RHJ/Cooper and Lilyea - 2022 - I’m Interested in Autoethnography, but How Do I Do.pdf:application/pdf},
}

@inproceedings{crabtreeSupportingEthnographicStudies2006,
	address = {University Park PA USA},
	title = {Supporting ethnographic studies of ubiquitous computing in the wild},
	isbn = {978-1-59593-367-6},
	url = {https://dl.acm.org/doi/10.1145/1142405.1142417},
	doi = {10.1145/1142405.1142417},
	abstract = {Ethnography has become a staple feature of IT research over the last twenty years, shaping our understanding of the social character of computing systems and informing their design in a wide variety of settings. The emergence of ubiquitous computing raises new challenges for ethnography however, distributing interaction across a burgeoning array of small, mobile devices and online environments which exploit invisible sensing systems. Understanding interaction requires ethnographers to reconcile interactions that are, for example, distributed across devices on the street with online interactions in order to assemble coherent understandings of the social character and purchase of ubiquitous computing systems. We draw upon four recent studies to show how ethnographers are replaying system recordings of interaction alongside existing resources such as video recordings to do this and identify key challenges that need to be met to support ethnographic study of ubiquitous computing in the wild.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {Proceedings of the 6th conference on {Designing} {Interactive} systems},
	publisher = {ACM},
	author = {Crabtree, Andy and Benford, Steve and Greenhalgh, Chris and Tennent, Paul and Chalmers, Matthew and Brown, Barry},
	month = jun,
	year = {2006},
	pages = {60--69},
	annote = {event-place: University Park, PA, USA},
	file = {Crabtree et al. - 2006 - Supporting ethnographic studies of ubiquitous comp.pdf:/Users/eleanorrow/Zotero/storage/S7ZVIQ33/Crabtree et al. - 2006 - Supporting ethnographic studies of ubiquitous comp.pdf:application/pdf},
}

@inproceedings{daiTransformerXLAttentiveLanguage2019a,
	address = {Florence, Italy},
	title = {Transformer-{XL}: {Attentive} {Language} {Models} beyond a {Fixed}-{Length} {Context}},
	shorttitle = {Transformer-{XL}},
	url = {https://www.aclweb.org/anthology/P19-1285},
	doi = {10.18653/v1/P19-1285},
	abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a ﬁxed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a ﬁxed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, TransformerXL learns dependency that is 80\% longer than RNNs and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-ofthe-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without ﬁnetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorﬂow and PyTorch1.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc and Salakhutdinov, Ruslan},
	month = jul,
	year = {2019},
	pages = {2978--2988},
	file = {Dai et al. - 2019 - Transformer-XL Attentive Language Models beyond a.pdf:/Users/eleanorrow/Zotero/storage/PNS8NXSH/Dai et al. - 2019 - Transformer-XL Attentive Language Models beyond a.pdf:application/pdf},
}

@article{davisCoCreativeDrawingAgent2021b,
	title = {Co-{Creative} {Drawing} {Agent} with {Object} {Recognition}},
	volume = {12},
	issn = {2334-0924, 2326-909X},
	url = {https://ojs.aaai.org/index.php/AIIDE/article/view/12863},
	doi = {10.1609/aiide.v12i1.12863},
	abstract = {This paper describes an updated version of a co-creative drawing system called the Drawing Apprentice. The system collaborates with users by analyzing their drawn input and responding in a real time dialogical and improvisational interaction. The current system includes an object recognition module that employs deep learning to classify sketched objects. The system architecture and implementation are described along with its evaluation during a public demonstration during which artists, non-artists, and designers provided feedback about the experience interacting with the system.},
	language = {en},
	number = {1},
	urldate = {2023-12-30},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
	author = {Davis, Nicholas and Hsiao, Chih-Pin and Singh, Kunwar Yashraj and Magerko, Brian},
	month = jun,
	year = {2021},
	pages = {9--15},
	file = {Davis et al. - 2021 - Co-Creative Drawing Agent with Object Recognition.pdf:/Users/eleanorrow/Zotero/storage/SMX9KN46/Davis et al. - 2021 - Co-Creative Drawing Agent with Object Recognition.pdf:application/pdf},
}

@misc{demirPatchBasedImageInpainting2018,
	title = {Patch-{Based} {Image} {Inpainting} with {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1803.07422},
	abstract = {Area of image inpainting over relatively large missing regions recently advanced substantially through adaptation of dedicated deep neural networks. However, current network solutions still introduce undesired artifacts and noise to the repaired regions. We present an image inpainting method that is based on the celebrated generative adversarial network (GAN) framework. The proposed PGGAN method includes a discriminator network that combines a global GAN (G-GAN) architecture with a patchGAN approach. PGGAN ﬁrst shares network layers between G-GAN and patchGAN, then splits paths to produce two adversarial losses that feed the generator network in order to capture both local continuity of image texture and pervasive global features in images. The proposed framework is evaluated extensively, and the results including comparison to recent state-of-the-art demonstrate that it achieves considerable improvements on both visual and quantitative evaluations.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Demir, Ugur and Unal, Gozde},
	month = mar,
	year = {2018},
	note = {arXiv:1803.07422 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Demir and Unal - 2018 - Patch-Based Image Inpainting with Generative Adver.pdf:/Users/eleanorrow/Zotero/storage/P6BZ2Q5B/Demir and Unal - 2018 - Patch-Based Image Inpainting with Generative Adver.pdf:application/pdf},
}

@misc{dengContinuousLatentProcess2021,
	title = {Continuous {Latent} {Process} {Flows}},
	url = {http://arxiv.org/abs/2106.15580},
	abstract = {Partial observations of continuous time-series dynamics at arbitrary time stamps exist in many disciplines. Fitting this type of data using statistical models with continuous dynamics is not only promising at an intuitive level but also has practical beneﬁts, including the ability to generate continuous trajectories and to perform inference on previously unseen time stamps. Despite exciting progress in this area, the existing models still face challenges in terms of their representation power and the quality of their variational approximations. We tackle these challenges with continuous latent process ﬂows (CLPF), a principled architecture decoding continuous latent processes into continuous observable processes using a time-dependent normalizing ﬂow driven by a stochastic differential equation. To optimize our model using maximum likelihood, we propose a novel piecewise construction of a variational posterior process and derive the corresponding variational lower bound using importance weighting of trajectories. An ablation study demonstrates the effectiveness of our contributions and comparisons to state-of-the-art baselines show our model’s favourable performance on both synthetic and real-world data.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Deng, Ruizhi and Brubaker, Marcus A. and Mori, Greg and Lehrmann, Andreas M.},
	month = oct,
	year = {2021},
	note = {arXiv:2106.15580 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {arXiv:2106.15580 [cs, stat]},
	annote = {Comment: Accepted to NeurIPS 2021},
	annote = {Comment: Accepted to NeurIPS 2021},
	file = {Deng et al. - 2021 - Continuous Latent Process Flows.pdf:/Users/eleanorrow/Zotero/storage/G6EENNH3/Deng et al. - 2021 - Continuous Latent Process Flows.pdf:application/pdf},
}

@misc{dongMusPyToolkitSymbolic2020,
	title = {{MusPy}: {A} {Toolkit} for {Symbolic} {Music} {Generation}},
	shorttitle = {{MusPy}},
	url = {http://arxiv.org/abs/2008.01951},
	abstract = {In this paper, we present MusPy, an open source Python library for symbolic music generation. MusPy provides easy-to-use tools for essential components in a music generation system, including dataset management, data I/O, data preprocessing and model evaluation. In order to showcase its potential, we present statistical analysis of the eleven datasets currently supported by MusPy. Moreover, we conduct a cross-dataset generalizability experiment by training an autoregressive model on each dataset and measuring held-out likelihood on the others—a process which is made easier by MusPy’s dataset management system. The results provide a map of domain overlap between various commonly used datasets and show that some datasets contain more representative cross-genre samples than others. Along with the dataset analysis, these results might serve as a guide for choosing datasets in future research. Source code and documentation are available at https://github.com/salu133445/muspy.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Dong, Hao-Wen and Chen, Ke and McAuley, Julian and Berg-Kirkpatrick, Taylor},
	month = aug,
	year = {2020},
	note = {arXiv:2008.01951 [cs, eess, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	annote = {arXiv: 2008.01951},
	annote = {Comment: Accepted by International Society for Music Information Retrieval Conference (ISMIR), 2020},
	file = {Dong et al. - 2020 - MusPy A Toolkit for Symbolic Music Generation.pdf:/Users/eleanorrow/Zotero/storage/5ELJWDSK/Dong et al. - 2020 - MusPy A Toolkit for Symbolic Music Generation.pdf:application/pdf},
}

@misc{dyerNotesNoiseContrastive2014,
	title = {Notes on {Noise} {Contrastive} {Estimation} and {Negative} {Sampling}},
	url = {http://arxiv.org/abs/1410.8251},
	abstract = {Estimating the parameters of probabilistic models of language such as maxent models and probabilistic neural models is computationally difﬁcult since it involves evaluating partition functions by summing over an entire vocabulary, which may be millions of word types in size. Two closely related strategies—noise contrastive estimation (Mnih and Teh, 2012; Mnih and Kavukcuoglu, 2013; Vaswani et al., 2013) and negative sampling (Mikolov et al., 2012; Goldberg and Levy, 2014)—have emerged as popular solutions to this computational problem, but some confusion remains as to which is more appropriate and when. This document explicates their relationships to each other and to other estimation techniques. The analysis shows that, although they are superﬁcially similar, NCE is a general parameter estimation technique that is asymptotically unbiased, while negative sampling is best understood as a family of binary classiﬁcation models that are useful for learning word representations but not as a general-purpose estimator.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Dyer, Chris},
	month = oct,
	year = {2014},
	note = {arXiv:1410.8251 [cs]},
	keywords = {Computer Science - Machine Learning, Contrastive Learning, Loss Function, NCE, NCE Loss},
	annote = {Comment: 4 pages},
	annote = {\_eprint: 1410.8251},
	file = {Dyer - 2014 - Notes on Noise Contrastive Estimation and Negative.pdf:/Users/eleanorrow/Zotero/storage/5HP5V45L/Dyer - 2014 - Notes on Noise Contrastive Estimation and Negative.pdf:application/pdf},
}

@misc{ensMMMExploringConditional2020,
	title = {{MMM} : {Exploring} {Conditional} {Multi}-{Track} {Music} {Generation} with the {Transformer}},
	shorttitle = {{MMM}},
	url = {http://arxiv.org/abs/2008.06048},
	doi = {10.48550/arXiv.2008.06048},
	abstract = {We propose the Multi-Track Music Machine (MMM), a generative system based on the Transformer architecture that is capable of generating multi-track music. In contrast to previous work, which represents musical material as a single time-ordered sequence, where the musical events corresponding to diﬀerent tracks are interleaved, we create a time-ordered sequence of musical events for each track and concatenate several tracks into a single sequence. This takes advantage of the Transformer’s attention-mechanism, which can adeptly handle longterm dependencies. We explore how various representations can oﬀer the user a high degree of control at generation time, providing an interactive demo that accommodates track-level and bar-level inpainting, and oﬀers control over track instrumentation and note density.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Ens, Jeff and Pasquier, Philippe},
	month = aug,
	year = {2020},
	note = {arXiv:2008.06048 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Multimedia, Computer Science - Sound, music generation, transformer},
	annote = {arXiv:2008.06048 [cs]},
	annote = {arXiv:2008.06048 [cs] type: article},
	file = {Ens and Pasquier - 2020 - MMM  Exploring Conditional Multi-Track Music Gene.pdf:/Users/eleanorrow/Zotero/storage/5ZUITKG3/Ens and Pasquier - 2020 - MMM  Exploring Conditional Multi-Track Music Gene.pdf:application/pdf},
}

@misc{falconFrameworkContrastiveSelfSupervised2020,
	title = {A {Framework} {For} {Contrastive} {Self}-{Supervised} {Learning} {And} {Designing} {A} {New} {Approach}},
	url = {http://arxiv.org/abs/2009.00104},
	abstract = {Contrastive self-supervised learning (CSL) is an approach to learn useful representations by solving a pretext task which selects and compares anchor, negative and positive (APN) features from an unlabeled dataset. We present a conceptual framework which characterizes CSL approaches in ﬁve aspects (1) data augmentation pipeline, (2) encoder selection, (3) representation extraction, (4) similarity measure, and (5) loss function. We analyze three leading CSL approaches–AMDIM, CPC and SimCLR–, and show that despite different motivations, they are special cases under this framework. We show the utility of our framework by designing Yet Another DIM (YADIM) which achieves competitive results on CIFAR-10, STL-10 and ImageNet, and is more robust to the choice of encoder and the representation extraction strategy. To support ongoing CSL research, we release the PyTorch implementation of this conceptual framework along with standardized implementations of AMDIM, CPC (V2), SimCLR, BYOL, Moco (V2) and YADIM.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Falcon, William and Cho, Kyunghyun},
	month = aug,
	year = {2020},
	note = {arXiv:2009.00104 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {\_eprint: 2009.00104},
	annote = {FRAMEWORK FOR CONTRASTIVE LEARNING APPROACHES. See accompanying blog: https://towardsdatascience.com/a-framework-for-contrastive-self-supervised-learning-and-designing-a-new-approach-3caab5d29619 Looks at CPC, SimCLR and AMDIM approaches to contrastive learning.},
	file = {Falcon and Cho - 2020 - A Framework For Contrastive Self-Supervised Learni.pdf:/Users/eleanorrow/Zotero/storage/NI6PGLQR/Falcon and Cho - 2020 - A Framework For Contrastive Self-Supervised Learni.pdf:application/pdf},
}

@misc{ferreiraComputerGeneratedMusicTabletop2020,
	title = {Computer-{Generated} {Music} for {Tabletop} {Role}-{Playing} {Games}},
	url = {http://arxiv.org/abs/2008.07009},
	abstract = {In this paper we present Bardo Composer, a system to generate background music for tabletop role-playing games. Bardo Composer uses a speech recognition system to translate player speech into text, which is classiﬁed according to a model of emotion. Bardo Composer then uses Stochastic BiObjective Beam Search, a variant of Stochastic Beam Search that we introduce in this paper, with a neural model to generate musical pieces conveying the desired emotion. We performed a user study with 116 participants to evaluate whether people are able to correctly identify the emotion conveyed in the pieces generated by the system. In our study we used pieces generated for Call of the Wild, a Dungeons and Dragons campaign available on YouTube. Our results show that human subjects could correctly identify the emotion of the generated music pieces as accurately as they were able to identify the emotion of pieces written by humans.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Ferreira, Lucas N. and Lelis, Levi H. S. and Whitehead, Jim},
	month = aug,
	year = {2020},
	note = {arXiv:2008.07009 [cs, eess, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	annote = {Comment: To be published in the 16th AAAI Conference ON Artificial Intelligence and Interactive Digital Entertainment},
	annote = {Issue: arXiv:2008.07009 arXiv: 2008.07009},
	file = {Ferreira et al. - 2020 - Computer-Generated Music for Tabletop Role-Playing.pdf:/Users/eleanorrow/Zotero/storage/C52QRTIB/Ferreira et al. - 2020 - Computer-Generated Music for Tabletop Role-Playing.pdf:application/pdf},
}

@misc{ferreiraLearningGenerateMusic2021,
	title = {Learning to {Generate} {Music} {With} {Sentiment}},
	url = {http://arxiv.org/abs/2103.06125},
	abstract = {Deep Learning models have shown very promising results in automatically composing polyphonic music pieces. However, it is very hard to control such models in order to guide the compositions towards a desired goal. We are interested in controlling a model to automatically generate music with a given sentiment. This paper presents a generative Deep Learning model that can be directed to compose music with a given sentiment. Besides music generation, the same model can be used for sentiment analysis of symbolic music. We evaluate the accuracy of the model in classifying sentiment of symbolic music using a new dataset of video game soundtracks. Results show that our model is able to obtain good prediction accuracy. A user study shows that human subjects agreed that the generated music has the intended sentiment, however negative pieces can be ambiguous.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Ferreira, Lucas N. and Whitehead, Jim},
	month = mar,
	year = {2021},
	note = {arXiv:2103.06125 [cs, eess]},
	keywords = {Computer Science - Information Retrieval, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: International Society for Music Information Retrieval (2019)},
	annote = {Issue: arXiv:2103.06125 arXiv: 2103.06125},
	file = {Ferreira and Whitehead - 2021 - Learning to Generate Music With Sentiment.pdf:/Users/eleanorrow/Zotero/storage/ALHLBHSP/Ferreira and Whitehead - 2021 - Learning to Generate Music With Sentiment.pdf:application/pdf},
}

@misc{fiebrinkMachineLearningAlgorithm2016,
	title = {The {Machine} {Learning} {Algorithm} as {Creative} {Musical} {Tool}},
	url = {http://arxiv.org/abs/1611.00379},
	abstract = {Machine learning is the capacity of a computational system to learn structures from datasets in order to make predictions on newly seen data. Such an approach oﬀers a signiﬁcant advantage in music scenarios in which musicians can teach the system to learn an idiosyncratic style, or can break the rules to explore the system’s capacity in unexpected ways. In this chapter we draw on music, machine learning, and humancomputer interaction to elucidate an understanding of machine learning algorithms as creative tools for music and the sonic arts. We motivate a new understanding of learning algorithms as human-computer interfaces. We show that, like other interfaces, learning algorithms can be characterised by the ways their aﬀordances intersect with goals of human users. We also argue that the nature of interaction between users and algorithms impacts the usability and usefulness of those algorithms in profound ways. This human-centred view of machine learning motivates our concluding discussion of what it means to employ machine learning as a creative tool.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Fiebrink, Rebecca and Caramiaux, Baptiste},
	month = nov,
	year = {2016},
	note = {arXiv:1611.00379 [cs]},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
	annote = {arXiv:1611.00379 [cs]},
	annote = {Comment: Pre-print to appear in the Oxford Handbook on Algorithmic Music. Oxford University Press},
	annote = {Comment: Pre-print to appear in the Oxford Handbook on Algorithmic Music. Oxford University Press},
	file = {Fiebrink and Caramiaux - 2016 - The Machine Learning Algorithm as Creative Musical.pdf:/Users/eleanorrow/Zotero/storage/2QGCZRFA/Fiebrink and Caramiaux - 2016 - The Machine Learning Algorithm as Creative Musical.pdf:application/pdf},
}

@inproceedings{foxwellComposingAlgorithmsTwo2012,
	address = {Corfu Greece},
	title = {Composing with algorithms: two novel generative composition tools},
	isbn = {978-1-4503-1569-2},
	shorttitle = {Composing with algorithms},
	url = {https://dl.acm.org/doi/10.1145/2371456.2371468},
	doi = {10.1145/2371456.2371468},
	abstract = {Most composers may at some point struggle with the creative process. Breaking frHH  IURP  ZULWHU¶V  EORFN  DQG  WKH  FRPSRVHU¶V  own limitations can be achieved through the use of algorithmic approaches to music creation. This project is aimed at development of two algorithmic compositional tools, which can be used within the Digital Audio Workstation Ableton with Max for Live software. The first employs 2nd order Markov chains to aid composers in creation of unique midi melodies. The second approach uses Cellular Automata to select loop points in digital audio files, shuffling audio segments in time to create new patterns. Results suggest these systems have the potential to inspire composers, allow them to augment their compositions, and in turn help them overcome creative problems.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {Proceedings of the 7th {Audio} {Mostly} {Conference}: {A} {Conference} on {Interaction} with {Sound}},
	publisher = {ACM},
	author = {Foxwell, Iain and Knox, Don},
	month = sep,
	year = {2012},
	pages = {76--81},
	annote = {event-place: Corfu, Greece},
	file = {Foxwell and Knox - 2012 - Composing with algorithms two novel generative co.pdf:/Users/eleanorrow/Zotero/storage/QWM528KS/Foxwell and Knox - 2012 - Composing with algorithms two novel generative co.pdf:application/pdf},
}

@article{fradetMIDITOKPYTHONPACKAGE?,
	title = {{MIDITOK}: {A} {PYTHON} {PACKAGE} {FOR} {MIDI} {FILE} {TOKENIZATION}},
	abstract = {This article presents MidiTok, a Python package to encode MIDI ﬁles into sequences of tokens to be used with sequential Deep Learning models like Transformers or Recurrent Neural Networks. It allows researchers and developers to encode datasets with various strategies built around the idea that they share common parameters. This key idea makes it easy to :1) optimize the size of the vocabulary and the elements it can represent w.r.t. the MIDI speciﬁcations; 2) compare tokenization methods to see which performs best in which case; 3) measure the relevance of additional information like chords or tempo changes. Code and documentation of MidiTok are on Github 1 .},
	language = {en},
	journal = {?},
	author = {Fradet, Nathan and Briot, Jean-Pierre and Chhel, Fabien},
	pages = {3},
	file = {Fradet et al. - MIDITOK A PYTHON PACKAGE FOR MIDI FILE TOKENIZATI.pdf:/Users/eleanorrow/Zotero/storage/4UZYEGAJ/Fradet et al. - MIDITOK A PYTHON PACKAGE FOR MIDI FILE TOKENIZATI.pdf:application/pdf},
}

@inproceedings{frichMappingLandscapeCreativity2019,
	address = {Glasgow Scotland Uk},
	title = {Mapping the {Landscape} of {Creativity} {Support} {Tools} in {HCI}},
	isbn = {978-1-4503-5970-2},
	url = {https://dl.acm.org/doi/10.1145/3290605.3300619},
	doi = {10.1145/3290605.3300619},
	abstract = {Creativity Support Tools (CSTs) play a fundamental role in the study of creativity in Human-Computer Interaction (HCI). Even so, there is no consensus definition of the term ‘CST’ in HCI, and in most studies, CSTs have been construed as oneoff exploratory prototypes, typically built by the researchers themselves. This makes it difficult to clearly demarcate CST research, but also to compare findings across studies, which impedes advancement in digital creativity as a growing field of research. Based on a literature review of 143 papers from the ACM Digital Library (1999-2018), we contribute a first overview of the key characteristics of CSTs developed by the HCI community. Moreover, we propose a tentative definition of a CST to help strengthen knowledge sharing across CST studies. We end by discussing our study’s implications for future HCI research on CSTs and digital creativity.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Frich, Jonas and MacDonald Vermeulen, Lindsay and Remy, Christian and Biskjaer, Michael Mose and Dalsgaard, Peter},
	month = may,
	year = {2019},
	pages = {1--18},
	file = {Frich et al. - 2019 - Mapping the Landscape of Creativity Support Tools .pdf:/Users/eleanorrow/Zotero/storage/B6JXWZR5/Frich et al. - 2019 - Mapping the Landscape of Creativity Support Tools .pdf:application/pdf},
}

@misc{galeStateSparsityDeep2019,
	title = {The {State} of {Sparsity} in {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1902.09574},
	abstract = {We rigorously evaluate three state-of-the-art techniques for inducing sparsity in deep neural networks on two large-scale learning tasks: Transformer trained on WMT 2014 English-to-German, and ResNet-50 trained on ImageNet. Across thousands of experiments, we demonstrate that complex techniques (Molchanov et al., 2017; Louizos et al., 2017b) shown to yield high compression rates on smaller datasets perform inconsistently, and that simple magnitude pruning approaches achieve comparable or better results. Based on insights from our experiments, we achieve a new state-of-the-art sparsity-accuracy trade-off for ResNet-50 using only magnitude pruning. Additionally, we repeat the experiments performed by Frankle \& Carbin (2018) and Liu et al. (2018) at scale and show that unstructured sparse architectures learned through pruning cannot be trained from scratch to the same test set performance as a model trained with joint sparsiﬁcation and optimization. Together, these results highlight the need for large-scale benchmarks in the ﬁeld of model compression. We open-source our code, top performing model checkpoints, and results of all hyperparameter conﬁgurations to establish rigorous baselines for future work on compression and sparsiﬁcation.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Gale, Trevor and Elsen, Erich and Hooker, Sara},
	month = feb,
	year = {2019},
	note = {arXiv:1902.09574 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {\_eprint: 1902.09574},
	file = {Gale et al. - 2019 - The State of Sparsity in Deep Neural Networks.pdf:/Users/eleanorrow/Zotero/storage/QIIGHXEI/Gale et al. - 2019 - The State of Sparsity in Deep Neural Networks.pdf:application/pdf},
}

@inproceedings{gaverAmbiguityResourceDesign2003,
	address = {Ft. Lauderdale Florida USA},
	title = {Ambiguity as a resource for design},
	isbn = {978-1-58113-630-2},
	url = {https://dl.acm.org/doi/10.1145/642611.642653},
	doi = {10.1145/642611.642653},
	abstract = {Ambiguity is usually considered anathema in Human Computer Interaction. We argue, in contrast, that it is a resource for design that can be used to encourage close personal engagement with systems. We illustrate this with examples from contemporary arts and design practice, and distinguish three broad classes of ambiguity according to where they are located in the interpretative relationship linking person and artefact. Ambiguity of information finds its source in the artefact itself, ambiguity of context in the sociocultural discourses that are used to interpret it, and ambiguity of relationship in the interpretative and evaluative stance of the individual. For each of these categories, we describe tactics for emphasising ambiguity that may help designers and other practitioners understand and craft its use.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Gaver, William W. and Beaver, Jacob and Benford, Steve},
	month = apr,
	year = {2003},
	keywords = {ambiguity},
	pages = {233--240},
	file = {Gaver et al. - 2003 - Ambiguity as a resource for design.pdf:/Users/eleanorrow/Zotero/storage/XZTDEFRN/Gaver et al. - 2003 - Ambiguity as a resource for design.pdf:application/pdf},
}

@article{georgesWesternClassicalMusic2017,
	title = {Western classical music development: a statistical analysis of composers similarity, differentiation and evolution},
	volume = {112},
	issn = {0138-9130, 1588-2861},
	shorttitle = {Western classical music development},
	url = {https://doi.org/10.1007/s11192-017-2387-x},
	doi = {10.1007/s11192-017-2387-x},
	abstract = {This paper proposes a statistical analysis that captures similarities and differences between classical music composers with the eventual aim to understand why particular composers ‘sound’ different even if their ‘lineages’ (inﬂuences network) are similar or why they ‘sound’ alike if their ‘lineages’ are different. In order to do this we use statistical methods and measures of association or similarity (based on presence/absence of traits such as speciﬁc ‘ecological’ characteristics and personal musical inﬂuences) that have been developed in biosystematics, scientometrics, and bibliographic coupling. This paper also represents a ﬁrst step towards a more ambitious goal of developing an evolutionary model of Western classical music.},
	language = {en},
	number = {1},
	urldate = {2023-12-30},
	journal = {Scientometrics},
	author = {Georges, Patrick},
	month = jul,
	year = {2017},
	keywords = {Classical composers, Differentiation, Evolution, Imitation, Influences network, Similarity indices},
	pages = {21--53},
	file = {Georges - 2017 - Western classical music development a statistical.pdf:/Users/eleanorrow/Zotero/storage/XEZBI99D/Georges - 2017 - Western classical music development a statistical.pdf:application/pdf},
}

@incollection{ghediniCreatingMusicTexts2016,
	address = {Singapore},
	title = {Creating {Music} and {Texts} with {Flow} {Machines}},
	isbn = {978-981-287-617-1 978-981-287-618-8},
	url = {https://link.springer.com/10.1007/978-981-287-618-8_18},
	abstract = {This chapter introduces the vision and the technical challenges of the Flow Machines project. Flow Machines aim at fostering creativity in artistic domains such as music and literature. We first observe that typically, great artists do not output just single artefacts but develop novel, individual styles. Style mirrors an individual’s uniqueness; style makes an artist's work recognised and recognisable. Artists develop their own style after prolonged periods of imitation and exploration of the style of others. We envision style exploration as the application of existing styles, considered as texture, to arbitrary constraints, considered as structure. The goal of Flow Machines is to assist this process by allowing users to explicitly manipulate styles as computational objects. During interactions with Flow Machines, the user can create artefacts (melodies, texts, orchestrations) by combining styles with arbitrary constraints. Style exploration under user-defined constraints raises complex sequence generation issues that were addressed and solved for the most part during the first half of the project. We illustrate the potential of these techniques for style exploration with three examples.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {Multidisciplinary {Contributions} to the {Science} of {Creative} {Thinking}},
	publisher = {Springer Singapore},
	author = {Ghedini, Fiammetta and Pachet, François and Roy, Pierre},
	editor = {Corazza, Giovanni Emanuele and Agnoli, Sergio},
	year = {2016},
	doi = {10.1007/978-981-287-618-8_18},
	note = {Series Title: Creativity in the Twenty First Century},
	keywords = {Flow Machines, Inpainting, Models},
	pages = {325--343},
	annote = {ISBN: 9789812876188},
	file = {Ghedini et al. - 2016 - Creating Music and Texts with Flow Machines.pdf:/Users/eleanorrow/Zotero/storage/T9AA8F3B/Ghedini et al. - 2016 - Creating Music and Texts with Flow Machines.pdf:application/pdf},
}

@inproceedings{gillickWhatPlayHow2021,
	address = {Shanghai, China},
	title = {What to {Play} and {How} to {Play} it: {Guiding} {Generative} {Music} {Models} with {Multiple} {Demonstrations}},
	shorttitle = {What to {Play} and {How} to {Play} it},
	url = {https://nime.pubpub.org/pub/s3x60926},
	doi = {10.21428/92fbeb44.06e2d5f4},
	abstract = {We propose and evaluate an approach to incorporating multiple user-provided inputs, each demonstrating a complementary set of musical characteristics, to guide the output of a generative model for synthesizing short music performances or loops. We focus on user inputs that describe both “what to play” (via scores in MIDI format) and “how to play it” (via rhythmic inputs to specify expressive timing and dynamics). Through experiments, we demonstrate that our method can facilitate human-AI cocreation of drum loops with diverse and customizable outputs. In the process, we argue for the interaction paradigm of mapping by demonstration as a promising approach to working with deep learning models that are capable of generating complex and realistic musical parts.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {{NIME} 2021},
	publisher = {PubPub},
	author = {Gillick, Jon and Bamman, David},
	month = jun,
	year = {2021},
	file = {Gillick and Bamman - 2021 - What to Play and How to Play it Guiding Generativ.pdf:/Users/eleanorrow/Zotero/storage/U3THEYIQ/Gillick and Bamman - 2021 - What to Play and How to Play it Guiding Generativ.pdf:application/pdf},
}

@article{goodfellowGenerativeAdversarialNetworks2020a,
	title = {Generative adversarial networks},
	volume = {63},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3422622},
	doi = {10.1145/3422622},
	abstract = {Generative adversarial networks are a kind of artificial intelligence algorithm designed to solve the generative modeling problem. The goal of a generative model is to study a collection of training examples and learn the probability distribution that generated them. Generative Adversarial Networks (GANs) are then able to generate more examples from the estimated probability distribution. Generative models based on deep learning are common, but GANs are among the most successful generative models (especially in terms of their ability to generate realistic highresolution images). GANs have been successfully applied to a wide variety of tasks (mostly in research settings) but continue to present unique challenges and research opportunities because they are based on game theory while most other approaches to generative modeling are based on optimization.},
	language = {en},
	number = {11},
	urldate = {2023-12-30},
	journal = {Communications of the ACM},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = oct,
	year = {2020},
	pages = {139--144},
	annote = {\_eprint: 1406.2661},
	file = {Goodfellow et al. - 2020 - Generative adversarial networks.pdf:/Users/eleanorrow/Zotero/storage/UJZ3QWVY/Goodfellow et al. - 2020 - Generative adversarial networks.pdf:application/pdf},
}

@misc{guoInteractiveMusicInfilling2022,
	title = {An interactive music infilling interface for pop music composition},
	url = {http://arxiv.org/abs/2203.12736},
	abstract = {Artificial intelligence (AI) has been widely applied to music generation topics such as continuation, melody/harmony generation, genre transfer and music infilling application. Although with the burst interest to apply AI to music, there are still few interfaces for the musicians to take advantage of the latest progress of the AI technology. This makes those tools less valuable in practice and harder to find its advantage/drawbacks without utilizing them in the real scenario. This work builds a max patch for interactive music infilling application with different levels of control, including track density/polyphony/occupation rate and bar tonal tension control. The user can select the melody/bass/harmony track as the infilling content up to 16 bars. The infilling algorithm is based on the author's previous work, and the interface sends/receives messages to the AI system hosted in the cloud. This interface lowers the barrier of AI technology and can generate different variations of the selected content. Those results can give several alternatives to the musicians' composition, and the interactive process realizes the value of the AI infilling system.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Guo, Rui},
	month = mar,
	year = {2022},
	note = {arXiv:2203.12736 [cs, eess]},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Multimedia, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, inpainting, maxpatch},
	annote = {Issue: arXiv:2203.12736 Publisher: arXiv arXiv: 2203.12736},
	file = {Guo - 2022 - An interactive music infilling interface for pop m.pdf:/Users/eleanorrow/Zotero/storage/EUDE792V/Guo - 2022 - An interactive music infilling interface for pop m.pdf:application/pdf},
}

@misc{guoMusIACExtensibleGenerative2022,
	title = {{MusIAC}: {An} extensible generative framework for {Music} {Infilling} {Applications} with multi-level {Control}},
	shorttitle = {{MusIAC}},
	url = {http://arxiv.org/abs/2202.05528},
	abstract = {We present a novel music generation framework for music inﬁlling, with a user friendly interface. Inﬁlling refers to the task of generating musical sections given the surrounding multi-track music. The proposed transformer-based framework is extensible for new control tokens as the added music control tokens such as tonal tension per bar and track polyphony level in this work. We explore the eﬀects of including several musically meaningful control tokens, and evaluate the results using objective metrics related to pitch and rhythm. Our results demonstrate that adding additional control tokens helps to generate music with stronger stylistic similarities to the original music. It also provides the user with more control to change properties like the music texture and tonal tension in each bar compared to previous research which only provided control for track density. We present the model in a Google Colab notebook to enable interactive generation.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Guo, Rui and Simpson, Ivor and Kiefer, Chris and Magnusson, Thor and Herremans, Dorien},
	month = feb,
	year = {2022},
	note = {arXiv:2202.05528 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multimedia, con-, infilling, music control, music generation, music representation, music transformer, trollable generation},
	annote = {Comment: preprint for The 11th International Conference on Artificial Intelligence in Music, Sound, Art and Design (EvoMUSART) 2022},
	annote = {\_eprint: 2202.05528},
	file = {Guo et al. - 2022 - MusIAC An extensible generative framework for Mus.pdf:/Users/eleanorrow/Zotero/storage/IZWBUYIZ/Guo et al. - 2022 - MusIAC An extensible generative framework for Mus.pdf:application/pdf},
}

@inproceedings{guzdialFriendCollaboratorStudent2019,
	address = {Glasgow Scotland Uk},
	title = {Friend, {Collaborator}, {Student}, {Manager}: {How} {Design} of an {AI}-{Driven} {Game} {Level} {Editor} {Affects} {Creators}},
	isbn = {978-1-4503-5970-2},
	shorttitle = {Friend, {Collaborator}, {Student}, {Manager}},
	url = {https://dl.acm.org/doi/10.1145/3290605.3300854},
	doi = {10.1145/3290605.3300854},
	abstract = {Machine learning advances have afforded an increase in algorithms capable of creating art, music, stories, games, and more. However, it is not yet well-understood how machine learning algorithms might best collaborate with people to support creative expression. To investigate how practicing designers perceive the role of AI in the creative process, we developed a game level design tool for Super Mario Bros.style games with a built-in AI level designer. In this paper we discuss our design of the Morai Maker intelligent tool through two mixed-methods studies with a total of over onehundred participants. Our findings are as follows: (1) level designers vary in their desired interactions with, and role of, the AI, (2) the AI prompted the level designers to alter their design practices, and (3) the level designers perceived the AI as having potential value in their design practice, varying based on their desired role for the AI.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Guzdial, Matthew and Liao, Nicholas and Chen, Jonathan and Chen, Shao-Yu and Shah, Shukan and Shah, Vishwa and Reno, Joshua and Smith, Gillian and Riedl, Mark O.},
	month = may,
	year = {2019},
	keywords = {Artificial intelligence, Human computer collaboration, Human-AI interaction},
	pages = {1--13},
	annote = {ISBN: 9781450359702 \_eprint: 1901.06417},
	file = {Guzdial et al. - 2019 - Friend, Collaborator, Student, Manager How Design.pdf:/Users/eleanorrow/Zotero/storage/TJ2U2266/Guzdial et al. - 2019 - Friend, Collaborator, Student, Manager How Design.pdf:application/pdf},
}

@phdthesis{hadjeresInteractiveDeepGenerative2018,
	title = {Interactive {Deep} {Generative} {Models} for {Symbolic} {Music}},
	language = {en},
	school = {Sorbonne Université},
	author = {Hadjeres, Gaëtan},
	year = {2018},
	file = {Hadjeres - Interactive deep generative models for symbolic mu.pdf:/Users/eleanorrow/Zotero/storage/YIVMDIM3/Hadjeres - Interactive deep generative models for symbolic mu.pdf:application/pdf},
}

@misc{hadjeresInteractiveMusicGeneration2017,
	title = {Interactive {Music} {Generation} with {Positional} {Constraints} using {Anticipation}-{RNNs}},
	url = {http://arxiv.org/abs/1709.06404},
	abstract = {Recurrent Neural Networks (RNNS) are now widely used on sequence generation tasks due to their ability to learn long-range dependencies and to generate sequences of arbitrary length. However, their left-to-right generation procedure only allows a limited control from a potential user which makes them unsuitable for interactive and creative usages such as interactive music generation.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Hadjeres, Gaëtan and Nielsen, Frank},
	month = sep,
	year = {2017},
	note = {arXiv:1709.06404 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, RNN, Statistics - Machine Learning},
	annote = {Comment: 9 pages, 7 figures},
	annote = {\_eprint: 1709.06404},
	file = {Hadjeres and Nielsen - 2017 - Interactive Music Generation with Positional Const.pdf:/Users/eleanorrow/Zotero/storage/HAVDNHKB/Hadjeres and Nielsen - 2017 - Interactive Music Generation with Positional Const.pdf:application/pdf},
}

@misc{hadjeresVectorQuantizedContrastive2020,
	title = {Vector {Quantized} {Contrastive} {Predictive} {Coding} for {Template}-based {Music} {Generation}},
	url = {http://arxiv.org/abs/2004.10120},
	abstract = {In this work, we propose a ﬂexible method for generating variations of discrete sequences in which tokens can be grouped into basic units, like sentences in a text or bars in music. More precisely, given a template sequence, we aim at producing novel sequences sharing perceptible similarities with the original template without relying on any annotation; so our problem of generating variations is intimately linked to the problem of learning relevant high-level representations without supervision. Our contribution is two-fold: First, we propose a self-supervised encoding technique, named Vector Quantized Contrastive Predictive Coding which allows to learn a meaningful assignment of the basic units over a discrete set of codes, together with mechanisms allowing to control the information content of these learnt discrete representations. Secondly, we show how these compressed representations can be used to generate variations of a template sequence by using an appropriate attention pattern in the Transformer architecture. We illustrate our approach on the corpus of J.S. Bach chorales where we discuss the musical meaning of the learnt discrete codes and show that our proposed method allows to generate coherent and high-quality variations of a given template.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Hadjeres, Gaëtan and Crestel, Léopold},
	month = apr,
	year = {2020},
	note = {arXiv:2004.10120 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Contrastive Predictive Coding, Electrical Engineering and Systems Science - Audio and Speech Processing, Music generation, Self-supervised Learning, Vector Quantization},
	annote = {Comment: 15 pages, 13 figures},
	annote = {\_eprint: 2004.10120},
	file = {Hadjeres and Crestel - 2020 - Vector Quantized Contrastive Predictive Coding for.pdf:/Users/eleanorrow/Zotero/storage/I3G9VJND/Hadjeres and Crestel - 2020 - Vector Quantized Contrastive Predictive Coding for.pdf:application/pdf},
}

@misc{hadjeresPianoInpaintingApplication2021,
	title = {The {Piano} {Inpainting} {Application}},
	url = {http://arxiv.org/abs/2107.05944},
	abstract = {Autoregressive models are now capable of generating high-quality minute-long expressive MIDI piano performances. Even though this progress suggests new tools to assist music composition, we observe that generative algorithms are still not widely used by artists due to the limited control they offer, prohibitive inference times or the lack of integration within musicians’ workﬂows. In this work, we present the Piano Inpainting Application (PIA), a generative model focused on “inpainting” piano performances, as we believe that this elementary operation (restoring missing parts of a piano performance) encourages human-machine interaction and opens up new ways to approach music composition. Our approach relies on an encoder-decoder Linear Transformer architecture trained on a novel representation for MIDI piano performances termed Structured MIDI Encoding. By uncovering an interesting synergy between Linear Transformers and our inpainting task, we are able to efﬁciently inpaint contiguous regions of a piano performance, which makes our model suitable for interactive and responsive A.I.-assisted composition. Finally, we introduce our freely-available Ableton Live PIA plugin, which allows musicians to smoothly generate or modify any MIDI clip using PIA within a widely-used professional Digital Audio Workstation.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Hadjeres, Gaëtan and Crestel, Léopold},
	month = jul,
	year = {2021},
	note = {arXiv:2107.05944 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Hadjeres and Crestel - 2021 - The Piano Inpainting Application.pdf:/Users/eleanorrow/Zotero/storage/K3QSCGE9/Hadjeres and Crestel - 2021 - The Piano Inpainting Application.pdf:application/pdf},
}

@misc{hernandez-olivanMusicCompositionDeep2021,
	title = {Music {Composition} with {Deep} {Learning}: {A} {Review}},
	shorttitle = {Music {Composition} with {Deep} {Learning}},
	url = {http://arxiv.org/abs/2108.12290},
	abstract = {Generating a complex work of art such as a musical composition requires exhibiting true creativity that depends on a variety of factors that are related to the hierarchy of musical language. Music generation have been faced with Algorithmic methods and recently, with Deep Learning models that are being used in other ﬁelds such as Computer Vision. In this paper we want to put into context the existing relationships between AI-based music composition models and human musical composition and creativity processes. We give an overview of the recent Deep Learning models for music composition and we compare these models to the music composition process from a theoretical point of view. We have tried to answer some of the most relevant open questions for this task by analyzing the ability of current Deep Learning models to generate music with creativity or the similarity between AI and human composition processes, among others.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Hernandez-Olivan, Carlos and Beltran, Jose R.},
	month = sep,
	year = {2021},
	note = {arXiv:2108.12290 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {\_eprint: 2108.12290},
	file = {Hernandez-Olivan and Beltran - 2021 - Music Composition with Deep Learning A Review.pdf:/Users/eleanorrow/Zotero/storage/RWCJFGKQ/Hernandez-Olivan and Beltran - 2021 - Music Composition with Deep Learning A Review.pdf:application/pdf},
}

@article{herremansFunctionalTaxonomyMusic2018,
	title = {A {Functional} {Taxonomy} of {Music} {Generation} {Systems}},
	volume = {50},
	issn = {0360-0300, 1557-7341},
	url = {http://arxiv.org/abs/1812.04186},
	doi = {10.1145/3108242},
	abstract = {Digital advances have transformed the face of automatic music generation since its beginnings at the dawn of computing. Despite the many breakthroughs, issues such as the musical tasks targeted by different machines and the degree to which they succeed remain open questions. We present a functional taxonomy for music generation systems with reference to existing systems. The taxonomy organizes systems according to the purposes for which they were designed. It also reveals the inter-relatedness amongst the systems. This design-centered approach contrasts with predominant methods-based surveys and facilitates the identification of grand challenges to set the stage for new breakthroughs.},
	language = {en},
	number = {5},
	urldate = {2023-12-30},
	journal = {ACM Computing Surveys},
	author = {Herremans, Dorien and Chuan, Ching-Hua and Chew, Elaine},
	month = sep,
	year = {2018},
	note = {arXiv:1812.04186 [cs, eess]},
	keywords = {68Txx, 68-XX, algorithmic composition, automatic composition, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, functional survey, Music generation, survey, taxonomy},
	pages = {1--30},
	annote = {Comment: survey, music generation, taxonomy, functional survey, survey, automatic composition, algorithmic composition},
	file = {Herremans et al. - 2018 - A Functional Taxonomy of Music Generation Systems.pdf:/Users/eleanorrow/Zotero/storage/JZEFPGEJ/Herremans et al. - 2018 - A Functional Taxonomy of Music Generation Systems.pdf:application/pdf},
}

@article{hewahiGenerationMusicPieces2019,
	title = {Generation of music pieces using machine learning: long short-term memory neural networks approach},
	volume = {26},
	issn = {2576-5299},
	shorttitle = {Generation of music pieces using machine learning},
	url = {https://www.tandfonline.com/doi/full/10.1080/25765299.2019.1649972},
	doi = {10.1080/25765299.2019.1649972},
	abstract = {In this article, we explore the usage of long short-term memory neural network (NN) in generating music pieces and propose an approach to do so. Bach’s musical style has been selected to train the NN to make it able to generate similar music pieces. The proposed approach takes midi files, converting them to song files and then encoding them to be as inputs for the NN. Before inputting the files into the NNs, an augmentation process which augments the file into different keys is performed then the file is fed into the NN for training. The last step is the music generation. The main objective is to provide the NN with an arbitrary note and then the NN starts amending it gradually until producing a good piece of music. Various experiments have been conducted to explore the best values of parameters that can be selected to obtain good music generations. The obtained generated music pieces are accepted in terms of rhythm and harmony; however, some other problems exist such as in certain cases the tone stops or in some other cases getting short melodies that do not change.},
	language = {en},
	number = {1},
	urldate = {2023-12-30},
	journal = {Arab Journal of Basic and Applied Sciences},
	author = {Hewahi, Nabil and AlSaigal, Salman and AlJanahi, Sulaiman},
	month = jan,
	year = {2019},
	keywords = {On paper},
	pages = {397--413},
	annote = {ISBN: null Publisher: Taylor \& Francis},
	annote = {Mentioned Use of Jukebox, which produces music based on genre and beats per second and is available for business to license},
	file = {Hewahi et al. - 2019 - Generation of music pieces using machine learning.pdf:/Users/eleanorrow/Zotero/storage/C9MQTUZH/Hewahi et al. - 2019 - Generation of music pieces using machine learning.pdf:application/pdf},
}

@article{hookStrongConceptsIntermediatelevel2012,
	title = {Strong concepts: {Intermediate}-level knowledge in interaction design research},
	volume = {19},
	issn = {1073-0516, 1557-7325},
	shorttitle = {Strong concepts},
	url = {https://dl.acm.org/doi/10.1145/2362364.2362371},
	doi = {10.1145/2362364.2362371},
	abstract = {Design-oriented research practices create opportunities for constructing knowledge that is more abstracted than particular instances, without aspiring to be at the scope of generalized theories. We propose an intermediate design knowledge form that we name
              strong concepts
              that has the following properties: is generative and carries a core design idea, cutting across particular use situations and even application domains; concerned with interactive behavior, not static appearance; is a design element and a part of an artifact and, at the same time, speaks of a use practice and behavior over time; and finally, resides on an abstraction level above particular instances. We present two strong concepts—social navigation and seamfulness—and discuss how they fulfil criteria we might have on knowledge, such as being contestable, defensible, and substantive. Our aim is to foster an academic culture of discursive knowledge construction of intermediate-level knowledge and of how it can be produced and assessed in design-oriented HCI research.},
	language = {en},
	number = {3},
	urldate = {2023-12-30},
	journal = {ACM Transactions on Computer-Human Interaction},
	author = {Höök, Kristina and Löwgren, Jonas},
	month = oct,
	year = {2012},
	keywords = {bare-skin connection, Design research, seamfulness, social navigation, strong concepts},
	pages = {1--18},
	file = {Höök and Löwgren - 2012 - Strong concepts Intermediate-level knowledge in i.pdf:/Users/eleanorrow/Zotero/storage/P8XMQS6R/Höök and Löwgren - 2012 - Strong concepts Intermediate-level knowledge in i.pdf:application/pdf},
}

@misc{hsiaoCompoundWordTransformer2021,
	title = {Compound {Word} {Transformer}: {Learning} to {Compose} {Full}-{Song} {Music} over {Dynamic} {Directed} {Hypergraphs}},
	shorttitle = {Compound {Word} {Transformer}},
	url = {http://arxiv.org/abs/2101.02402},
	abstract = {To apply neural sequence models such as the Transformers to music generation tasks, one has to represent a piece of music by a sequence of tokens drawn from a ﬁnite set of pre-deﬁned vocabulary. Such a vocabulary usually involves tokens of various types. For example, to describe a musical note, one needs separate tokens to indicate the note’s pitch, duration, velocity (dynamics), and placement (onset time) along the time grid. While different types of tokens may possess different properties, existing models usually treat them equally, in the same way as modeling words in natural languages. In this paper, we present a conceptually different approach that explicitly takes into account the type of the tokens, such as note types and metric types. And, we propose a new Transformer decoder architecture that uses different feed-forward heads to model tokens of different types. With an expansion-compression trick, we convert a piece of music to a sequence of compound words by grouping neighboring tokens, greatly reducing the length of the token sequences. We show that the resulting model can be viewed as a learner over dynamic directed hypergraphs. And, we employ it to learn to compose expressive Pop piano music of full-song length (involving up to 10K individual tokens per song), both conditionally and unconditionally. Our experiment shows that, compared to state-of-the-art models, the proposed model converges 5–10 times faster at training (i.e., within a day on a single GPU with 11 GB memory), and with comparable quality in the generated music.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Hsiao, Wen-Yi and Liu, Jen-Yu and Yeh, Yin-Cheng and Yang, Yi-Hsuan},
	month = jan,
	year = {2021},
	note = {arXiv:2101.02402 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {\_eprint: 2101.02402},
	file = {Hsiao et al. - 2021 - Compound Word Transformer Learning to Compose Ful.pdf:/Users/eleanorrow/Zotero/storage/HAVVCEBN/Hsiao et al. - 2021 - Compound Word Transformer Learning to Compose Ful.pdf:application/pdf},
}

@misc{huCanMachinesGenerate2022,
	title = {Can {Machines} {Generate} {Personalized} {Music}? {A} {Hybrid} {Favorite}-aware {Method} for {User} {Preference} {Music} {Transfer}},
	shorttitle = {Can {Machines} {Generate} {Personalized} {Music}?},
	url = {http://arxiv.org/abs/2201.08526},
	abstract = {User preference music transfer (UPMT) is a new problem in music style transfer that can be applied to many scenarios but remains understudied. Transferring an arbitrary song to ﬁt a user’s preferences increases musical diversity and improves user engagement, which can greatly beneﬁt individuals’ mental health. Most music style transfer approaches rely on datadriven methods. In general, however, constructing a large training dataset is challenging because users can rarely provide enough of their favorite songs. To address this problem, this paper proposes a novel hybrid method called User Preference Transformer (UPTransformer) which uses prior knowledge of only one piece of a user’s favorite music. Based on the distribution of music events in the provided music, we propose a new favorite-aware loss function to ﬁne-tune the Transformer-based model. Two steps are proposed in the transfer phase to achieve UPMT based on the extracted music pattern in a user’s favorite music. Additionally, to alleviate the problem of evaluating melodic similarity in music style transfer, we propose a new concept called pattern similarity (PS) to measure the similarity between two pieces of music. Statistical tests indicate that the results of PS are consistent with the similarity score in a qualitative experiment. Furthermore, experimental results on subjects show that the transferred music achieves better performance in musicality, similarity, and user preferences.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Hu, Zhejing and Liu, Yan and Chen, Gong and Liu, Yongxu},
	month = jan,
	year = {2022},
	note = {arXiv:2201.08526 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {\_eprint: arXiv:2201.08526v1},
	file = {Hu et al. - 2022 - Can Machines Generate Personalized Music A Hybrid.pdf:/Users/eleanorrow/Zotero/storage/QG9RS766/Hu et al. - 2022 - Can Machines Generate Personalized Music A Hybrid.pdf:application/pdf},
}

@misc{huangDeepLearningMusic2016,
	title = {Deep {Learning} for {Music}},
	url = {http://arxiv.org/abs/1606.04930},
	abstract = {Our goal is to be able to build a generative model from a deep neural network architecture to try to create music that has both harmony and melody and is passable as music composed by humans. Previous work in music generation has mainly been focused on creating a single melody. More recent work on polyphonic music modeling, centered around time series probability density estimation, has met some partial success. In particular, there has been a lot of work based off of Recurrent Neural Networks combined with Restricted Boltzmann Machines (RNNRBM) and other similar recurrent energy based models. Our approach, however, is to perform end-to-end learning and generation with deep neural nets alone.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Huang, Allen and Wu, Raymond},
	month = jun,
	year = {2016},
	note = {arXiv:1606.04930 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, On paper},
	annote = {Comment: 8 pages, Stanford CS224D},
	annote = {Pages: arXiv:1606.04930 Publication Title: arXiv e-prints},
	file = {Huang and Wu - 2016 - Deep Learning for Music.pdf:/Users/eleanorrow/Zotero/storage/CSNVI24U/Huang and Wu - 2016 - Deep Learning for Music.pdf:application/pdf},
}

@misc{huangCounterpointConvolution2019,
	title = {Counterpoint by {Convolution}},
	url = {http://arxiv.org/abs/1903.07227},
	abstract = {Machine learning models of music typically break up the task of composition into a chronological process, composing a piece of music in a single pass from beginning to end. On the contrary, human composers write music in a nonlinear fashion, scribbling motifs here and there, often revisiting choices previously made. In order to better approximate this process, we train a convolutional neural network to complete partial musical scores, and explore the use of blocked Gibbs sampling as an analogue to rewriting. Neither the model nor the generative procedure are tied to a particular causal direction of composition.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Huang, Cheng-Zhi Anna and Cooijmans, Tim and Roberts, Adam and Courville, Aaron and Eck, Douglas},
	month = mar,
	year = {2019},
	note = {arXiv:1903.07227 [cs, eess, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, H.5.5, I.2, Statistics - Machine Learning},
	annote = {Comment: Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017},
	annote = {Issue: arXiv:1903.07227 arXiv: 1903.07227},
	file = {Huang et al. - 2019 - Counterpoint by Convolution.pdf:/Users/eleanorrow/Zotero/storage/HVYG6XFT/Huang et al. - 2019 - Counterpoint by Convolution.pdf:application/pdf},
}

@misc{huangBachDoodleApproachable2019,
	title = {The {Bach} {Doodle}: {Approachable} music composition with machine learning at scale},
	shorttitle = {The {Bach} {Doodle}},
	url = {http://arxiv.org/abs/1907.06637},
	abstract = {To make music composition more approachable, we designed the first AI-powered Google Doodle, the Bach Doodle, where users can create their own melody and have it harmonized by a machine learning model Coconet (Huang et al., 2017) in the style of Bach. For users to input melodies, we designed a simplified sheet-music based interface. To support an interactive experience at scale, we re-implemented Coconet in TensorFlow.js (Smilkov et al., 2019) to run in the browser and reduced its runtime from 40s to 2s by adopting dilated depth-wise separable convolutions and fusing operations. We also reduced the model download size to approximately 400KB through post-training weight quantization. We calibrated a speed test based on partial model evaluation time to determine if the harmonization request should be performed locally or sent to remote TPU servers. In three days, people spent 350 years worth of time playing with the Bach Doodle, and Coconet received more than 55 million queries. Users could choose to rate their compositions and contribute them to a public dataset, which we are releasing with this paper. We hope that the community finds this dataset useful for applications ranging from ethnomusicological studies, to music education, to improving machine learning models.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Huang, Cheng-Zhi Anna and Hawthorne, Curtis and Roberts, Adam and Dinculescu, Monica and Wexler, James and Hong, Leon and Howcroft, Jacob},
	month = jul,
	year = {2019},
	note = {arXiv:1907.06637 [cs, eess, stat]},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	annote = {Comment: Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2019},
	annote = {Issue: arXiv:1907.06637 arXiv: 1907.06637},
	file = {Huang et al. - 2019 - The Bach Doodle Approachable music composition wi.pdf:/Users/eleanorrow/Zotero/storage/YGFE9AC8/Huang et al. - 2019 - The Bach Doodle Approachable music composition wi.pdf:application/pdf},
}

@misc{huangAISongContest2020,
	title = {{AI} {Song} {Contest}: {Human}-{AI} {Co}-{Creation} in {Songwriting}},
	shorttitle = {{AI} {Song} {Contest}},
	url = {http://arxiv.org/abs/2010.05388},
	abstract = {Machine learning is challenging the way we make music. Although research in deep generative models has dramatically improved the capability and ﬂuency of music models, recent work has shown that it can be challenging for humans to partner with this new class of algorithms. In this paper, we present ﬁndings on what 13 musician/developer teams, a total of 61 users, needed when co-creating a song with AI, the challenges they faced, and how they leveraged and repurposed existing characteristics of AI to overcome some of these challenges. Many teams adopted modular approaches, such as independently running multiple smaller models that align with the musical building blocks of a song, before re-combining their results. As ML models are not easily steerable, teams also generated massive numbers of samples and curated them post-hoc, or used a range of strategies to direct the generation, or algorithmically ranked the samples. Ultimately, teams not only had to manage the “ﬂare and focus” aspects of the creative process, but also juggle them with a parallel process of exploring and curating multiple ML models and outputs. These ﬁndings reﬂect a need to design machine learning-powered music interfaces that are more decomposable, steerable, interpretable, and adaptive, which in return will enable artists to more effectively explore how AI can extend their personal expression.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Huang, Cheng-Zhi Anna and Koops, Hendrik Vincent and Newton-Rex, Ed and Dinculescu, Monica and Cai, Carrie J.},
	month = oct,
	year = {2020},
	note = {arXiv:2010.05388 [cs, eess]},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Human-AI Partnerships, I.2, J.5},
	annote = {arXiv: 2010.05388 [cs, eess] Number: arXiv:2010.05388},
	annote = {Comment: 6 pages + 3 pages of references},
	annote = {Comment: 6 pages + 3 pages of references},
	annote = {Issue: arXiv:2010.05388 arXiv: 2010.05388},
	file = {Huang et al. - 2020 - AI Song Contest Human-AI Co-Creation in Songwriti.pdf:/Users/eleanorrow/Zotero/storage/QHN2PSBK/Huang et al. - 2020 - AI Song Contest Human-AI Co-Creation in Songwriti.pdf:application/pdf},
}

@misc{hungMusicalCompositionStyle2019,
	title = {Musical {Composition} {Style} {Transfer} via {Disentangled} {Timbre} {Representations}},
	url = {http://arxiv.org/abs/1905.13567},
	abstract = {Music creation involves not only composing the different parts (e.g., melody, chords) of a musical work but also arranging/selecting the instruments to play the different parts. While the former has received increasing attention, the latter has not been much investigated. This paper presents, to the best of our knowledge, the ﬁrst deep learning models for rearranging music of arbitrary genres. Specifically, we build encoders and decoders that take a piece of polyphonic musical audio as input, and predict as output its musical score. We investigate disentanglement techniques such as adversarial training to separate latent factors that are related to the musical content (pitch) of different parts of the piece, and that are related to the instrumentation (timbre) of the parts per short-time segment. By disentangling pitch and timbre, our models have an idea of how each piece was composed and arranged. Moreover, the models can realize “composition style transfer” by rearranging a musical piece without much affecting its pitch content. We validate the effectiveness of the models by experiments on instrument activity detection and composition style transfer. To facilitate follow-up research, we open source our code at https://github. com/biboamy/instrument-disentangle.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Hung, Yun-Ning and Chiang, I.-Tung and Chen, Yi-An and Yang, Yi-Hsuan},
	month = may,
	year = {2019},
	note = {arXiv:1905.13567 [cs, eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {arXiv:1905.13567 [cs, eess]},
	annote = {Comment: Accepted by the 28th International Joint Conference on Artificial Intelligence. arXiv admin note: text overlap with arXiv:1811.03271},
	annote = {Comment: Accepted by the 28th International Joint Conference on Artificial Intelligence. arXiv admin note: text overlap with arXiv:1811.03271},
	file = {Hung et al. - 2019 - Musical Composition Style Transfer via Disentangle.pdf:/Users/eleanorrow/Zotero/storage/BRFH7HFQ/Hung et al. - 2019 - Musical Composition Style Transfer via Disentangle.pdf:application/pdf},
}

@misc{huntComposingComputerGenerated,
	title = {Composing computer generated music, an observational study using {IGME}: the {Interactive} {Generative} {Music} {Environment}},
	abstract = {Computer composed music remains a novel and challenging problem to solve. Despite an abundance of techniques and systems little research has explored how these might be useful for end-users looking to compose with generative and algorithmic music techniques. User interfaces for generative music systems are often inaccessible to non-programmers and neglect established composition workﬂow and design paradigms that are familiar to computer-based music composers. We have developed a system called the Interactive Generative Music Environment (IGME) that attempts to bridge the gap between generative music and music sequencing software, through an easy to use score editing interface. This paper discusses a series of user studies in which users explore generative music composition with IGME. A questionnaire evaluates the user’s perception of interacting with generative music and from this provide recommendations for future generative music systems and interfaces.},
	language = {en},
	author = {Hunt, Samuel J and Mitchell, Thomas J and Nash, Chris},
	file = {Hunt et al. - Composing computer generated music, an observation.pdf:/Users/eleanorrow/Zotero/storage/E75C6KX6/Hunt et al. - Composing computer generated music, an observation.pdf:application/pdf},
}

@inproceedings{hunterUsingParticipatoryDesign2019,
	address = {Nottingham United Kingdom},
	title = {Using {Participatory} {Design} in the {Development} of a {New} {Musical} {Interface}: {Understanding} {Musician}'s {Needs} beyond {Usability}},
	isbn = {978-1-4503-7297-8},
	shorttitle = {Using {Participatory} {Design} in the {Development} of a {New} {Musical} {Interface}},
	url = {https://dl.acm.org/doi/10.1145/3356590.3356635},
	doi = {10.1145/3356590.3356635},
	abstract = {The design of New Musical Interfaces (NMI) often involves an intersection of disciplines: NMI design, Human-computer Interaction (HCI) and Interaction Design (IxD). This intersection has brought into NMI design approaches and philosophies arising from the focus of HCI and IxD on human-centred design. Not unexpectedly, a number of challenges and questions have arisen along with a signiﬁcant tension related to the nature of user involvement in the design process. This project adopted an approach informed by participatory design where musicians became integral to the design process. Through this process a deeper understanding of the needs of musicians beyond usability considerations was developed. Whilst this understanding is bounded by the speciﬁcs of the design context, this project is submitted as a case study that may provide some guidance in the processes adopted by others when designing NMIs.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {Proceedings of the 14th {International} {Audio} {Mostly} {Conference}: {A} {Journey} in {Sound}},
	publisher = {ACM},
	author = {Hunter, Trevor and Worthy, Peter and Matthews, Ben and Viller, Stephen},
	month = sep,
	year = {2019},
	keywords = {Human-Computer Interaction, Interaction Design, New Musical Interface Design, Participatory Design, User experience},
	pages = {268--271},
	file = {Hunter et al. - 2019 - Using Participatory Design in the Development of a.pdf:/Users/eleanorrow/Zotero/storage/972S3QR7/Hunter et al. - 2019 - Using Participatory Design in the Development of a.pdf:application/pdf},
}

@article{hutchingsAdaptiveMusicComposition2020,
	title = {Adaptive {Music} {Composition} for {Games}},
	volume = {12},
	issn = {2475-1502, 2475-1510},
	url = {http://arxiv.org/abs/1907.01154},
	doi = {10.1109/TG.2019.2921979},
	abstract = {The generation of music that adapts dynamically to content and actions has an important role in building more immersive, memorable and emotive game experiences. To date, the development of adaptive music systems for video games is limited by both the nature of algorithms used for real-time music generation and the limited modelling of player action, game world context and emotion in current games. We propose that these issues must be addressed in tandem for the quality and ﬂexibility of adaptive game music to signiﬁcantly improve. Cognitive models of knowledge organisation and emotional affect are integrated with multi-modal, multi-agent composition techniques to produce a novel Adaptive Music System (AMS). The system is integrated into two stylistically distinct games. Gamers reported an overall higher immersion and correlation of music with game-world concepts with the AMS than with the original game soundtracks in both games.},
	language = {en},
	number = {3},
	urldate = {2023-12-30},
	journal = {IEEE Transactions on Games},
	author = {Hutchings, Patrick and McCormack, Jon},
	month = sep,
	year = {2020},
	note = {arXiv:1907.01154 [cs, eess]},
	keywords = {Agent-based modeling, computer generated music, Computer Science - Artificial Intelligence, Computer Science - Multimedia, Computer Science - Sound, D.2.11, Electrical Engineering and Systems Science - Audio and Speech Processing, H.5.5, I.2.1, neural networks},
	pages = {270--280},
	annote = {Comment: Preprint. Accepted for publication in IEEE Transactions on Games, 2019},
	annote = {Comment: Preprint. Accepted for publication in IEEE Transactions on Games, 2019},
	annote = {\_eprint: arXiv:1907.01154v1},
	file = {Hutchings and McCormack - 2020 - Adaptive Music Composition for Games.pdf:/Users/eleanorrow/Zotero/storage/6SZMPAWC/Hutchings and McCormack - 2020 - Adaptive Music Composition for Games.pdf:application/pdf},
}

@misc{jiComprehensiveSurveyDeep2020,
	title = {A {Comprehensive} {Survey} on {Deep} {Music} {Generation}: {Multi}-level {Representations}, {Algorithms}, {Evaluations}, and {Future} {Directions}},
	shorttitle = {A {Comprehensive} {Survey} on {Deep} {Music} {Generation}},
	url = {http://arxiv.org/abs/2011.06801},
	abstract = {The utilization of deep learning techniques in generating various contents (such as image, text, etc.) has become a trend. Especially music, the topic of this paper, has attracted widespread attention of countless researchers.The whole process of producing music can be divided into three stages, corresponding to the three levels of music generation: score generation produces scores, performance generation adds performance characteristics to the scores, and audio generation converts scores with performance characteristics into audio by assigning timbre or generates music in audio format directly. Previous surveys have explored the network models employed in the field of automatic music generation. However, the development history, the model evolution, as well as the pros and cons of same music generation task have not been clearly illustrated. This paper attempts to provide an overview of various composition tasks under different music generation levels, covering most of the currently popular music generation tasks using deep learning. In addition, we summarize the datasets suitable for diverse tasks, discuss the music representations, the evaluation methods as well as the challenges under different levels, and finally point out several future directions.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Ji, Shulei and Luo, Jing and Yang, Xinyu},
	month = nov,
	year = {2020},
	note = {arXiv:2011.06801 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: 96 pages,this is a draft},
	file = {Ji et al. - 2020 - A Comprehensive Survey on Deep Music Generation M.pdf:/Users/eleanorrow/Zotero/storage/YSS4JK4M/Ji et al. - 2020 - A Comprehensive Survey on Deep Music Generation M.pdf:application/pdf},
}

@misc{jiaScalingVisualVisionLanguage2021,
	title = {Scaling {Up} {Visual} and {Vision}-{Language} {Representation} {Learning} {With} {Noisy} {Text} {Supervision}},
	url = {http://arxiv.org/abs/2102.05918},
	abstract = {Pre-trained representations are becoming crucial for many NLP and perception tasks. While representation learning in NLP has transitioned to training on raw text without human annotations, visual and vision-language representations still rely heavily on curated training datasets that are expensive or require expert knowledge. For vision applications, representations are mostly learned using datasets with explicit class labels such as ImageNet or OpenImages. For vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all involve a non-trivial data collection (and cleaning) process. This costly curation process limits the size of datasets and hence hinders the scaling of trained models. In this paper, we leverage a noisy dataset of over one billion image alt-text pairs, obtained without expensive ﬁltering or post-processing steps in the Conceptual Captions dataset. A simple dual-encoder architecture learns to align visual and language representations of the image and text pairs using a contrastive loss. We show that the scale of our corpus can make up for its noise and leads to state-of-the-art representations even with such a simple learning scheme. Our visual representation achieves strong performance when transferred to classiﬁcation tasks such as ImageNet and VTAB. The aligned visual and language representations also set new state-of-the-art results on Flickr30K and MSCOCO benchmarks, even when compared with more sophisticated cross-attention models. The representations also enable cross-modality search with complex text and text + image queries.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc V. and Sung, Yunhsuan and Li, Zhen and Duerig, Tom},
	month = jun,
	year = {2021},
	note = {arXiv:2102.05918 [cs]},
	keywords = {CLIP, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Contrastive Predictive Coding, NLP, Related to Contrastive Predictive Coding},
	annote = {Comment: ICML 2021},
	file = {Jia et al. - 2021 - Scaling Up Visual and Vision-Language Representati.pdf:/Users/eleanorrow/Zotero/storage/TIPMUN74/Jia et al. - 2021 - Scaling Up Visual and Vision-Language Representati.pdf:application/pdf},
}

@misc{kangProjectMilestoneGenerating2018,
	title = {Project milestone: {Generating} music with {Machine} {Learning}},
	abstract = {Composing music is a very interesting challenge that tests the composer’s creative capacity, whether it a human or a computer. Although there have been many arguments on the matter, almost all of music is some regurgitation or alteration of a sonic idea created before. Thus, with enough data and the correct algorithm, machine learning should be able to make music that would sound human. This report outlines various approaches to music composition through Naive Bayes and Neural Network models, and although there were some mixed results by the model, it is evident that musical ideas can be gleaned from these algorithms in hopes of making a new piece of music.},
	language = {en},
	author = {Kang, David and Kim, Jung Youn and Ringdahl, Simen},
	year = {2018},
	file = {Kang et al. - Project milestone Generating music with Machine L.pdf:/Users/eleanorrow/Zotero/storage/W6MQ5KH6/Kang et al. - Project milestone Generating music with Machine L.pdf:application/pdf},
}

@misc{karrasElucidatingDesignSpace2022,
	title = {Elucidating the {Design} {Space} of {Diffusion}-{Based} {Generative} {Models}},
	url = {http://arxiv.org/abs/2206.00364},
	abstract = {We argue that the theory and practice of diffusion-based generative models are currently unnecessarily convoluted and seek to remedy the situation by presenting a design space that clearly separates the concrete design choices. This lets us identify several changes to both the sampling and training processes, as well as preconditioning of the score networks. Together, our improvements yield new state-of-the-art FID of 1.79 for CIFAR-10 in a class-conditional setting and 1.97 in an unconditional setting, with much faster sampling (35 network evaluations per image) than prior designs. To further demonstrate their modular nature, we show that our design changes dramatically improve both the efﬁciency and quality obtainable with pre-trained score networks from previous work, including improving the FID of a previously trained ImageNet-64 model from 2.07 to near-SOTA 1.55, and after re-training with our proposed improvements to a new SOTA of 1.36.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Karras, Tero and Aittala, Miika and Aila, Timo and Laine, Samuli},
	month = oct,
	year = {2022},
	note = {arXiv:2206.00364 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {arXiv:2206.00364 [cs, stat]},
	annote = {Comment: NeurIPS 2022},
	annote = {Comment: NeurIPS 2022},
	file = {Karras et al. - 2022 - Elucidating the Design Space of Diffusion-Based Ge.pdf:/Users/eleanorrow/Zotero/storage/GTZPSUSQ/Karras et al. - 2022 - Elucidating the Design Space of Diffusion-Based Ge.pdf:application/pdf},
}

@misc{keskarCTRLConditionalTransformer2019,
	title = {{CTRL}: {A} {Conditional} {Transformer} {Language} {Model} for {Controllable} {Generation}},
	shorttitle = {{CTRL}},
	url = {http://arxiv.org/abs/1909.05858},
	abstract = {Large-scale language models show promising text generation capabilities, but users cannot easily control particular aspects of the generated text. We release CTRL, a 1.63 billion-parameter conditional transformer language model, trained to condition on control codes that govern style, content, and task-speciﬁc behavior. Control codes were derived from structure that naturally co-occurs with raw text, preserving the advantages of unsupervised learning while providing more explicit control over text generation. These codes also allow CTRL to predict which parts of the training data are most likely given a sequence. This provides a potential method for analyzing large amounts of data via model-based source attribution. We have released multiple full-sized, pretrained versions of CTRL at https://github.com/salesforce/ctrl.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Keskar, Nitish Shirish and McCann, Bryan and Varshney, Lav R. and Xiong, Caiming and Socher, Richard},
	month = sep,
	year = {2019},
	note = {arXiv:1909.05858 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {arXiv:1909.05858 [cs]},
	file = {Keskar et al. - 2019 - CTRL A Conditional Transformer Language Model for.pdf:/Users/eleanorrow/Zotero/storage/PQSPFMP8/Keskar et al. - 2019 - CTRL A Conditional Transformer Language Model for.pdf:application/pdf},
}

@misc{khoslaSupervisedContrastiveLearning2021,
	title = {Supervised {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2004.11362},
	abstract = {Contrastive learning applied to self-supervised representation learning has seen a resurgence in recent years, leading to state of the art performance in the unsupervised training of deep image models. Modern batch contrastive approaches subsume or signiﬁcantly outperform traditional contrastive losses such as triplet, max-margin and the N-pairs loss. In this work, we extend the self-supervised batch contrastive approach to the fully-supervised setting, allowing us to effectively leverage label information. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. We analyze two possible versions of the supervised contrastive (SupCon) loss, identifying the best-performing formulation of the loss. On ResNet-200, we achieve top-1 accuracy of 81.4\% on the ImageNet dataset, which is 0.8\% above the best number reported for this architecture. We show consistent outperformance over cross-entropy on other datasets and two ResNet variants. The loss shows beneﬁts for robustness to natural corruptions, and is more stable to hyperparameter settings such as optimizers and data augmentations. Our loss function is simple to implement and reference TensorFlow code is released at https://t.ly/supcon 1.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
	month = mar,
	year = {2021},
	note = {arXiv:2004.11362 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Khosla et al. - 2021 - Supervised Contrastive Learning.pdf:/Users/eleanorrow/Zotero/storage/LM6T7SR7/Khosla et al. - 2021 - Supervised Contrastive Learning.pdf:application/pdf},
}

@misc{kohRethinkingRecurrentLatent2018,
	title = {Rethinking {Recurrent} {Latent} {Variable} {Model} for {Music} {Composition}},
	url = {http://arxiv.org/abs/1810.03226},
	abstract = {We present a model for capturing musical features and creating novel sequences of music, called the ConvolutionalVariational Recurrent Neural Network. To generate sequential data, the model uses an encoder-decoder architecture with latent probabilistic connections to capture the hidden structure of music. Using the sequence-to-sequence model, our generative model can exploit samples from a prior distribution and generate a longer sequence of music. We compare the performance of our proposed model with other types of Neural Networks using the criteria of Information Rate that is implemented by Variable Markov Oracle, a method that allows statistical characterization of musical information dynamics and detection of motifs in a song. Our results suggest that the proposed model has a better statistical resemblance to the musical structure of the training data, which improves the creation of new sequences of music in the style of the originals.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Koh, Eunjeong Stella and Dubnov, Shlomo and Wright, Dustin},
	month = oct,
	year = {2018},
	note = {arXiv:1810.03226 [cs, eess]},
	keywords = {CNN, Computer Science - Machine Learning, Computer Science - Multimedia, Computer Science - Sound, Electrical Engineering and Systems, Electrical Engineering and Systems Science - Audio and Speech Processing, RNN, Science - Audio and Speech Processing},
	annote = {Comment: Published as a conference paper at IEEE MMSP 2018},
	annote = {Pages: arXiv:1810.03226 Publication Title: arXiv e-prints},
	file = {Koh et al. - 2018 - Rethinking Recurrent Latent Variable Model for Mus.pdf:/Users/eleanorrow/Zotero/storage/CH6EL9GJ/Koh et al. - 2018 - Rethinking Recurrent Latent Variable Model for Mus.pdf:application/pdf},
}

@article{kritsisAdaptabilityRecurrentNeural2021,
	title = {On the {Adaptability} of {Recurrent} {Neural} {Networks} for {Real}-{Time} {Jazz} {Improvisation} {Accompaniment}},
	volume = {3},
	issn = {2624-8212},
	url = {https://www.frontiersin.org/articles/10.3389/frai.2020.508727/full},
	doi = {10.3389/frai.2020.508727},
	abstract = {Jazz improvisation on a given lead sheet with chords is an interesting scenario for studying the behaviour of artiﬁcial agents when they collaborate with humans. Speciﬁcally in jazz improvisation, the role of the accompanist is crucial for reﬂecting the harmonic and metric characteristics of a jazz standard, while identifying in real-time the intentions of the soloist and adapt the accompanying performance parameters accordingly. This paper presents a study on a basic implementation of an artiﬁcial jazz accompanist, which provides accompanying chord voicings to a human soloist that is conditioned by the soloing input and the harmonic and metric information provided in a lead sheet chart. The model of the artiﬁcial agent includes a separate model for predicting the intentions of the human soloist, towards providing proper accompaniment to the human performer in real-time. Simple implementations of Recurrent Neural Networks are employed both for modeling the predictions of the artiﬁcial agent and for modeling the expectations of human intention. A publicly available dataset is modiﬁed with a probabilistic reﬁnement process for including all the necessary information for the task at hand and test-case compositions on two jazz standards show the ability of the system to comply with the harmonic constraints within the chart. Furthermore, the system is indicated to be able to provide varying output with different soloing conditions, while there is no signiﬁcant sacriﬁce of “musicality” in generated music, as shown in subjective evaluations. Some important limitations that need to be addressed for obtaining more informative results on the potential of the examined approach are also discussed.},
	language = {en},
	urldate = {2023-12-30},
	journal = {Frontiers in Artificial Intelligence},
	author = {Kritsis, Kosmas and Kylafi, Theatina and Kaliakatsos-Papakostas, Maximos and Pikrakis, Aggelos and Katsouros, Vassilis},
	month = feb,
	year = {2021},
	pages = {508727},
	file = {Kritsis et al. - 2021 - On the Adaptability of Recurrent Neural Networks f.pdf:/Users/eleanorrow/Zotero/storage/2EVVL4QD/Kritsis et al. - 2021 - On the Adaptability of Recurrent Neural Networks f.pdf:application/pdf},
}

@inproceedings{landryParticipatoryDesignResearch2017,
	address = {University Park Campus},
	title = {Participatory {Design} {Research} {Methodologies}: {A} {Case} {Study} in {Dancer} {Sonification}},
	isbn = {978-0-9670904-4-3},
	shorttitle = {Participatory {Design} {Research} {Methodologies}},
	url = {http://hdl.handle.net/1853/58379},
	doi = {10.21785/icad2017.069},
	abstract = {Given that embodied interaction is widespread in HumanComputer Interaction, interests on the importance of body movements and emotions are gradually increasing. The present paper describes our process of designing and testing a dancer sonification system using a participatory design research methodology. The end goal of the dancer sonification project is to have dancers generate aesthetically pleasing music in realtime based on their dance gestures, instead of dancing to prerecorded music. The generated music should reflect both the kinetic activities and affective contents of the dancer’s movement. To accomplish these goals, expert dancers and musicians were recruited as domain experts in affective gesture and auditory communication. Much of the dancer sonification literature focuses exclusively on describing the final performance piece or the techniques used to process motion data into auditory control parameters. This paper focuses on the methods we used to identify, select, and test the most appropriate motion to sound mappings for a dancer sonification system.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {Proceedings of the 23rd {International} {Conference} on {Auditory} {Display} - {ICAD} 2017},
	publisher = {The International Community for Auditory Display},
	author = {Landry, Steven and Jeon, Myounghoon},
	month = jun,
	year = {2017},
	keywords = {Participatory Design},
	pages = {182--187},
	annote = {Publisher: Georgia Institute of Technology},
	file = {Landry and Jeon - 2017 - Participatory Design Research Methodologies A Cas.pdf:/Users/eleanorrow/Zotero/storage/LMDAJKYI/Landry and Jeon - 2017 - Participatory Design Research Methodologies A Cas.pdf:application/pdf},
}

@article{le-khacContrastiveRepresentationLearning2020,
	title = {Contrastive {Representation} {Learning}: {A} {Framework} and {Review}},
	volume = {8},
	issn = {2169-3536},
	shorttitle = {Contrastive {Representation} {Learning}},
	url = {https://ieeexplore.ieee.org/document/9226466/},
	doi = {10.1109/ACCESS.2020.3031549},
	abstract = {Contrastive Learning has recently received interest due to its success in self-supervised representation learning in the computer vision domain. However, the origins of Contrastive Learning date as far back as the 1990s and its development has spanned across many ﬁelds and domains including Metric Learning and natural language processing. In this paper, we provide a comprehensive literature review and we propose a general Contrastive Representation Learning framework that simpliﬁes and uniﬁes many different contrastive learning methods. We also provide a taxonomy for each of the components of contrastive learning in order to summarise it and distinguish it from other forms of machine learning. We then discuss the inductive biases which are present in any contrastive learning system and we analyse our framework under different views from various sub-ﬁelds of Machine Learning. Examples of how contrastive learning has been applied in computer vision, natural language processing, audio processing, and others, as well as in Reinforcement Learning are also presented. Finally, we discuss the challenges and some of the most promising future research directions ahead.},
	language = {en},
	urldate = {2023-12-30},
	journal = {IEEE access : practical innovations, open solutions},
	author = {Le-Khac, Phuc H. and Healy, Graham and Smeaton, Alan F.},
	year = {2020},
	keywords = {Contrastive learning, deep learning, machine learning, representation learning, self-supervised learning, unsupervised learning},
	pages = {193907--193934},
	annote = {\_eprint: 2010.05113},
	file = {Le-Khac et al. - 2020 - Contrastive Representation Learning A Framework a.pdf:/Users/eleanorrow/Zotero/storage/W9KQ4A5K/Le-Khac et al. - 2020 - Contrastive Representation Learning A Framework a.pdf:application/pdf},
}

@misc{liDiffusionLMImprovesControllable2022,
	title = {Diffusion-{LM} {Improves} {Controllable} {Text} {Generation}},
	url = {http://arxiv.org/abs/2205.14217},
	abstract = {Controlling the behavior of language models (LMs) without re-training is a major open problem in natural language generation. While recent works have demonstrated successes on controlling simple sentence attributes (e.g., sentiment), there has been little progress on complex, fine-grained controls (e.g., syntactic structure). To address this challenge, we develop a new non-autoregressive language model based on continuous diffusions that we call Diffusion-LM. Building upon the recent successes of diffusion models in continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variables. The continuous, hierarchical nature of these intermediate variables enables a simple gradient-based algorithm to perform complex, controllable generation tasks. We demonstrate successful control of Diffusion-LM for six challenging fine-grained control tasks, significantly outperforming prior work.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Li, Xiang Lisa and Thickstun, John and Gulrajani, Ishaan and Liang, Percy and Hashimoto, Tatsunori B.},
	month = may,
	year = {2022},
	note = {arXiv:2205.14217 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {arXiv:2205.14217 [cs]},
	file = {Li et al. - 2022 - Diffusion-LM Improves Controllable Text Generation.pdf:/Users/eleanorrow/Zotero/storage/UPDAET6R/Li et al. - 2022 - Diffusion-LM Improves Controllable Text Generation.pdf:application/pdf},
}

@misc{liuBach2014Music2014,
	title = {Bach in 2014: {Music} {Composition} with {Recurrent} {Neural} {Network}},
	shorttitle = {Bach in 2014},
	url = {http://arxiv.org/abs/1412.3191},
	abstract = {We propose a framework for computer music composition that uses resilient propagation (RProp) and long short term memory (LSTM) recurrent neural network. In this paper, we show that LSTM network learns the structure and characteristics of music pieces properly by demonstrating its ability to recreate music. We also show that predicting existing music using RProp outperforms Back propagation through time (BPTT).},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Liu, I.-Ting and Ramakrishnan, Bhiksha},
	month = dec,
	year = {2014},
	note = {arXiv:1412.3191 [cs]},
	keywords = {and Evolutionary Computing, Computer Science - Artificial Intelligence, Computer Science - Neural, Computer Science - Neural and Evolutionary Computing, On paper},
	annote = {Pages: arXiv:1412.3191 Publication Title: arXiv e-prints},
	file = {Liu and Ramakrishnan - 2014 - Bach in 2014 Music Composition with Recurrent Neu.pdf:/Users/eleanorrow/Zotero/storage/AI77VKQ7/Liu and Ramakrishnan - 2014 - Bach in 2014 Music Composition with Recurrent Neu.pdf:application/pdf},
}

@misc{liuLetUsBuild2022,
	title = {Let us {Build} {Bridges}: {Understanding} and {Extending} {Diffusion} {Generative} {Models}},
	shorttitle = {Let us {Build} {Bridges}},
	url = {http://arxiv.org/abs/2208.14699},
	abstract = {Diffusion-based generative models have achieved promising results recently, but raise an array of open questions in terms of conceptual understanding, theoretical analysis, algorithm improvement and extensions to discrete, structured, non-Euclidean domains. This work tries to re-exam the overall framework, in order to gain better theoretical understandings and develop algorithmic extensions for data from arbitrary domains. By viewing diffusion models as latent variable models with unobserved diffusion trajectories and applying maximum likelihood estimation (MLE) with latent trajectories imputed from an auxiliary distribution, we show that both the model construction and the imputation of latent trajectories amount to constructing diffusion bridge processes that achieve deterministic values and constraints at end point, for which we provide a systematic study and a suit of tools. Leveraging our framework, we present 1) a ﬁrst theoretical error analysis for learning diffusion generation models, and 2) a simple and uniﬁed approach to learning on data from different discrete and constrained domains. Experiments show that our methods perform superbly on generating images, semantic segments and 3D point clouds.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Liu, Xingchao and Wu, Lemeng and Ye, Mao and Liu, Qiang},
	month = aug,
	year = {2022},
	note = {arXiv:2208.14699 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Issue: arXiv:2208.14699 arXiv: 2208.14699},
	file = {Liu et al. - 2022 - Let us Build Bridges Understanding and Extending .pdf:/Users/eleanorrow/Zotero/storage/KWIJGQUH/Liu et al. - 2022 - Let us Build Bridges Understanding and Extending .pdf:application/pdf},
}

@inproceedings{louieNoviceAIMusicCoCreation2020,
	address = {Honolulu HI USA},
	series = {{CHI} '20},
	title = {Novice-{AI} {Music} {Co}-{Creation} via {AI}-{Steering} {Tools} for {Deep} {Generative} {Models}},
	isbn = {978-1-4503-6708-0},
	url = {https://dl.acm.org/doi/10.1145/3313831.3376739},
	doi = {10.1145/3313831.3376739},
	abstract = {While generative deep neural networks (DNNs) have demonstrated their capacity for creating novel musical compositions, less attention has been paid to the challenges and potential of co-creating with these musical AIs, especially for novices. In a needﬁnding study with a widely used, interactive musical AI, we found that the AI can overwhelm users with the amount of musical content it generates, and frustrate them with its nondeterministic output. To better match co-creation needs, we developed AI-steering tools, consisting of Voice Lanes that restrict content generation to particular voices; Example-Based Sliders to control the similarity of generated content to an existing example; Semantic Sliders to nudge music generation in high-level directions (happy/sad, conventional/surprising); and Multiple Alternatives of generated content to audition and choose from. In a summative study (N=21), we discovered the tools not only increased users’ trust, control, comprehension, and sense of collaboration with the AI, but also contributed to a greater sense of self-efﬁcacy and ownership of the composition relative to the AI.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Louie, Ryan and Coenen, Andy and Huang, Cheng Zhi and Terry, Michael and Cai, Carrie J.},
	month = apr,
	year = {2020},
	keywords = {co-creation, generative deep neural networks, human-ai interaction, Human-AI Partnerships},
	pages = {1--13},
	file = {Louie et al. - 2020 - Novice-AI Music Co-Creation via AI-Steering Tools .pdf:/Users/eleanorrow/Zotero/storage/J7SR93XU/Louie et al. - 2020 - Novice-AI Music Co-Creation via AI-Steering Tools .pdf:application/pdf},
}

@inproceedings{louieExpressiveCommunicationEvaluating2022a,
	address = {Helsinki Finland},
	title = {Expressive {Communication}: {Evaluating} {Developments} in {Generative} {Models} and {Steering} {Interfaces} for {Music} {Creation}},
	isbn = {978-1-4503-9144-3},
	shorttitle = {Expressive {Communication}},
	url = {https://dl.acm.org/doi/10.1145/3490099.3511159},
	doi = {10.1145/3490099.3511159},
	abstract = {There is an increasing interest from ML and HCI communities in empowering creators with better generative models and more intuitive interfaces with which to control them. In music, ML researchers have focused on training models capable of generating pieces with increasing long-range structure and musical coherence, while HCI researchers have separately focused on designing steering interfaces that support user control and ownership. In this study, we investigate how developments in both models and user interfaces are important for empowering co-creation where the goal is to create music that communicates particular imagery or ideas (e.g., as is common for other purposeful tasks in music creation like establishing mood or creating accompanying music for another media). Our study is distinguished in that it measures communication through both composer’s self-reported experiences, and how listeners evaluate this communication through the music. In an evaluation study with 26 composers creating 100+ pieces of music and listeners providing 1000+ head-to-head comparisons, we fnd that more expressive models and more steerable interfaces are important and complementary ways to make a diference in composers communicating through music and supporting their creative empowerment.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {27th {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {ACM},
	author = {Louie, Ryan and Engel, Jesse and Huang, Cheng-Zhi Anna},
	month = mar,
	year = {2022},
	pages = {405--417},
	file = {Louie et al. - 2022 - Expressive Communication Evaluating Developments .pdf:/Users/eleanorrow/Zotero/storage/9SAEXRRE/Louie et al. - 2022 - Expressive Communication Evaluating Developments .pdf:application/pdf},
}

@misc{luTransferringStyleHomophonic2018,
	title = {Transferring the {Style} of {Homophonic} {Music} {Using} {Recurrent} {Neural} {Networks} and {Autoregressive} {Models}},
	abstract = {Utilizing deep learning techniques to generate musical contents has caught wide attention in recent years. Within this context, this paper investigates a speciﬁc problem related to music generation, music style transfer. This practical problem aims to alter the style of a given music piece from one to another while preserving the essence of that piece, such as melody and chord progression. In particular, we discuss the style transfer of homophonic music, composed of a predominant melody part and an accompaniment part, where the latter is modiﬁed through Gibbs sampling on a generative model combining recurrent neural networks and autoregressive models. Both objective and subjective test experiment are performed to assess the performance of transferring the style of an arbitrary music piece having a homophonic texture into two different distinct styles, Bachs chorales and Jazz.},
	language = {en},
	author = {Lu, Wei-Tsung and Su, Li},
	year = {2018},
	file = {Lu and Su - 2018 - TRANSFERRING THE STYLE OF HOMOPHONIC MUSIC USING R.pdf:/Users/eleanorrow/Zotero/storage/29672S6B/Lu and Su - 2018 - TRANSFERRING THE STYLE OF HOMOPHONIC MUSIC USING R.pdf:application/pdf},
}

@inproceedings{luActionsSpeakLouder2021,
	title = {Actions {Speak} {Louder} than {Listening}: {Evaluating} {Music} {Style} {Transfer} based on {Editing} {Experience}},
	shorttitle = {Actions {Speak} {Louder} than {Listening}},
	url = {http://arxiv.org/abs/2110.12855},
	doi = {10.1145/3474085.3475529},
	abstract = {The subjective evaluation of music generation techniques has been mostly done with questionnaire-based listening tests while ignoring the perspectives from music composition, arrangement, and soundtrack editing. In this paper, we propose an editing test to evaluate users’ editing experience of music generation models in a systematic way. To do this, we design a new music style transfer model combining the non-chronological inference architecture, autoregressive models and the Transformer, which serves as an improvement from the baseline model on the same style transfer task. Then, we compare the performance of the two models with a conventional listening test and the proposed editing test, in which the quality of generated samples is assessed by the amount of effort (e.g., the number of required keyboard and mouse actions) spent by users to polish a music clip. Results on two target styles indicate that the improvement over the baseline model can be reflected by the editing test quantitatively. Also, the editing test provides profound insights which are not accessible from usual listening tests. The major contribution of this paper is the systematic presentation of the editing test and the corresponding insights, while the proposed music style transfer model based on state-of-the-art neural networks represents another contribution.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Multimedia}},
	author = {Lu, Wei-Tsung and Wu, Meng-Hsuan and Chiu, Yuh-Ming and Su, Li},
	month = oct,
	year = {2021},
	note = {arXiv:2110.12855 [cs, eess]},
	keywords = {artificial intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Computer Science - Multimedia, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, human-centered computing, music generation, neural networks, style transfer},
	pages = {3936--3944},
	annote = {Comment: 9 pages, Proceedings of the 29th ACM International Conference on Multimedia},
	annote = {ISBN: 9781450386517 \_eprint: arXiv:2110.12855v1},
	file = {Lu et al. - 2021 - Actions Speak Louder than Listening Evaluating Mu.pdf:/Users/eleanorrow/Zotero/storage/APRXWLT2/Lu et al. - 2021 - Actions Speak Louder than Listening Evaluating Mu.pdf:application/pdf},
}

@inproceedings{luceroSampleOneFirstPerson2019,
	address = {San Diego CA USA},
	title = {A {Sample} of {One}: {First}-{Person} {Research} {Methods} in {HCI}},
	isbn = {978-1-4503-6270-2},
	shorttitle = {A {Sample} of {One}},
	url = {https://dl.acm.org/doi/10.1145/3301019.3319996},
	doi = {10.1145/3301019.3319996},
	abstract = {First-person research (i.e., research that involves data collection and experiences from the researcher themselves) continues to become a viable addition and, possibly even, alternative to more traditional HCI methods. While we have seen the benefits of using methods such as autoethnography, autobiographical design, and autoethnographical research through design, we also see the need to further explore, define, and investigate the practices, techniques, tactics, and implications of first-person research in HCI. To address this, this one-day workshop aims to bring together a community of researchers, designers, and practitioners who are interested in exploring and reimagining research in HCI and interaction design, with an emphasis on first-person methods.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {Companion {Publication} of the 2019 on {Designing} {Interactive} {Systems} {Conference} 2019 {Companion}},
	publisher = {ACM},
	author = {Lucero, Andrés and Desjardins, Audrey and Neustaedter, Carman and Höök, Kristina and Hassenzahl, Marc and Cecchinato, Marta E.},
	month = jun,
	year = {2019},
	keywords = {autobiographical design, autoethnography, design research, first-person research, research through design},
	pages = {385--388},
	file = {Lucero et al. - 2019 - A Sample of One First-Person Research Methods in .pdf:/Users/eleanorrow/Zotero/storage/FCAC79BL/Lucero et al. - 2019 - A Sample of One First-Person Research Methods in .pdf:application/pdf},
}

@misc{lugmayrRePaintInpaintingUsing2022,
	title = {{RePaint}: {Inpainting} using {Denoising} {Diffusion} {Probabilistic} {Models}},
	shorttitle = {{RePaint}},
	url = {http://arxiv.org/abs/2201.09865},
	abstract = {Free-form inpainting is the task of adding new content to an image in the regions specified by an arbitrary binary mask. Most existing approaches train for a certain distribution of masks, which limits their generalization capabilities to unseen mask types. Furthermore, training with pixel-wise and perceptual losses often leads to simple textural extensions towards the missing areas instead of semantically meaningful generation. In this work, we propose RePaint: A Denoising Diffusion Probabilistic Model (DDPM) based inpainting approach that is applicable to even extreme masks. We employ a pretrained unconditional DDPM as the generative prior. To condition the generation process, we only alter the reverse diffusion iterations by sampling the unmasked regions using the given image information. Since this technique does not modify or condition the original DDPM network itself, the model produces high-quality and diverse output images for any inpainting form. We validate our method for both faces and general-purpose image inpainting using standard and extreme masks. RePaint outperforms state-of-the-art Autoregressive, and GAN approaches for at least five out of six mask distributions. Github Repository: git.io/RePaint},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Lugmayr, Andreas and Danelljan, Martin and Romero, Andres and Yu, Fisher and Timofte, Radu and Van Gool, Luc},
	month = aug,
	year = {2022},
	note = {arXiv:2201.09865 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: We missed out on other diffusion models that work on inpainting. We corrected that and apologize for this mistake},
	annote = {Issue: arXiv:2201.09865 arXiv: 2201.09865},
	file = {Lugmayr et al. - 2022 - RePaint Inpainting using Denoising Diffusion Prob.pdf:/Users/eleanorrow/Zotero/storage/79G2C7PP/Lugmayr et al. - 2022 - RePaint Inpainting using Denoising Diffusion Prob.pdf:application/pdf},
}

@misc{luoCAPTContrastivePreTraining2020,
	title = {{CAPT}: {Contrastive} {Pre}-{Training} for {Learning} {Denoised} {Sequence} {Representations}},
	shorttitle = {{CAPT}},
	url = {http://arxiv.org/abs/2010.06351},
	abstract = {Pre-trained self-supervised models such as BERT have achieved striking success in learning sequence representations, especially for natural language processing. These models typically corrupt the given sequences with certain types of noise, such as masking, shufﬂing, or substitution, and then try to recover the original input. However, such pre-training approaches are prone to learning representations that are covariant with the noise, leading to the discrepancy between the pre-training and ﬁne-tuning stage. To remedy this, we present ContrAstive Pre-Training (CAPT) to learn noise invariant sequence representations. The proposed CAPT encourages the consistency between representations of the original sequence and its corrupted version via unsupervised instance-wise training signals. In this way, it not only alleviates the pretrain-ﬁnetune discrepancy induced by the noise of pre-training, but also aids the pre-trained model in better capturing global semantics of the input via more effective sentence-level supervision. Different from most prior work that focuses on a particular modality, comprehensive empirical evidence on 11 natural language understanding and cross-modal tasks illustrates that CAPT is applicable for both language and vision-language tasks, and obtains surprisingly consistent improvement, including 0.6\% absolute gain on GLUE benchmarks and 0.8\% absolute increment on NLVR2.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Luo, Fuli and Yang, Pengcheng and Li, Shicheng and Ren, Xuancheng and Sun, Xu},
	month = oct,
	year = {2020},
	note = {arXiv:2010.06351 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Corrected typos},
	annote = {\_eprint: 2010.06351},
	file = {Luo et al. - 2020 - CAPT Contrastive Pre-Training for Learning Denois.pdf:/Users/eleanorrow/Zotero/storage/EX82FCME/Luo et al. - 2020 - CAPT Contrastive Pre-Training for Learning Denois.pdf:application/pdf},
}

@inproceedings{lupkerScoreTransformerDeepLearning2021,
	address = {Shanghai, China},
	title = {Score-{Transformer}: {A} {Deep} {Learning} {Aid} for {Music} {Composition}},
	shorttitle = {Score-{Transformer}},
	url = {https://nime.pubpub.org/pub/7a6ij1ak},
	doi = {10.21428/92fbeb44.21d4fd1f},
	abstract = {Creating an artificially intelligent (AI) aid for music composers requires a practical and modular approach, one that allows the composer to manipulate the technology when needed in the search for new sounds. Many existing approaches fail to capture the interest of composers as they are limited beyond their demonstrative purposes, allow for only minimal interaction from the composer or require GPU access to generate samples quickly. This paper introduces Score-Transformer (ST), a practical integration of deep learning technology to aid in the creation of new music which works seamlessly alongside any popular software notation (Finale, Sibelius, etc.). ScoreTransformer is built upon a variant of the powerful transformer model, currently used in state-of-the-art natural language models. Owing to hierarchical and sequential similarities between music and language, the transformer model can learn to write polyphonic MIDI music based on any styles, genres, or composers it is trained upon. This paper briefly outlines how the model learns and later notates music based upon any prompt given to it from the user. Furthermore, ST can be updated at any time on additional MIDI recordings minimizing the risk of the software becoming outdated or impractical for continued use.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {{NIME} 2021},
	publisher = {PubPub},
	author = {Lupker, Jeffrey A. T.},
	month = jun,
	year = {2021},
	annote = {https://nime.pubpub.org/pub/7a6ij1ak},
	file = {Lupker - 2021 - Score-Transformer A Deep Learning Aid for Music C.pdf:/Users/eleanorrow/Zotero/storage/KS5BZ8QJ/Lupker - 2021 - Score-Transformer A Deep Learning Aid for Music C.pdf:application/pdf},
}

@misc{marafiotiAudioInpaintingMusic2022,
	title = {Audio inpainting of music by means of neural networks},
	url = {http://arxiv.org/abs/1810.12138},
	abstract = {We study the ability of deep neural networks (DNNs) to restore missing audio content based on its context, i.e., inpaint audio gaps. We focus on a condition which has not received much attention yet: gaps in the range of tens of milliseconds. We propose a DNN structure that is provided with the signal surrounding the gap in the form of time-frequency (TF) coeﬃcients. Two DNNs with either complex-valued TF coeﬃcient output or magnitude TF coeﬃcient output were studied by separately training them on inpainting two types of audio signals (music and musical instruments) having 64-ms long gaps. The magnitude DNN outperformed the complex-valued DNN in terms of signal-to-noise ratios and objective diﬀerence grades. Although, for instruments, a reference inpainting obtained through linear predictive coding performed better in both metrics, it performed worse than the magnitude DNN for music. This demonstrates the potential of the magnitude DNN, in particular for inpainting signals that are more complex than single instrument sounds.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Marafioti, Andrés and Holighaus, Nicki and Majdak, Piotr and Perraudin, Nathanaël},
	month = feb,
	year = {2022},
	note = {arXiv:1810.12138 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: Presented at the 146th AES Convention [arXiv:1810.12138v2]. For the journal version, published in published in IEEE TASLP, see [arXiv:1810.12138v2]},
	file = {Marafioti et al. - 2022 - Audio inpainting of music by means of neural netwo.pdf:/Users/eleanorrow/Zotero/storage/BPXUTCSR/Marafioti et al. - 2022 - Audio inpainting of music by means of neural netwo.pdf:application/pdf},
}

@article{marafiotiGACELAGenerativeAdversarial2021,
	title = {{GACELA} -- {A} generative adversarial context encoder for long audio inpainting},
	volume = {15},
	issn = {1932-4553, 1941-0484},
	url = {http://arxiv.org/abs/2005.05032},
	doi = {10.1109/JSTSP.2020.3037506},
	abstract = {We introduce GACELA, a generative adversarial network (GAN) designed to restore missing musical audio data with a duration ranging between hundreds of milliseconds to a few seconds, i.e., to perform long-gap audio inpainting. While previous work either addressed shorter gaps or relied on exemplars by copying available information from other signal parts, GACELA addresses the inpainting of long gaps in two aspects. First, it considers various time scales of audio information by relying on five parallel discriminators with increasing resolution of receptive fields. Second, it is conditioned not only on the available information surrounding the gap, i.e., the context, but also on the latent variable of the conditional GAN. This addresses the inherent multi-modality of audio inpainting at such long gaps and provides the option of user-defined inpainting. GACELA was tested in listening tests on music signals of varying complexity and gap durations ranging from 375{\textasciitilde}ms to 1500{\textasciitilde}ms. While our subjects were often able to detect the inpaintings, the severity of the artifacts decreased from unacceptable to mildly disturbing. GACELA represents a framework capable to integrate future improvements such as processing of more auditory-related features or more explicit musical features.},
	language = {en},
	number = {1},
	urldate = {2023-12-30},
	journal = {IEEE Journal of Selected Topics in Signal Processing},
	author = {Marafioti, Andres and Majdak, Piotr and Holighaus, Nicki and Perraudin, Nathanaël},
	month = jan,
	year = {2021},
	note = {arXiv:2005.05032 [cs, eess, stat]},
	keywords = {Audio, cGAN, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, GAN, Inpainting, Models, Statistics - Machine Learning},
	pages = {120--131},
	annote = {\_eprint: arXiv:2005.05032v1},
	file = {Marafioti et al. - 2021 - GACELA -- A generative adversarial context encoder.pdf:/Users/eleanorrow/Zotero/storage/84JNPK3P/Marafioti et al. - 2021 - GACELA -- A generative adversarial context encoder.pdf:application/pdf},
}

@article{martinUnderstandingMusicalPredictions2020,
	title = {Understanding {Musical} {Predictions} {With} an {Embodied} {Interface} for {Musical} {Machine} {Learning}},
	volume = {3},
	issn = {2624-8212},
	url = {https://www.frontiersin.org/article/10.3389/frai.2020.00006/full},
	doi = {10.3389/frai.2020.00006},
	abstract = {Machine-learning models of music often exist outside the worlds of musical performance practice and abstracted from the physical gestures of musicians. In this work, we consider how a recurrent neural network (RNN) model of simple music gestures may be integrated into a physical instrument so that predictions are sonically and physically entwined with the performer’s actions. We introduce EMPI, an embodied musical prediction interface that simpliﬁes musical interaction and prediction to just one dimension of continuous input and output. The predictive model is a mixture density RNN trained to estimate the performer’s next physical input action and the time at which this will occur. Predictions are represented sonically through synthesized audio, and physically with a motorized output indicator. We use EMPI to investigate how performers understand and exploit different predictive models to make music through a controlled study of performances with different models and levels of physical feedback. We show that while performers often favor a model trained on human-sourced data, they ﬁnd different musical affordances in models trained on synthetic, and even random, data. Physical representation of predictions seemed to affect the length of performances. This work contributes new understandings of how musicians use generative ML models in real-time performance backed up by experimental evidence. We argue that a constrained musical interface can expose the affordances of embodied predictive interactions.},
	language = {en},
	urldate = {2023-12-30},
	journal = {Frontiers in Artificial Intelligence},
	author = {Martin, Charles Patrick and Glette, Kyrre and Nygaard, Tønnes Frostad and Torresen, Jim},
	month = mar,
	year = {2020},
	pages = {6},
	file = {Martin et al. - 2020 - Understanding Musical Predictions With an Embodied.pdf:/Users/eleanorrow/Zotero/storage/FTY5WJ85/Martin et al. - 2020 - Understanding Musical Predictions With an Embodied.pdf:application/pdf},
}

@misc{mathieuDisentanglingDisentanglementVariational2019,
	title = {Disentangling {Disentanglement} in {Variational} {Autoencoders}},
	url = {http://arxiv.org/abs/1812.02833},
	abstract = {We develop a generalisation of disentanglement in variational autoencoders (VAEs)—decomposition of the latent representation—characterising it as the fulﬁlment of two factors: a) the latent encodings of the data having an appropriate level of overlap, and b) the aggregate encoding of the data conforming to a desired structure, represented through the prior. Decomposition permits disentanglement, i.e. explicit independence between latents, as a special case, but also allows for a much richer class of properties to be imposed on the learnt representation, such as sparsity, clustering, independent subspaces, or even intricate hierarchical dependency relationships. We show that the β-VAE varies from the standard VAE predominantly in its control of latent overlap and that for the standard choice of an isotropic Gaussian prior, its objective is invariant to rotations of the latent representation. Viewed from the decomposition perspective, breaking this invariance with simple manipulations of the prior can yield better disentanglement with little or no detriment to reconstructions. We further demonstrate how other choices of prior can assist in producing different decompositions and introduce an alternative training objective that allows the control of both decomposition factors in a principled manner.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Mathieu, Emile and Rainforth, Tom and Siddharth, N. and Teh, Yee Whye},
	month = jun,
	year = {2019},
	note = {arXiv:1812.02833 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Accepted for publication at ICML 2019},
	annote = {ISBN: 9781510886988 \_eprint: 1812.02833},
	file = {Mathieu et al. - 2019 - Disentangling Disentanglement in Variational Autoe.pdf:/Users/eleanorrow/Zotero/storage/BQ83W6B9/Mathieu et al. - 2019 - Disentangling Disentanglement in Variational Autoe.pdf:application/pdf},
}

@inproceedings{mcgarryTheyReAll2017,
	address = {Portland Oregon USA},
	title = {"{They}'re all going out to something weird": {Workflow}, {Legacy} and {Metadata} in the {Music} {Production} {Process}},
	isbn = {978-1-4503-4335-0},
	shorttitle = {"{They}'re all going out to something weird"},
	url = {https://dl.acm.org/doi/10.1145/2998181.2998325},
	doi = {10.1145/2998181.2998325},
	abstract = {In this paper we use results from two ethnographic studies of the music production process to examine some key issues regarding how work is currently accomplished in studio production environments. These issues relate in particular to workflows and how metadata is adapted to the specific needs of specific parts of the process. We find that there can be significant tensions between how reasoning is applied to metadata at different stages of production and that this can lead to overheads where metadata has to be either changed or created anew to make the process work. On the basis of these findings we articulate some of the potential solutions we are now examining. These centre in particular upon the notions of Digital/Dynamic Musical Objects and flexible metadata shells.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {Proceedings of the 2017 {ACM} {Conference} on {Computer} {Supported} {Cooperative} {Work} and {Social} {Computing}},
	publisher = {ACM},
	author = {McGarry, Glenn and Tolmie, Peter and Benford, Steve and Greenhalgh, Chris and Chamberlain, Alan},
	month = feb,
	year = {2017},
	pages = {995--1008},
	annote = {event-place: Portland Oregon USA},
	file = {McGarry et al. - 2017 - They're all going out to something weird Workfl.pdf:/Users/eleanorrow/Zotero/storage/FLXZEJ3Y/McGarry et al. - 2017 - They're all going out to something weird Workfl.pdf:application/pdf},
}

@inproceedings{mcgarryMeaningMixUsing2021,
	address = {virtual/Trento Italy},
	title = {The {Meaning} in "the {Mix}": {Using} {Ethnography} to {Inform} the {Design} of {Intelligent} {Tools} in the {Context} of {Music} {Production}},
	isbn = {978-1-4503-8569-5},
	shorttitle = {The {Meaning} in "the {Mix}"},
	url = {https://dl.acm.org/doi/10.1145/3478384.3478406},
	doi = {10.1145/3478384.3478406},
	abstract = {In this paper we report on two ethnographic studies of professional music producers at work in their respective studio settings, to underpin the design of intelligent tools and platforms in this domain. The studies are part of a body of work that explores this complex and technically challenging domain and explicates the ways in which the actors involved address the tension between artistic and engineering practices. This report focusses on the flow of work in the creation of a song in a digital audio workstation (DAW), which often eschews the technical requirement to document the process to maintain a creative “vibe”, and the troubles that occur in subsequent stages of the production process in which complex and often messy compositions of audio data are handed over. We conclude with implications for metadata used in the process, namely the labelling and organisation of audio, to drive tools that allow more control over the creative process by automating process tracking and documenting song data provenance.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {Audio {Mostly} 2021},
	publisher = {ACM},
	author = {McGarry, Glenn and Chamberlain, Alan and Crabtree, Andy and Greehalgh, Christopher},
	month = sep,
	year = {2021},
	pages = {40--47},
	annote = {event-place: virtual/Trento Italy},
	file = {McGarry et al. - 2021 - The Meaning in the Mix Using Ethnography to Inf.pdf:/Users/eleanorrow/Zotero/storage/TLXEEW6C/McGarry et al. - 2021 - The Meaning in the Mix Using Ethnography to Inf.pdf:application/pdf},
}

@incollection{mcphersonMusicDesignEthnography2019a,
	address = {Cham},
	title = {Music, {Design} and {Ethnography}: {An} {Interview} with {Steve} {Benford}},
	isbn = {978-3-319-92068-9 978-3-319-92069-6},
	shorttitle = {Music, {Design} and {Ethnography}},
	url = {http://link.springer.com/10.1007/978-3-319-92069-6_13},
	abstract = {Steve Benford is Professor of Collaborative Computing at the Mixed Reality Laboratory at the University of Nottingham. His research interests span creative and cultural applications of computing, from interactive art to mainstream entertainment, with a particular focus on new interaction techniques. He has over twenty years experience of collaborating with artists to create, tour and study interactive performances. Steve’s research has fuelled the emergence of new cultural forms such as pervasive games and mixed reality performance, while also delivering foundational principles for user experience design, most notably his work on trajectories, uncomfortable interactions, spectator interfaces and most recently the hybrid craft of making of physical-digital artefacts. He has previously held an EPSRC Dream Fellowship (2011–2014), a Visiting Professor at the BBC in 2012 and a Visiting Researcher at Microsoft Research in 2013.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {New {Directions} in {Music} and {Human}-{Computer} {Interaction}},
	publisher = {Springer International Publishing},
	author = {McPherson, Andrew and Benford, Steve},
	editor = {Holland, Simon and Mudd, Tom and Wilkie-McKenna, Katie and McPherson, Andrew and Wanderley, Marcelo M.},
	year = {2019},
	doi = {10.1007/978-3-319-92069-6_13},
	note = {Series Title: Springer Series on Cultural Computing},
	pages = {213--220},
	file = {McPherson and Benford - 2019 - Music, Design and Ethnography An Interview with S.pdf:/Users/eleanorrow/Zotero/storage/RI6CVXIV/McPherson and Benford - 2019 - Music, Design and Ethnography An Interview with S.pdf:application/pdf},
}

@misc{mikolovEfficientEstimationWord2013,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {http://arxiv.org/abs/1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = sep,
	year = {2013},
	note = {arXiv:1301.3781 [cs]},
	keywords = {Computer Science - Computation and Language, On paper},
	annote = {Pages: arXiv:1301.3781 Publication Title: arXiv e-prints},
	file = {Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf:/Users/eleanorrow/Zotero/storage/XFGCDLWZ/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf:application/pdf},
}

@article{mirandaMusicEmergentBehavior2003,
	title = {On the {Music} of {Emergent} {Behavior}: {What} {Can} {Evolutionary} {Computation} {Bring} to the {Musician}?},
	volume = {36},
	issn = {0024-094X, 1530-9282},
	shorttitle = {On the {Music} of {Emergent} {Behavior}},
	url = {https://direct.mit.edu/leon/article/36/1/55-59/44322},
	doi = {10.1162/002409403321152329},
	abstract = {In this article, the author focuses on issues concerning musical composition practices in which emergent behavior is used to generate musical material, musical form or both. The author gives special attention to the potential of cellular automata and adaptive imitation games for music-making. The article begins by presenting two case-study systems, followed by an assessment of their role in the composition of a number of pieces. It then continues with a discussion in which the author suggests that adaptive imitation games may hold the key to fostering more effective links between evolutionary computation paradigms and creative musical processes.},
	language = {en},
	number = {1},
	urldate = {2023-12-30},
	journal = {Leonardo},
	author = {Miranda, Eduardo Reck},
	year = {2003},
	pages = {55--59},
	file = {Miranda - 2003 - On the Music of Emergent Behavior What Can Evolut.pdf:/Users/eleanorrow/Zotero/storage/EKQH6VTB/Miranda - 2003 - On the Music of Emergent Behavior What Can Evolut.pdf:application/pdf},
}

@misc{mittalSymbolicMusicGeneration2021,
	title = {Symbolic {Music} {Generation} with {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2103.16091},
	abstract = {Score-based generative models and diffusion probabilistic models have been successful at generating high-quality samples in a variety of continuous domains. However, due to their Langevin-inspired sampling mechanisms, their application to discrete symbolic music data has been limited. In this work, we present a technique for training diffusion models on symbolic music data by parameterizing the discrete domain in the continuous latent space of a pre-trained variational autoencoder. Our method is nonautoregressive and learns to generate sequences of latent embeddings through the reverse process and offers parallel generation with a constant number of iterative reﬁnement steps. We show strong unconditional generation and posthoc conditional inﬁlling results compared to autoregressive language models operating over the same continuous embeddings.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Mittal, Gautam and Engel, Jesse and Hawthorne, Curtis and Simon, Ian},
	month = nov,
	year = {2021},
	note = {arXiv:2103.16091 [cs, eess, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	annote = {Comment: ISMIR 2021},
	file = {Mittal et al. - 2021 - Symbolic Music Generation with Diffusion Models.pdf:/Users/eleanorrow/Zotero/storage/MDNAYUH7/Mittal et al. - 2021 - Symbolic Music Generation with Diffusion Models.pdf:application/pdf},
}

@incollection{mitzenmacherEstimatingResemblanceMIDI2001,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Estimating {Resemblance} of {MIDI} {Documents}},
	volume = {2153},
	isbn = {978-3-540-42560-1 978-3-540-44808-2},
	url = {http://link.springer.com/10.1007/3-540-44808-X_6},
	abstract = {Search engines often employ techniques for determining syntactic similarity of Web pages. Such a tool allows them to avoid returning multiple copies of essentially the same page when a user makes a query. Here we describe our experience extending these techniques to MIDI music les. The music domain requires modi cation to cope with problems introduced in the musical setting, such as polyphony. Our experience suggests that when used properly these techniques prove useful for determining duplicates and clustering databases in the musical setting as well.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {Algorithm {Engineering} and {Experimentation}},
	publisher = {Springer Berlin Heidelberg},
	author = {Mitzenmacher, Michael and Owen, Sean},
	editor = {Goos, Gerhard and Hartmanis, Juris and Van Leeuwen, Jan and Buchsbaum, Adam L. and Snoeyink, Jack},
	year = {2001},
	doi = {10.1007/3-540-44808-X_6},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {78--90},
	file = {Mitzenmacher and Owen - 2001 - Estimating Resemblance of MIDI Documents.pdf:/Users/eleanorrow/Zotero/storage/LRYCGBHN/Mitzenmacher and Owen - 2001 - Estimating Resemblance of MIDI Documents.pdf:application/pdf},
}

@inproceedings{mivielleWhatLudicLudic2016,
	title = {What is ludic about ludic design ? {A} back and forth between theory and practice.},
	isbn = {978-1-84387-393-8},
	shorttitle = {What is ludic about ludic design ?},
	url = {http://ead.yasar.edu.tr/sites/default/files/Track%2021_WHAT%20IS%20LUDIC%20ABOUT%20LUDIC%20DESIGN_%20A%20BACK%20AND%20FORTH%20BETWEEN%20THEORY%20AND%20PRACTICE.pdf},
	doi = {10.7190/ead/2015/152},
	abstract = {This paper focuses on ludic design as part of the paradigm of design research that explores new ways to implement computer systems. We want to further explore the specificity of ludic design using research through. First, we introduce Bill Gaver’s definition and compare his claim to theories of ludicity that were developed in anthropology and psychology. Secondly, we analyze a portfolio of artifacts to better understand how ludicity is embedded in the designs and to reveal the underlying assumptions. We come up with three additional characteristics of ludic design: unconventionality, serendipity, and reflexivity through breaching experiment. Finally, we test our understanding of ludicity through a research artifact to validate our hypothesis.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {11th {EAD} {Conference} {Proceedings}: {The} {Value} of {Design} {Research}},
	publisher = {Sheffield Hallam University},
	author = {Mivielle, Cedric},
	month = aug,
	year = {2016},
	file = {Mivielle - 2016 - What is ludic about ludic design  A back and fort.pdf:/Users/eleanorrow/Zotero/storage/3XKGRUKK/Mivielle - 2016 - What is ludic about ludic design  A back and fort.pdf:application/pdf},
}

@misc{mogrenCRNNGANContinuousRecurrent2016,
	title = {C-{RNN}-{GAN}: {Continuous} recurrent neural networks with adversarial training},
	shorttitle = {C-{RNN}-{GAN}},
	url = {http://arxiv.org/abs/1611.09904},
	abstract = {Generative adversarial networks have been proposed as a way of efﬁciently training deep generative neural networks. We propose a generative adversarial model that works on continuous sequential data, and apply it by training it on a collection of classical music. We conclude that it generates music that sounds better and better as the model is trained, report statistics on generated music, and let the reader judge the quality by downloading the generated songs.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Mogren, Olof},
	month = nov,
	year = {2016},
	note = {arXiv:1611.09904 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: Accepted to Constructive Machine Learning Workshop (CML) at NIPS 2016 in Barcelona, Spain, December 10},
	annote = {Issue: arXiv:1611.09904 arXiv: 1611.09904},
	file = {Mogren - 2016 - C-RNN-GAN Continuous recurrent neural networks wi.pdf:/Users/eleanorrow/Zotero/storage/B5IX6U3N/Mogren - 2016 - C-RNN-GAN Continuous recurrent neural networks wi.pdf:application/pdf},
}

@article{mohaimenuzzamanEnvironmentalSoundClassification2023,
	title = {Environmental {Sound} {Classification} on the {Edge}: {A} {Pipeline} for {Deep} {Acoustic} {Networks} on {Extremely} {Resource}-{Constrained} {Devices}},
	volume = {133},
	issn = {00313203},
	shorttitle = {Environmental {Sound} {Classification} on the {Edge}},
	url = {http://arxiv.org/abs/2103.03483},
	doi = {10.1016/j.patcog.2022.109025},
	abstract = {Signiﬁcant eﬀorts are being invested to bring state-of-the-art classiﬁcation and recognition to edge devices with extreme resource constraints (memory, speed and lack of GPU support). Here, we demonstrate the ﬁrst deep network for acoustic recognition that is small enough for an oﬀ-the-shelf microcontroller, yet achieves state-of-the-art performance on standard benchmarks. Rather than handcrafting a once-oﬀ solution, we present a universal pipeline that converts a large deep convolutional network automatically via compression and quantization into a network for resource-impoverished edge devices. After introducing ACDNet, which produces above state-of-the-art accuracy on ESC-10 (96.65\%) and ESC-50 (87.1\%), we describe the compression pipeline and show that it allows us to achieve 97.22\% size reduction and 97.28\% FLOP reduction while maintaining close to state-of-the-art accuracy (83.65\% on ESC-50). We describe a successful implementation on a standard oﬀ-the shelf microcontroller and, beyond laboratory benchmarks, report successful tests on real-world data sets.},
	language = {en},
	urldate = {2023-12-30},
	journal = {Pattern Recognition},
	author = {Mohaimenuzzaman, Md and Bergmeir, Christoph and West, Ian Thomas and Meyer, Bernd},
	month = jan,
	year = {2023},
	note = {arXiv:2103.03483 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {109025},
	file = {Mohaimenuzzaman et al. - 2023 - Environmental Sound Classification on the Edge A .pdf:/Users/eleanorrow/Zotero/storage/IQF78QVY/Mohaimenuzzaman et al. - 2023 - Environmental Sound Classification on the Edge A .pdf:application/pdf},
}

@article{mooreDysfunctionsMIDI1988,
	title = {The {Dysfunctions} of {MIDI}},
	volume = {12},
	issn = {01489267},
	url = {https://www.jstor.org/stable/3679834?origin=crossref},
	doi = {10.2307/3679834},
	abstract = {The Musical Instrument Digital Interface (MIDI) is now a de facto standard for the digital representation of musical events. Actions of live musical performers are being 'MIDIfied', commercial software is being based on MIDI-gated conceptions, and digital synthesizers are being slaved to MIDI masters. MIDI-based hardware and software is also proliferating at a tremendous rate, virtually ensuring that the characteristics of MIDI will play an important role in shaping a significant portion of future music. This paper concentrates on known dysfunctions of MIDI from a purely musical point of view, paying particular attention to performance capture, the digital representation of musical control processes, and synthesizer control.},
	number = {1},
	urldate = {2023-12-30},
	journal = {Computer Music Journal},
	author = {Moore, F. Richard},
	year = {1988},
	pages = {19},
	file = {Moore - 1988 - The Dysfunctions of MIDI.pdf:/Users/eleanorrow/Zotero/storage/J2DV62TR/Moore - 1988 - The Dysfunctions of MIDI.pdf:application/pdf},
}

@article{mullensiefenMusicalityNonMusiciansIndex2014,
	title = {The {Musicality} of {Non}-{Musicians}: {An} {Index} for {Assessing} {Musical} {Sophistication} in the {General} {Population}},
	volume = {9},
	issn = {1932-6203},
	shorttitle = {The {Musicality} of {Non}-{Musicians}},
	url = {https://dx.plos.org/10.1371/journal.pone.0089642},
	doi = {10.1371/journal.pone.0089642},
	abstract = {Musical skills and expertise vary greatly in Western societies. Individuals can differ in their repertoire of musical behaviours as well as in the level of skill they display for any single musical behaviour. The types of musical behaviours we refer to here are broad, ranging from performance on an instrument and listening expertise, to the ability to employ music in functional settings or to communicate about music. In this paper, we first describe the concept of ‘musical sophistication’ which can be used to describe the multi-faceted nature of musical expertise. Next, we develop a novel measurement instrument, the Goldsmiths Musical Sophistication Index (Gold-MSI) to assess self-reported musical skills and behaviours on multiple dimensions in the general population using a large Internet sample (n = 147,636). Thirdly, we report results from several lab studies, demonstrating that the Gold-MSI possesses good psychometric properties, and that self-reported musical sophistication is associated with performance on two listening tasks. Finally, we identify occupation, occupational status, age, gender, and wealth as the main socio-demographic factors associated with musical sophistication. Results are discussed in terms of theoretical accounts of implicit and statistical music learning and with regard to social conditions of sophisticated musical engagement.},
	language = {en},
	number = {2},
	urldate = {2023-12-30},
	journal = {PLoS ONE},
	author = {Müllensiefen, Daniel and Gingras, Bruno and Musil, Jason and Stewart, Lauren},
	editor = {Snyder, Joel},
	month = feb,
	year = {2014},
	keywords = {Aptitude tests, Behavior, Bioacoustics, Emotions, Music cognition, Music perception, Professions, Psychometrics},
	pages = {e89642},
	annote = {Publisher: Public Library of Science},
	file = {Müllensiefen et al. - 2014 - The Musicality of Non-Musicians An Index for Asse.pdf:/Users/eleanorrow/Zotero/storage/S9P5LY9I/Müllensiefen et al. - 2014 - The Musicality of Non-Musicians An Index for Asse.pdf:application/pdf},
}

@misc{nashCrowddrivenMusicInteractive,
	title = {Crowd-driven {Music}: {Interactive} and {Generative} {Approaches} using {Machine} {Vision} and {Manhattan}},
	abstract = {This paper details technologies and artistic approaches to crowddriven music, discussed in the context of a live public installation in which activity in a public space (e.g. a busy railway platform) is used to drive the automated composition and performance of music. The approach presented uses realtime machine vision applied to a live video feed of a scene, from which detected objects and people are fed into Manhattan (Nash, 2014), a digital music notation that integrates sequencing and programming to support the live creation of complex musical works that combine static, algorithmic, and interactive elements. The paper discusses the technical details of the system and artistic development of specific musical works, introducing novel techniques to mapping chaotic systems to musical expression and exploring issues of agency, aesthetic, accessibility, and adaptability relating to composing interactive music for crowds and public spaces. In particular, performances as part of an installation for BBC Music Day 2018 are described.},
	language = {en},
	author = {Nash, Chris},
	file = {Nash - Crowd-driven Music Interactive and Generative App.pdf:/Users/eleanorrow/Zotero/storage/NNMNX9EV/Nash - Crowd-driven Music Interactive and Generative App.pdf:application/pdf},
}

@misc{neillOverviewNeuralNetwork2020,
	title = {An {Overview} of {Neural} {Network} {Compression}},
	url = {http://arxiv.org/abs/2006.03669},
	abstract = {Overparameterized networks trained to convergence have shown impressive performance in domains such as computer vision and natural language processing. Pushing state of the art on salient tasks within these domains corresponds to these models becoming larger and more diﬃcult for machine learning practitioners to use given the increasing memory and storage requirements, not to mention the larger carbon footprint. Thus, in recent years there has been a resurgence in model compression techniques, particularly for deep convolutional neural networks and self-attention based networks such as the Transformer.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Neill, James O'},
	month = aug,
	year = {2020},
	note = {arXiv:2006.03669 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 53 pages},
	annote = {\_eprint: 2006.03669},
	file = {Neill - 2020 - An Overview of Neural Network Compression.pdf:/Users/eleanorrow/Zotero/storage/U5GDFCZY/Neill - 2020 - An Overview of Neural Network Compression.pdf:application/pdf},
}

@article{newmanHUMANAIMUSICCREATION2023,
	title = {{HUMAN}-{AI} {MUSIC} {CREATION}: {UNDERSTANDING} {THE} {PERCEPTIONS} {AND} {EXPERIENCES} {OF} {MUSIC} {CREATORS} {FOR} {ETHICAL} {AND} {PRODUCTIVE} {COLLABORATION}},
	abstract = {Recently, there has been a surge in Artiﬁcial Intelligence (AI) tools that allow creators to develop melodies, harmonies, lyrics, and mixes with the touch of a button. The reception of and discussion on the use of these tools - and more broadly, any AI-based art creation tools - tend to be polarizing, with opinions ranging from enthusiasm about their potential to fear about how these tools will impact the livelihood and creativity of human creators. However, a more desirable future path is most likely somewhere in between these two polar opposites where productive and ethical human-AI collaboration could happen through the use of these tools. To explore this possibility, we ﬁrst need to improve our understanding of how music creators perceive and utilize these types of tools in their creative process. We conducted case studies of a range of music creators to better understand their perception and usage of AI-based music creation tools. Through a thematic analysis of these cases, we identify the opportunities and challenges related to the use of AI for music creation from the perspective of the musicians and discuss the design implications for AI music tools.},
	language = {en},
	author = {Newman, Michele and Morris, Lidia and Lee, Jin Ha},
	year = {2023},
	file = {Newman et al. - 2023 - HUMAN-AI MUSIC CREATION UNDERSTANDING THE PERCEPT.pdf:/Users/eleanorrow/Zotero/storage/3TILGJUT/Newman et al. - 2023 - HUMAN-AI MUSIC CREATION UNDERSTANDING THE PERCEPT.pdf:application/pdf},
}

@inproceedings{reedExploringExperiencesNew2022,
	address = {The University of Auckland, New Zealand},
	title = {Exploring {Experiences} with {New} {Musical} {Instruments} through {Micro}-phenomenology},
	url = {https://nime.pubpub.org/pub/o7sza3sq},
	doi = {10.21428/92fbeb44.b304e4b1},
	abstract = {This paper introduces micro-phenomenology, a research discipline for exploring and uncovering the structures of lived experience, as a beneficial methodology for studying and evaluating interactions with digital musical instruments. Compared to other subjective methods, micro-phenomenology evokes and returns one to the moment of experience, allowing access to dimensions and observations which may not be recalled in reflection alone. We present a case study of five micro-phenomenological interviews conducted with musicians about their experiences with existing digital musical instruments. The interviews reveal deep, clear descriptions of different modalities of synchronic moments in interaction, especially in tactile connections and bodily sensations. We highlight the elements of interaction captured in these interviews which would not have been revealed otherwise and the importance of these elements in researching perception, understanding, interaction, and performance with digital musical instruments.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {{NIME} 2022},
	publisher = {PubPub},
	author = {Reed, Courtney N. and Nordmoen, Charlotte and Martelloni, Andrea and Lepri, Giacomo and Robson, Nicole and Zayas-Garin, Eevee and Cotton, Kelsey and Mice, Lia and McPherson, Andrew},
	month = jun,
	year = {2022},
	file = {Reed et al. - 2022 - Exploring Experiences with New Musical Instruments.pdf:/Users/eleanorrow/Zotero/storage/SZJMZGFQ/Reed et al. - 2022 - Exploring Experiences with New Musical Instruments.pdf:application/pdf},
}

@article{omodhrainFrameworkEvaluationDigital2011a,
	title = {A {Framework} for the {Evaluation} of {Digital} {Musical} {Instruments}},
	volume = {35},
	issn = {0148-9267, 1531-5169},
	url = {https://direct.mit.edu/comj/article/35/1/28-42/94317},
	doi = {10.1162/COMJ_a_00038},
	language = {en},
	number = {1},
	urldate = {2023-12-30},
	journal = {Computer Music Journal},
	author = {O'Modhrain, Sile},
	month = mar,
	year = {2011},
	pages = {28--42},
	annote = {Publisher: The MIT Press},
	file = {O'Modhrain - 2011 - A Framework for the Evaluation of Digital Musical .pdf:/Users/eleanorrow/Zotero/storage/56QDPGZP/O'Modhrain - 2011 - A Framework for the Evaluation of Digital Musical .pdf:application/pdf},
}

@inproceedings{ohLeadYouHelp2018,
	address = {Montreal QC Canada},
	title = {I {Lead}, {You} {Help} but {Only} with {Enough} {Details}: {Understanding} {User} {Experience} of {Co}-{Creation} with {Artificial} {Intelligence}},
	volume = {2018-April},
	isbn = {978-1-4503-5620-6},
	shorttitle = {I {Lead}, {You} {Help} but {Only} with {Enough} {Details}},
	url = {https://dl.acm.org/doi/10.1145/3173574.3174223},
	doi = {10.1145/3173574.3174223},
	abstract = {Recent advances in artiﬁcial intelligence (AI) have increased the opportunities for users to interact with the technology. Now, users can even collaborate with AI in creative activities such as art. To understand the user experience in this new user–AI collaboration, we designed a prototype, DuetDraw, an AI interface that allows users and the AI agent to draw pictures collaboratively. We conducted a user study employing both quantitative and qualitative methods. Thirty participants performed a series of drawing tasks with the think-aloud method, followed by post-hoc surveys and interviews. Our ﬁndings are as follows: (1) Users were signiﬁcantly more content with DuetDraw when the tool gave detailed instructions. (2) While users always wanted to lead the task, they also wanted the AI to explain its intentions but only when the users wanted it to do so. (3) Although users rated the AI relatively low in predictability, controllability, and comprehensibility, they enjoyed their interactions with it during the task. Based on these ﬁndings, we discuss implications for user interfaces where users can collaborate with AI in creative works.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Oh, Changhoon and Song, Jungwoo and Choi, Jinhan and Kim, Seonghyeon and Lee, Sungwoo and Suh, Bongwon},
	month = apr,
	year = {2018},
	keywords = {Artificial intelligence, Human computer collaboration, Human-AI interaction},
	pages = {1--13},
	annote = {ISBN: 9781450356206},
	file = {Oh et al. - 2018 - I Lead, You Help but Only with Enough Details Und.pdf:/Users/eleanorrow/Zotero/storage/DKRTT7RT/Oh et al. - 2018 - I Lead, You Help but Only with Enough Details Und.pdf:application/pdf},
}

@misc{olesenEvolutionaryPlanningLatent2020,
	title = {Evolutionary {Planning} in {Latent} {Space}},
	url = {http://arxiv.org/abs/2011.11293},
	abstract = {Planning is a powerful approach to reinforcement learning with several desirable properties. However, it requires a model of the world, which is not readily available in many real-life problems. In this paper, we propose to learn a world model that enables Evolutionary Planning in Latent Space (EPLS). We use a Variational Auto Encoder (VAE) to learn a compressed latent representation of individual observations and extend a Mixture Density Recurrent Neural Network (MDRNN) to learn a stochastic, multi-modal forward model of the world that can be used for planning. We use the Random Mutation Hill Climbing (RMHC) to ﬁnd a sequence of actions that maximize expected reward in this learned model of the world. We demonstrate how to build a model of the world by bootstrapping it with rollouts from a random policy and iteratively reﬁning it with rollouts from an increasingly accurate planning policy using the learned world model. After a few iterations of this reﬁnement, our planning agents are better than standard model-free reinforcement learning approaches demonstrating the viability of our approach1 2.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Olesen, Thor V. A. N. and Nguyen, Dennis T. T. and Palm, Rasmus Berg and Risi, Sebastian},
	month = nov,
	year = {2020},
	note = {arXiv:2011.11293 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Evolutionary Model, Evolutionary Planning, Iterative Training, Latent Space, Model-Based Reinforcement Learning, Planning, World Models},
	annote = {Comment: Code to reproduce the experiments are available at https://github.com/two2tee/WorldModelPlanning Video of driving performance is available at https://youtu.be/3M39QgeF27U},
	annote = {\_eprint: 2011.11293},
	file = {Olesen et al. - 2020 - Evolutionary Planning in Latent Space.pdf:/Users/eleanorrow/Zotero/storage/WR2IQUUY/Olesen et al. - 2020 - Evolutionary Planning in Latent Space.pdf:application/pdf},
}

@misc{pachetComprehensiveOnlineDatabase,
	title = {A {Comprehensive} {Online} {Database} of {Machine}-{Readable} {Leadsheets} for {Jazz} {Standards}},
	abstract = {Jazz standards are songs representative of a body of musical knowledge shared by most professional jazz musicians. As such, the corpus of jazz standards constitutes a unique opportunity to study a musical genre with a “closed-world” approach, since most jazz composers are no longer in activity today. Although many scores for jazz standards can be found on the Internet, no effort, to our knowledge, has been dedicated so far to building a comprehensive database of machine-readable scores for jazz standards. This paper reports on the rationale, design and population of such a database, containing harmonic (chord progressions) as well as melodic and structural information. The database can be used to feed both analysis and generation systems. We report on preliminary results in this vein. We get around the tricky and often unclear copyright issues imposed by the publishing industry, by providing only statistical information about songs. The completeness of such a database should benefit many research experiments in MIR and opens up novel and exciting applications in music generation exploiting symbolic information, notably in style modeling.},
	language = {en},
	author = {Pachet, François and Suzda, Jeff and Martín, Daniel},
	file = {Pachet et al. - A COMPREHENSIVE ONLINE DATABASE OF MACHINE- READAB.pdf:/Users/eleanorrow/Zotero/storage/EK6DU6J2/Pachet et al. - A COMPREHENSIVE ONLINE DATABASE OF MACHINE- READAB.pdf:application/pdf},
}

@incollection{papadopoulosAssistedLeadSheet2016a,
	address = {Cham},
	title = {Assisted {Lead} {Sheet} {Composition} {Using} {FlowComposer}},
	volume = {9892},
	isbn = {978-3-319-44952-4 978-3-319-44953-1},
	url = {http://link.springer.com/10.1007/978-3-319-44953-1_48},
	abstract = {We present FlowComposer, a web application that helps users compose musical lead sheets, i.e. melodies with chord labels. FlowComposer integrates a constrained-based lead sheet generation tool in which the user retains full control over the generation process. Users specify the style of the lead sheet by selecting a corpus of existing lead sheets. The system then produces a complete lead sheet in that style, either from scratch, or from a partial lead sheet entered by the user. The generation algorithm is based on a graphical model that combines two Markov chains enriched by Regular constraints, representing the melody and its related chord sequence. The model is sampled using our recent result in eﬃcient sampling of the Regular constraint. The paper reports on the design and deployment of FlowComposer as a web-service, part of an ecosystem of online tools for the creation of lead sheets. FlowComposer is currently used in professional musical productions, from which we collect and show a number of representative examples.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {Principles and {Practice} of {Constraint} {Programming}},
	publisher = {Springer International Publishing},
	author = {Papadopoulos, Alexandre and Roy, Pierre and Pachet, François},
	editor = {Rueher, Michel},
	year = {2016},
	doi = {10.1007/978-3-319-44953-1_48},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {769--785},
	annote = {ISBN: 978-3-319-44952-4},
	file = {Papadopoulos et al. - 2016 - Assisted Lead Sheet Composition Using FlowComposer.pdf:/Users/eleanorrow/Zotero/storage/9XY5W49J/Papadopoulos et al. - 2016 - Assisted Lead Sheet Composition Using FlowComposer.pdf:application/pdf},
}

@inproceedings{pathakContextEncodersFeature2016a,
	address = {Las Vegas, NV, USA},
	title = {Context {Encoders}: {Feature} {Learning} by {Inpainting}},
	volume = {2016-Decem},
	isbn = {978-1-4673-8851-1},
	shorttitle = {Context {Encoders}},
	url = {http://ieeexplore.ieee.org/document/7780647/},
	doi = {10.1109/CVPR.2016.278},
	abstract = {We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders – a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classiﬁcation, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {Proceedings of the {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A.},
	month = jun,
	year = {2016},
	pages = {2536--2544},
	annote = {ISBN: 9781467388504 \_eprint: 1604.07379},
	file = {Pathak et al. - 2016 - Context Encoders Feature Learning by Inpainting.pdf:/Users/eleanorrow/Zotero/storage/2R6J34MZ/Pathak et al. - 2016 - Context Encoders Feature Learning by Inpainting.pdf:application/pdf},
}

@article{petitmenginDiscoveringStructuresLived2019,
	title = {Discovering the structures of lived experience: {Towards} a micro-phenomenological analysis method},
	volume = {18},
	issn = {1568-7759, 1572-8676},
	shorttitle = {Discovering the structures of lived experience},
	url = {http://link.springer.com/10.1007/s11097-018-9597-4},
	doi = {10.1007/s11097-018-9597-4},
	abstract = {This paper describes a method for analyzing a corpus of descriptions collected through micro-phenomenological interviews. This analysis aims at identifying the structure of the singular experiences which have been described, and in particular their diachronic structure, while unfolding generic experiential structures through an iterative approach. After summarizing the principles of the micro-phenomenological interview, and then describing the process of preparation of the verbatim, the article presents on the one hand, the principles and conceptual devices of the analysis method and on the other hand several dimensions of the analysis process: the modes of structural unfolding of generic structures, the mutual guidance of the processes of structural and experiential unfolding, the tracking of analysis processes, and finally the assessment of analysis results.},
	language = {en},
	number = {4},
	urldate = {2023-12-30},
	journal = {Phenomenology and the Cognitive Sciences},
	author = {Petitmengin, Claire and Remillieux, Anne and Valenzuela-Moguillansky, Camila},
	month = sep,
	year = {2019},
	keywords = {Diachronic structure, Entretien d’explicitation, Micro-phenomenological analysis, Micro-phenomenological interview, Micro-phenomenology, Qualitative analysis, Synchronic structure},
	pages = {691--730},
	file = {Petitmengin et al. - 2019 - Discovering the structures of lived experience To.pdf:/Users/eleanorrow/Zotero/storage/FAN4TGZH/Petitmengin et al. - 2019 - Discovering the structures of lived experience To.pdf:application/pdf},
}

@inproceedings{priceFactorsInfluencingStudents2017,
	address = {Tacoma Washington USA},
	title = {Factors {Influencing} {Students}' {Help}-{Seeking} {Behavior} while {Programming} with {Human} and {Computer} {Tutors}},
	isbn = {978-1-4503-4968-0},
	url = {https://dl.acm.org/doi/10.1145/3105726.3106179},
	doi = {10.1145/3105726.3106179},
	abstract = {When novice students encounter di culty when learning to program, some can seek help from instructors or teaching assistants. is one-on-one tutoring is highly e ective at fostering learning, but busy instructors and large class sizes can make expert help a scarce resource. Increasingly, programming environments a empt to imitate this human support by providing students with hints and feedback. In order to design e ective, computer-based help, it is important to understand how and why students seek and avoid help when programming, and how this process di ers when the help is provided by a human or a computer. We explore these questions through a qualitative analysis of 15 students’ interviews, in which they re ect on solving two programming problems with human and computer help. We discuss implications for help design and present hypotheses on students’ help-seeking behavior.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {Proceedings of the 2017 {ACM} {Conference} on {International} {Computing} {Education} {Research}},
	publisher = {ACM},
	author = {Price, Thomas W. and Liu, Zhongxiu and Cateté, Veronica and Barnes, Tiffany},
	month = aug,
	year = {2017},
	pages = {127--135},
	file = {Price et al. - 2017 - Factors Influencing Students' Help-Seeking Behavio.pdf:/Users/eleanorrow/Zotero/storage/CEIDFHFB/Price et al. - 2017 - Factors Influencing Students' Help-Seeking Behavio.pdf:application/pdf},
}

@article{randlesHowComposersApproach2013,
	title = {How {Composers} {Approach} {Teaching} {Composition}: {Strategies} for {Music} {Teachers}},
	volume = {99},
	issn = {0027-4321, 1945-0087},
	shorttitle = {How {Composers} {Approach} {Teaching} {Composition}},
	url = {http://journals.sagepub.com/doi/10.1177/0027432112471398},
	doi = {10.1177/0027432112471398},
	abstract = {Composition pedagogy is explored from the perspective of a composer and a music teacher educator in this article. The primary goal is to help practicing music teachers develop strategies that will encourage students to create original music. The authors provide reflection about the process of helping students compose on the basis of personal experience compos ing and teaching young composers, via the work of leading scholars in music education and by using narrative excerpts and musical examples. Key strategies are identified that contribute to the successful teaching of composition, particularly at the beginning, middle, and the end of musical compositions. Contributing most notably to this discussion is the use of terminol ogy in teacher feedback.},
	language = {en},
	number = {3},
	urldate = {2023-12-30},
	journal = {Music Educators Journal},
	author = {Randles, Clint and Sullivan, Mark},
	month = mar,
	year = {2013},
	pages = {51--57},
	file = {Randles and Sullivan - 2013 - How Composers Approach Teaching Composition Strat.pdf:/Users/eleanorrow/Zotero/storage/XCK7LZ7S/Randles and Sullivan - 2013 - How Composers Approach Teaching Composition Strat.pdf:application/pdf},
}

@article{rappSearchDesignElements2021a,
	title = {In {Search} for {Design} {Elements}: {A} {New} {Perspective} for {Employing} {Ethnography} in {Human}-{Computer} {Interaction} {Design} {Research}},
	volume = {37},
	issn = {1044-7318, 1532-7590},
	shorttitle = {In {Search} for {Design} {Elements}},
	url = {https://www.tandfonline.com/doi/full/10.1080/10447318.2020.1843296},
	doi = {10.1080/10447318.2020.1843296},
	abstract = {Design ethnography has been widely used in Human-Computer Interaction (HCI) to understand people’s everyday behaviors, in order to build technologies capable of meeting users’ needs. Building on top of the recent debate on ethnography within HCI, this article proposes to employ reflexivity and theoretical pluralism to ground a new way of using design ethnography in HCI, directly envisioning novel designs during the fieldwork. Inspired by design practices like biomimicry, I describe the figure of the ethnodesigner, a digital design ethnographer who dives into ‘successful’ virtual environments in search for insightful design patterns, with the purpose of creating new designs in other, even distant, contexts. In this perspective, the fieldwork becomes an incessant source of inspiration for identifying effective ‘design elements’, understanding how they work and their ‘experiential effects’, and producing design implications to create novel technologies across multiple application domains.},
	language = {en},
	number = {8},
	urldate = {2023-12-30},
	journal = {International Journal of Human–Computer Interaction},
	author = {Rapp, Amon},
	month = may,
	year = {2021},
	pages = {783--802},
	file = {Rapp - 2021 - In Search for Design Elements A New Perspective f.pdf:/Users/eleanorrow/Zotero/storage/YX2ZC7T3/Rapp - 2021 - In Search for Design Elements A New Perspective f.pdf:application/pdf},
}

@misc{raymanExperimentalApproachesComposition2014,
	title = {Experimental {Approaches} to the {Composition} of {Interactive} {Video} {Game} {Music}},
	abstract = {This project explores experimental approaches and strategies to the composition of interactive music for the medium of video games. Whilst music in video games has not enjoyed the technological progress that other aspects of the software have received, budgets expand and incomes from releases grow. Music is now arguably less interactive than it was in the 1990’s, and whilst graphics occupy large amounts of resources and development time, audio does not garner the same attention. This portfolio develops strategies and audio engines, creating music using the techniques of aleatoric composition, real-time remixing of existing work, and generative synthesisers.},
	language = {en},
	author = {Rayman, Joshua},
	year = {2014},
	file = {Rayman - Experimental Approaches to the Composition of Inte.pdf:/Users/eleanorrow/Zotero/storage/N76KFTSG/Rayman - Experimental Approaches to the Composition of Inte.pdf:application/pdf},
}

@inproceedings{reimerEmbracingLessCommon2021a,
	address = {Shanghai, China},
	title = {Embracing {Less} {Common} {Evaluation} {Strategies} for {Studying} {User} {Experience} in {NIME}},
	url = {https://nime.pubpub.org/pub/fidgs435},
	doi = {10.21428/92fbeb44.807a000f},
	abstract = {Assessment of user experience (UX) is increasingly important in music interaction evaluation, as witnessed in previous NIME reviews describing varied and idiosyncratic evaluation strategies. This paper focuses on evaluations conducted in the last four years of NIME (2017 to 2020), compares results to previous research, and classifies evaluation types to describe how researchers approach and study UX in NIME. While results of this review confirm patterns such as the prominence of short-term, performer perspective evaluations, and the variety of evaluation strategies used, they also show that UX-focused evaluations are typically exploratory and limited to novice performers. Overall, these patterns indicate that current UX evaluation strategies do not address dynamic factors such as skill development, the evolution of the performerinstrument relationship, and hedonic and cognitive aspects of UX. To address such limitations, we discuss a number of less common tools developed within and outside of NIME that focus on dynamic aspects of UX, potentially leading to more informative and meaningful evaluation insights.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {{NIME} 2021},
	publisher = {PubPub},
	author = {Reimer, P. J. Charles and Wanderley, Marcelo M.},
	month = jun,
	year = {2021},
	pages = {1--28},
	file = {Reimer and Wanderley - 2021 - Embracing Less Common Evaluation Strategies for St.pdf:/Users/eleanorrow/Zotero/storage/WJU7D3GC/Reimer and Wanderley - 2021 - Embracing Less Common Evaluation Strategies for St.pdf:application/pdf},
}

@article{richardsFilmMusicThemes2016a,
	title = {Film {Music} {Themes}: {Analysis} and {Corpus} {Study}},
	volume = {22},
	issn = {1067-3040},
	shorttitle = {Film {Music} {Themes}},
	url = {http://www.mtosmt.org/issues/mto.16.22.1/mto.16.22.1.richards.html},
	doi = {10.30535/mto.22.1.3},
	abstract = {The phrase structure of film music themes remains virtually unexplored in scholarly literature. This article proposes an analytical system that expands and adapts Caplin 1998 in order to categorize the gamut of film music themes in some detail. This system is then applied to a cross-section of 482 themes from Oscar-nominated scores ranging from the early 1930s to the present day. In doing so, notable divisions appear around 1960 and 1990, times that coincide with trends that drastically affected the composition of film music in general.},
	language = {en},
	number = {1},
	urldate = {2023-12-30},
	journal = {Music Theory Online},
	author = {Richards, Mark},
	month = mar,
	year = {2016},
	file = {Richards - 2016 - Film Music Themes Analysis and Corpus Study.pdf:/Users/eleanorrow/Zotero/storage/4CLVJYTB/Richards - 2016 - Film Music Themes Analysis and Corpus Study.pdf:application/pdf},
}

@inproceedings{saeedContrastiveLearningGeneralPurpose2021a,
	address = {Toronto, ON, Canada},
	title = {Contrastive {Learning} of {General}-{Purpose} {Audio} {Representations}},
	isbn = {978-1-72817-605-5},
	url = {https://ieeexplore.ieee.org/document/9413528/},
	doi = {10.1109/ICASSP39728.2021.9413528},
	abstract = {We introduce COLA, a self-supervised pre-training approach for learning a general-purpose representation of audio. Our approach is based on contrastive learning: it learns a representation which assigns high similarity to audio segments extracted from the same recording while assigning lower similarity to segments from different recordings. We build on top of recent advances in contrastive learning for computer vision and reinforcement learning to design a lightweight, easy-toimplement self-supervised model of audio. We pre-train embeddings on the large-scale Audioset database and transfer these representations to 9 diverse classiﬁcation tasks, including speech, music, animal sounds, and acoustic scenes. We show that despite its simplicity, our method signiﬁcantly outperforms previous self-supervised systems. We furthermore conduct ablation studies to identify key design choices and release a library1 to pre-train and ﬁne-tune COLA models.},
	language = {en},
	urldate = {2023-12-30},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Saeed, Aaqib and Grangier, David and Zeghidour, Neil},
	month = jun,
	year = {2021},
	pages = {3875--3879},
	annote = {\_eprint: 2010.10915},
	file = {Saeed et al. - 2021 - Contrastive Learning of General-Purpose Audio Repr.pdf:/Users/eleanorrow/Zotero/storage/D6XUF9ME/Saeed et al. - 2021 - Contrastive Learning of General-Purpose Audio Repr.pdf:application/pdf},
}

@inproceedings{sakellariouMaximumEntropyModel2015,
	title = {Maximum {Entropy} {Model} for {Melodic} {Patterns}},
	abstract = {We introduce a model for music generation where melodies are seen as a network of interacting notes. Starting from the principle of maximum entropy we assign to this network a probability distribution, which is learned from an existing musical corpus. We use this model to generate novel musical sequences that mimic the style of the corpus. Our main result is that this model can reproduce high-order patterns despite having a polynomial sample complexity. This is in contrast with the more traditionally used Markov models that have an exponential sample complexity.},
	language = {en},
	booktitle = {{ICML} {Workshop} on {Constructive} {Machine} {Learning}},
	author = {Sakellariou, Jason and Tria, Francesca and Loreto, Vittorio and Pachet, François},
	year = {2015},
	file = {Sakellariou et al. - Maximum Entropy Model for Melodic Patterns.pdf:/Users/eleanorrow/Zotero/storage/I32GDDNK/Sakellariou et al. - Maximum Entropy Model for Melodic Patterns.pdf:application/pdf},
}

@misc{sarmentoDadaGPDatasetTokenized2021,
	title = {{DadaGP}: {A} {Dataset} of {Tokenized} {GuitarPro} {Songs} for {Sequence} {Models}},
	shorttitle = {{DadaGP}},
	url = {http://arxiv.org/abs/2107.14653},
	abstract = {Originating in the Renaissance and burgeoning in the digital era, tablatures are a commonly used music notation system which provides explicit representations of instrument ﬁngerings rather than pitches. GuitarPro has established itself as a widely used tablature format and software enabling musicians to edit and share songs for musical practice, learning, and composition. In this work, we present DadaGP, a new symbolic music dataset comprising 26,181 song scores in the GuitarPro format covering 739 musical genres, along with an accompanying tokenized format well-suited for generative sequence models such as the Transformer. The tokenized format is inspired by event-based MIDI encodings, often used in symbolic music generation models. The dataset is released with an encoder/decoder which converts GuitarPro ﬁles to tokens and back. We present results of a use case in which DadaGP is used to train a Transformer-based model to generate new songs in GuitarPro format. We discuss other relevant use cases for the dataset (guitar-bass transcription, music style transfer and artist/genre classiﬁcation) as well as ethical implications. DadaGP opens up the possibility to train GuitarPro score generators, ﬁne-tune models on custom data, create new styles of music, AI-powered songwriting apps, and human-AI improvisation.},
	language = {en},
	urldate = {2023-12-30},
	publisher = {arXiv},
	author = {Sarmento, Pedro and Kumar, Adarsh and Carr, C. J. and Zukowski, Zack and Barthet, Mathieu and Yang, Yi-Hsuan},
	month = jul,
	year = {2021},
	note = {arXiv:2107.14653 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {arXiv:2107.14653 [cs, eess]},
	annote = {Issue: arXiv:2107.14653 arXiv: 2107.14653},
	file = {Sarmento et al. - 2021 - DadaGP A Dataset of Tokenized GuitarPro Songs for.pdf:/Users/eleanorrow/Zotero/storage/UXSUH48B/Sarmento et al. - 2021 - DadaGP A Dataset of Tokenized GuitarPro Songs for.pdf:application/pdf},
}

@article{schiavioEditorialWhatMusical2021a,
	title = {Editorial: {What} {Is} {Musical} {Creativity}? {Interdisciplinary} {Dialogues} and {Approaches}},
	volume = {12},
	issn = {1664-1078},
	shorttitle = {Editorial},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2021.796096/full},
	doi = {10.3389/fpsyg.2021.796096},
	language = {en},
	urldate = {2023-12-30},
	journal = {Frontiers in Psychology},
	author = {Schiavio, Andrea and Bashwiner, David Michael and Jung, Rex Eugene},
	month = nov,
	year = {2021},
	pages = {796096},
	file = {Schiavio et al. - 2021 - Editorial What Is Musical Creativity Interdiscip.pdf:/Users/eleanorrow/Zotero/storage/NKUB9Z6E/Schiavio et al. - 2021 - Editorial What Is Musical Creativity Interdiscip.pdf:application/pdf},
}

@article{scurtoDesigningDeepReinforcement2021,
	title = {Designing {Deep} {Reinforcement} {Learning} for {Human} {Parameter} {Exploration}},
	volume = {28},
	issn = {1073-0516, 1557-7325},
	url = {https://dl.acm.org/doi/10.1145/3414472},
	doi = {10.1145/3414472},
	abstract = {Software tools for generating digital sound often present users with high-dimensional, parametric interfaces, that may not facilitate exploration of diverse sound designs. In this article, we propose to investigate artificial agents using deep reinforcement learning to explore parameter spaces in partnership with users for sound design. We describe a series of user-centred studies to probe the creative benefits of these agents and adapting their design to exploration. Preliminary studies observing users’ exploration strategies with parametric interfaces and testing different agent exploration behaviours led to the design of a fully-functioning prototype, called Co-Explorer, that we evaluated in a workshop with professional sound designers. We found that the Co-Explorer enables a novel creative workflow centred on human–machine partnership, which has been positively received by practitioners. We also highlight varied user exploration behaviours throughout partnering with our system. Finally, we frame design guidelines for enabling such co-exploration workflow in creative digital applications.},
	language = {en},
	number = {1},
	urldate = {2023-12-31},
	journal = {ACM Transactions on Computer-Human Interaction},
	author = {Scurto, Hugo and Kerrebroeck, Bavo Van and Caramiaux, Baptiste and Bevilacqua, Frédéric},
	month = feb,
	year = {2021},
	keywords = {audio/video, DL, Human-AI Partnerships, Interaction design, machine learning, ML, Sound Space},
	pages = {1--35},
	file = {Scurto et al. - 2021 - Designing Deep Reinforcement Learning for Human Pa.pdf:/Users/eleanorrow/Zotero/storage/8L6Z3EKZ/Scurto et al. - 2021 - Designing Deep Reinforcement Learning for Human Pa.pdf:application/pdf},
}

@article{shanAlgorithmicCompositionsBased2010,
	title = {Algorithmic compositions based on discovered musical patterns},
	volume = {46},
	issn = {1380-7501, 1573-7721},
	url = {http://link.springer.com/10.1007/s11042-009-0303-y},
	doi = {10.1007/s11042-009-0303-y},
	abstract = {Computer music composition is the dream of computer music researchers. In this paper, a top-down approach is investigated to discover the rules of musical composition from given music objects and to create a new music object of which style is similar to the given music objects based on the discovered composition rules. The proposed approach utilizes the data mining techniques in order to discover the styled rules of music composition characterized by music structures, melody styles and motifs. A new music object is generated based on the discovered rules. To measure the effectiveness of the proposed approach in computer music composition, a method similar to the Turing test was adopted to test the differences between the machine-generated and human-composed music. Experimental results show that it is hard to distinguish between them. The other experiment showed that the style of generated music is similar to that of the given music objects.},
	language = {en},
	number = {1},
	urldate = {2023-12-31},
	journal = {Multimedia Tools and Applications},
	author = {Shan, Man-Kwan and Chiu, Shih-Chuan},
	month = jan,
	year = {2010},
	pages = {1--23},
	file = {Shan and Chiu - 2010 - Algorithmic compositions based on discovered music.pdf:/Users/eleanorrow/Zotero/storage/WK84FBBH/Shan and Chiu - 2010 - Algorithmic compositions based on discovered music.pdf:application/pdf},
}

@article{shneidermanCreativitySupportTools2006a,
	title = {Creativity {Support} {Tools}: {Report} {From} a {U}.{S}. {National} {Science} {Foundation} {Sponsored} {Workshop}},
	volume = {20},
	issn = {1044-7318, 1532-7590},
	shorttitle = {Creativity {Support} {Tools}},
	url = {http://www.tandfonline.com/doi/abs/10.1207/s15327590ijhc2002_1},
	doi = {10.1207/s15327590ijhc2002_1},
	language = {en},
	number = {2},
	urldate = {2023-12-31},
	journal = {International Journal of Human-Computer Interaction},
	author = {Shneiderman, Ben and Fischer, Gerhard and Czerwinski, Mary and Resnick, Mitch and Myers, Brad and Candy, Linda and Edmonds, Ernest and Eisenberg, Mike and Giaccardi, Elisa and Hewett, Tom and Jennings, Pamela and Kules, Bill and Nakakoji, Kumiyo and Nunamaker, Jay and Pausch, Randy and Selker, Ted and Sylvan, Elisabeth and Terry, Michael},
	month = may,
	year = {2006},
	pages = {61--77},
	file = {Shneiderman et al. - 2006 - Creativity Support Tools Report From a U.S. Natio.pdf:/Users/eleanorrow/Zotero/storage/DL93V6V3/Shneiderman et al. - 2006 - Creativity Support Tools Report From a U.S. Natio.pdf:application/pdf},
}

@inproceedings{smithHumanAIPartnershipsGenerative2022,
	address = {The University of Auckland, New Zealand},
	title = {Human-{AI} {Partnerships} in {Generative} {Music}},
	url = {https://nime.pubpub.org/pub/2644jox5},
	doi = {10.21428/92fbeb44.1aaaf7ab},
	language = {en},
	urldate = {2023-12-31},
	booktitle = {{NIME} 2022},
	publisher = {PubPub},
	author = {Smith, Jason},
	month = jun,
	year = {2022},
	keywords = {Human-AI Partnerships},
	file = {Smith - 2022 - Human-AI Partnerships in Generative Music.pdf:/Users/eleanorrow/Zotero/storage/C6LXM688/Smith - 2022 - Human-AI Partnerships in Generative Music.pdf:application/pdf},
}

@article{smithDiscoveringThemesExacta,
	title = {Discovering {Themes} by {Exact} {Pattern} {Matching}},
	language = {en},
	author = {Smith, Lloyd and Medina, Richard},
	file = {Smith and Medina - Discovering Themes by Exact Pattern Matching.pdf:/Users/eleanorrow/Zotero/storage/R5LS3LIU/Smith and Medina - Discovering Themes by Exact Pattern Matching.pdf:application/pdf},
}

@misc{songHowTrainYour2021,
	title = {How to {Train} {Your} {Energy}-{Based} {Models}},
	url = {http://arxiv.org/abs/2101.03288},
	abstract = {Energy-Based Models (EBMs), also known as non-normalized probabilistic models, specify probability density or mass functions up to an unknown normalizing constant. Unlike most other probabilistic models, EBMs do not place a restriction on the tractability of the normalizing constant, thus are more ﬂexible to parameterize and can model a more expressive family of probability distributions. However, the unknown normalizing constant of EBMs makes training particularly diﬃcult. Our goal is to provide a friendly introduction to modern approaches for EBM training. We start by explaining maximum likelihood training with Markov chain Monte Carlo (MCMC), and proceed to elaborate on MCMC-free approaches, including Score Matching (SM) and Noise Constrastive Estimation (NCE). We highlight theoretical connections among these three approaches, and end with a brief survey on alternative training methods, which are still under active research. Our tutorial is targeted at an audience with basic understanding of generative models who want to apply EBMs or start a research project in this direction.},
	language = {en},
	urldate = {2023-12-31},
	publisher = {arXiv},
	author = {Song, Yang and Kingma, Diederik P.},
	month = feb,
	year = {2021},
	note = {arXiv:2101.03288 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Contrastive Predictive Coding, Statistics - Machine Learning},
	annote = {\_eprint: 2101.03288},
	file = {Song and Kingma - 2021 - How to Train Your Energy-Based Models.pdf:/Users/eleanorrow/Zotero/storage/749GBQVX/Song and Kingma - 2021 - How to Train Your Energy-Based Models.pdf:application/pdf},
}

@misc{spijkervetContrastiveLearningMusical2021,
	title = {Contrastive {Learning} of {Musical} {Representations}},
	url = {http://arxiv.org/abs/2103.09410},
	abstract = {While deep learning has enabled great advances in many areas of music, labeled music datasets remain especially hard, expensive, and time-consuming to create. In this work, we introduce SimCLR to the music domain and contribute a large chain of audio data augmentations to form a simple framework for self-supervised, contrastive learning of musical representations: CLMR. This approach works on raw time-domain music data and requires no labels to learn useful representations. We evaluate CLMR in the downstream task of music classification on the MagnaTagATune and Million Song datasets and present an ablation study to test which of our music-related innovations over SimCLR are most effective. A linear classifier trained on the proposed representations achieves a higher average precision than supervised models on the MagnaTagATune dataset, and performs comparably on the Million Song dataset. Moreover, we show that CLMR's representations are transferable using out-of-domain datasets, indicating that our method has strong generalisability in music classification. Lastly, we show that the proposed method allows data-efficient learning on smaller labeled datasets: we achieve an average precision of 33.1\% despite using only 259 labeled songs in the MagnaTagATune dataset (1\% of the full dataset) during linear evaluation. To foster reproducibility and future research on self-supervised learning in music, we publicly release the pre-trained models and the source code of all experiments of this paper.},
	language = {en},
	urldate = {2023-12-31},
	publisher = {arXiv},
	author = {Spijkervet, Janne and Burgoyne, John Ashley},
	month = sep,
	year = {2021},
	note = {arXiv:2103.09410 [cs, eess]},
	keywords = {Audio, Classification, CNN, Computer Science - Machine Learning, Computer Science - Sound, Contrastive, Contrastive Predictive Coding, Electrical Engineering and Systems Science - Audio and Speech Processing, Latent Space, Music, Related to Contrastive Predictive Coding},
	annote = {Comment: 15 pages, 8 figures. In Proceedings of the 22nd International Society for Music Information Retrieval Conference, ISMIR, 2021},
	annote = {\_eprint: 2103.09410},
	annote = {- Self supervised learning (SSL) - Data augmentation based contrastive method - Raw audio waveforms - CNN encoder},
	file = {Spijkervet and Burgoyne - 2021 - Contrastive Learning of Musical Representations.pdf:/Users/eleanorrow/Zotero/storage/SZJVD3HY/Spijkervet and Burgoyne - 2021 - Contrastive Learning of Musical Representations.pdf:application/pdf},
}

@article{sturmMachineLearningResearch2019a,
	title = {Machine learning research that matters for music creation: {A} case study},
	volume = {48},
	issn = {0929-8215, 1744-5027},
	shorttitle = {Machine learning research that matters for music creation},
	url = {https://www.tandfonline.com/doi/full/10.1080/09298215.2018.1515233},
	doi = {10.1080/09298215.2018.1515233},
	language = {en},
	number = {1},
	urldate = {2023-12-31},
	journal = {Journal of New Music Research},
	author = {Sturm, Bob L. and Ben-Tal, Oded and Monaghan, Úna and Collins, Nick and Herremans, Dorien and Chew, Elaine and Hadjeres, Gaëtan and Deruty, Emmanuel and Pachet, François},
	month = jan,
	year = {2019},
	pages = {36--55},
	file = {Sturm et al. - 2019 - Machine learning research that matters for music c.pdf:/Users/eleanorrow/Zotero/storage/QSJ75RNU/Sturm et al. - 2019 - Machine learning research that matters for music c.pdf:application/pdf},
}

@inproceedings{suhAISocialGlue2021,
	address = {Yokohama Japan},
	title = {{AI} as {Social} {Glue}: {Uncovering} the {Roles} of {Deep} {Generative} {AI} during {Social} {Music} {Composition}},
	isbn = {978-1-4503-8096-6},
	shorttitle = {{AI} as {Social} {Glue}},
	url = {https://dl.acm.org/doi/10.1145/3411764.3445219},
	doi = {10.1145/3411764.3445219},
	abstract = {Recent advances in deep generative neural networks have made it possible for artificial intelligence to actively collaborate with human beings in co-creating novel content (e.g. music, art). While substantial research focuses on (individual) human-AI collaborations, comparatively less research examines how AI can play a role in human-human collaborations during co-creation. In a qualitative lab study, we observed 30 participants (15 pairs) compose a musical phrase in pairs, both with and without AI. Our findings reveal that AI may play important roles in influencing human social dynamics during creativity, including: 1) implicitly seeding a common ground at the start of collaboration, 2) acting as a psychological safety net in creative risk-taking, 3) providing a force for group progress, 4) mitigating interpersonal stalling and friction, and 5) altering users’ collaborative and creative roles. This work contributes to the future of generative AI in social creativity by providing implications for how AI could enrich, impede, or alter creative social dynamics in the years to come.},
	language = {en},
	urldate = {2023-12-31},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Suh, Minhyang (Mia) and Youngblom, Emily and Terry, Michael and Cai, Carrie J},
	month = may,
	year = {2021},
	keywords = {human-AI co-creation, Human-AI Partnerships, machine learning, music composition},
	pages = {1--11},
	file = {Suh et al. - 2021 - AI as Social Glue Uncovering the Roles of Deep Ge.pdf:/Users/eleanorrow/Zotero/storage/U64V3CQN/Suh et al. - 2021 - AI as Social Glue Uncovering the Roles of Deep Ge.pdf:application/pdf},
}

@misc{sunCanonicalCapsulesSelfSupervised2021,
	title = {Canonical {Capsules}: {Self}-{Supervised} {Capsules} in {Canonical} {Pose}},
	shorttitle = {Canonical {Capsules}},
	url = {http://arxiv.org/abs/2012.04718},
	abstract = {We propose an unsupervised capsule architecture for 3D point clouds. We compute capsule decompositions of objects through permutation-equivariant attention, and selfsupervise the process by training with pairs of randomly rotated objects. Our key idea is to aggregate the attention masks into semantic keypoints, and use these to supervise a decomposition that satisﬁes the capsule invariance/equivariance properties. This not only enables the training of a semantically consistent decomposition, but also allows us to learn a canonicalization operation that enables object-centric reasoning. In doing so, we require neither classiﬁcation labels nor manually-aligned training datasets to train. Yet, by learning an object-centric representation in an unsupervised manner, our method outperforms the stateof-the-art on 3D point cloud reconstruction, registration, and unsupervised classiﬁcation. We will release the code and dataset to reproduce our results as soon as the paper is published.},
	language = {en},
	urldate = {2023-12-31},
	publisher = {arXiv},
	author = {Sun, Weiwei and Tagliasacchi, Andrea and Deng, Boyang and Sabour, Sara and Yazdani, Soroosh and Hinton, Geoffrey and Yi, Kwang Moo},
	month = nov,
	year = {2021},
	note = {arXiv:2012.04718 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: NeurIPS 2021; The first two authors contributed equally; Project website: https://canonical-capsules.github.io},
	annote = {\_eprint: 2012.04718},
	file = {Sun et al. - 2021 - Canonical Capsules Self-Supervised Capsules in Ca.pdf:/Users/eleanorrow/Zotero/storage/BVNZ44IX/Sun et al. - 2021 - Canonical Capsules Self-Supervised Capsules in Ca.pdf:application/pdf},
}

@misc{rouardCRASHRawAudio2021,
	title = {{CRASH}: {Raw} {Audio} {Score}-based {Generative} {Modeling} for {Controllable} {High}-resolution {Drum} {Sound} {Synthesis}},
	shorttitle = {{CRASH}},
	url = {http://arxiv.org/abs/2106.07431},
	abstract = {In this paper, we propose a novel score-base generative model for unconditional raw audio synthesis. Our proposal builds upon the latest developments on diffusion process modeling with stochastic differential equations, which already demonstrated promising results on image generation. We motivate novel heuristics for the choice of the diffusion processes better suited for audio generation, and consider the use of a conditional U-Net to approximate the score function. While previous approaches on diffusion models on audio were mainly designed as speech vocoders in medium resolution, our method termed CRASH (Controllable Raw Audio Synthesis with High-resolution) allows us to generate short percussive sounds in 44.1kHz in a controllable way. Through extensive experiments, we showcase on a drum sound generation task the numerous sampling schemes offered by our method (unconditional generation, deterministic generation, inpainting, interpolation, variations, class-conditional sampling) and propose the class-mixing sampling, a novel way to generate “hybrid” sounds. Our proposed method closes the gap with GAN-based methods on raw audio, while offering more ﬂexible generation capabilities with lighter and easier-to-train models.},
	language = {en},
	urldate = {2023-12-31},
	publisher = {arXiv},
	author = {Rouard, Simon and Hadjeres, Gaëtan},
	month = jun,
	year = {2021},
	note = {arXiv:2106.07431 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: 12 pages, 11 figures},
	annote = {\_eprint: arXiv:2106.07431v1},
	file = {Rouard and Hadjeres - 2021 - CRASH Raw Audio Score-based Generative Modeling f.pdf:/Users/eleanorrow/Zotero/storage/77MDXLT5/Rouard and Hadjeres - 2021 - CRASH Raw Audio Score-based Generative Modeling f.pdf:application/pdf},
}

@inproceedings{tanTutorialAIMusic2021,
	address = {Virtual Event China},
	title = {A {Tutorial} on {AI} {Music} {Composition}},
	isbn = {978-1-4503-8651-7},
	url = {https://dl.acm.org/doi/10.1145/3474085.3478875},
	doi = {10.1145/3474085.3478875},
	abstract = {AI music composition is one of the most attractive and important topics in artificial intelligence, music, and multimedia. The typical tasks in AI music composition include melody generation, song writing, accompaniment generation, arrangement, performance generation, timbre rendering, sound generation, and singing voice synthesis, which cover different modalities (e.g., symbolic music score, sound) and well match to the theme of ACM Multimedia. As the rapid development of artificial intelligence techniques such as content creation and deep learning, AI based music composition has achieved rapid progress, but still encountered a lot of challenges. A thorough introduction and review on the basics, the research progress, as well as how to address the challenges in AI music composition are timely and necessary for a broad audience working on artificial intelligence, music, and multimedia. In this tutorial, we will first introduce the background of AI music composition, including music basics and deep learning techniques for music composition. Then we will introduce AI music composition from two perspectives: 1) key components, which include music score generation, music performance generation, and music sound generation; 2) advanced topics, which include music structure/form/style/emotion modeling, timbre synthesis/transfer/mixing, etc. At last, we will point out some research challenges and future directions in AI music composition. This tutorial can serve both academic researchers and industry practitioners working on AI music composition.},
	language = {en},
	urldate = {2023-12-31},
	booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Tan, Xu and Li, Xiaobing},
	month = oct,
	year = {2021},
	keywords = {accompaniment generation, ai music, melody generation, music arrangement, music composition, singing voice synthesis, song writing},
	pages = {5678--5680},
	annote = {Publication Title: MM 2021 - Proceedings of the 29th ACM International Conference on Multimedia},
	file = {Tan and Li - 2021 - A Tutorial on AI Music Composition.pdf:/Users/eleanorrow/Zotero/storage/M72ZBSAN/Tan and Li - 2021 - A Tutorial on AI Music Composition.pdf:application/pdf},
}

@incollection{tanakaInteractionExperienceFuture2006,
	address = {Berlin/Heidelberg},
	title = {Interaction, {Experience} and the {Future} of {Music}},
	volume = {35},
	isbn = {978-1-4020-4031-3},
	url = {http://link.springer.com/10.1007/1-4020-4097-0_13},
	language = {en},
	urldate = {2023-12-31},
	booktitle = {Consuming {Music} {Together}},
	publisher = {Springer-Verlag},
	author = {Tanaka, Atau},
	editor = {O’Hara, Kenton and Brown, Barry},
	year = {2006},
	doi = {10.1007/1-4020-4097-0_13},
	note = {Series Title: Computer Supported Cooperative Work},
	keywords = {Participatory Design},
	pages = {267--288},
	file = {Tanaka - 2006 - Interaction, Experience and the Future of Music.pdf:/Users/eleanorrow/Zotero/storage/QJMDUVXT/Tanaka - 2006 - Interaction, Experience and the Future of Music.pdf:application/pdf},
}

@article{wangIndustrialStrengthAudioSearch,
	title = {An {Industrial}-{Strength} {Audio} {Search} {Algorithm}},
	language = {en},
	author = {Wang, Avery Li-Chun},
	file = {Wang - An Industrial-Strength Audio Search Algorithm.pdf:/Users/eleanorrow/Zotero/storage/N7JIUDHF/Wang - An Industrial-Strength Audio Search Algorithm.pdf:application/pdf},
}

@article{tatarMusicalAgentsTypology2019,
	title = {Musical agents: {A} typology and state of the art towards {Musical} {Metacreation}},
	volume = {48},
	issn = {0929-8215, 1744-5027},
	shorttitle = {Musical agents},
	url = {https://www.tandfonline.com/doi/full/10.1080/09298215.2018.1511736},
	doi = {10.1080/09298215.2018.1511736},
	abstract = {Musical agents are artificial agents that tackle musical creative tasks, partially or completely. This review of musical agents combines the terminology of Generative Arts (artistic practice) and the scientific literature of Computational Creativity, Multi-Agent Systems (MAS), and Artificial Intelligence. We define Musical Metacreation as a field that studies the partial or complete automation of musical tasks. We survey seventy-eight musical agent systems, and present a typology of musical agents. After examining the evaluation methodologies of musical agents, we propose possible future steps while mentioning ongoing discussions in the field.},
	language = {en},
	number = {1},
	urldate = {2023-12-31},
	journal = {Journal of New Music Research},
	author = {Tatar, Kıvanç and Pasquier, Philippe},
	month = jan,
	year = {2019},
	keywords = {Artificial Intelligence, Computational Creativity, Correction, creativity support, Multi-Agent Systems, Musical agents, Musical Metacreation, tools},
	pages = {56--105},
	annote = {Publisher: Routledge},
	file = {Tatar and Pasquier - 2019 - Musical agents A typology and state of the art to.pdf:/Users/eleanorrow/Zotero/storage/YVEQN5H4/Tatar and Pasquier - 2019 - Musical agents A typology and state of the art to.pdf:application/pdf},
}

@misc{thickstunAnticipatoryMusicTransformer2023,
	title = {Anticipatory {Music} {Transformer}},
	url = {http://arxiv.org/abs/2306.08620},
	abstract = {We introduce anticipation: a method for constructing a controllable generative model of a temporal point process (the event process) conditioned asynchronously on realizations of a second, correlated process (the control process). We achieve this by interleaving sequences of events and controls, such that controls appear following stopping times in the event sequence. This work is motivated by problems arising in the control of symbolic music generation. We focus on infilling control tasks, whereby the controls are a subset of the events themselves, and conditional generation completes a sequence of events given the fixed control events. We train anticipatory infilling models using the large and diverse Lakh MIDI music dataset. These models match the performance of autoregressive models for prompted music generation, with the additional capability to perform infilling control tasks, including accompaniment. Human evaluators report that an anticipatory model produces accompaniments with similar musicality to even music composed by humans over a 20-second clip.},
	language = {en},
	urldate = {2023-12-31},
	publisher = {arXiv},
	author = {Thickstun, John and Hall, David and Donahue, Chris and Liang, Percy},
	month = jun,
	year = {2023},
	note = {arXiv:2306.08620 [cs, eess, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	annote = {Comment: 33 pages, 6 figures},
	file = {Thickstun et al. - 2023 - Anticipatory Music Transformer.pdf:/Users/eleanorrow/Zotero/storage/NMWHIZ6C/Thickstun et al. - 2023 - Anticipatory Music Transformer.pdf:application/pdf},
}

@article{toivonenDataMiningMachine2015,
	title = {Data mining and machine learning in computational creativity},
	volume = {5},
	issn = {1942-4787, 1942-4795},
	url = {https://wires.onlinelibrary.wiley.com/doi/10.1002/widm.1170},
	doi = {10.1002/widm.1170},
	abstract = {Creative machines are an old idea, but only recently has computational creativity established itself as a research ﬁeld with its own identity and research agenda. The goal of computational creativity research is to model, simulate or enhance creativity using computational methods. Data mining and machine learning can be used in a number of ways to help computers learn how to be creative, such as learning to generate new artefacts or to evaluate various qualities of newly generated arfefacts. In this review paper we give an overview of research in computational creativity with a focus on the roles that data mining and machine learning have had and could have in creative systems.},
	language = {en},
	number = {6},
	urldate = {2023-12-31},
	journal = {WIREs Data Mining and Knowledge Discovery},
	author = {Toivonen, Hannu and Gross, Oskar},
	month = nov,
	year = {2015},
	keywords = {On paper},
	pages = {265--275},
	annote = {ISBN: 1942-4787},
	file = {Toivonen and Gross - 2015 - Data mining and machine learning in computational .pdf:/Users/eleanorrow/Zotero/storage/BU67AZXR/Toivonen and Gross - 2015 - Data mining and machine learning in computational .pdf:application/pdf},
}

@article{tomasHowArtsCan2017,
	title = {How {The} {Arts} {Can} {Help} {Tangible} {Interaction} {Design}: {A} {Critical} {Re}-{Orientation}},
	volume = {4},
	issn = {2227-9709},
	shorttitle = {How {The} {Arts} {Can} {Help} {Tangible} {Interaction} {Design}},
	url = {http://www.mdpi.com/2227-9709/4/3/31},
	doi = {10.3390/informatics4030031},
	abstract = {There is a long history of creative encounters between tangible interface design and the Arts. However, in comparison with media art, tangible interaction seems to be quite anchored into many of the traditional methodologies imported from human–computer interaction (HCI). How can the Arts help tangible interaction design? Building on Søren Pold’s Interface Aesthetics, a re-orientation of the role of the artist towards a critical examination of our research medium—tangible interaction—is proposed. In this essay, the beneﬁts of incorporating artistic research and its methodologies into our ﬁeld are described. With these methodologies it is possible to better assess experiential aspects of interaction—a relevant attribute which traditional HCI approaches cannot afford. In order to inform our community, three examples of critical artworks are comparatively studied and discussed.},
	language = {en},
	number = {3},
	urldate = {2023-12-31},
	journal = {Informatics},
	author = {Tomás, Enrique},
	month = sep,
	year = {2017},
	pages = {31},
	file = {Tomás - 2017 - How The Arts Can Help Tangible Interaction Design.pdf:/Users/eleanorrow/Zotero/storage/VZMCEJ6D/Tomás - 2017 - How The Arts Can Help Tangible Interaction Design.pdf:application/pdf},
}

@article{tomczakDrumTranslationTimbral2019,
	title = {Drum translation for timbral and rhythmic transformation},
	abstract = {Many recent approaches to creative transformations of musical audio have been motivated by the success of raw audio generation models such as WaveNet, in which audio samples are modeled by generative neural networks. This paper describes a generative audio synthesis model for multi-drum translation based on a WaveNet denosing autoencoder architecture. The timbre of an arbitrary source audio input is transformed to sound as if it were played by various percussive instruments while preserving its rhythmic structure. Two evaluations of the transformations are conducted based on the capacity of the model to preserve the rhythmic patterns of the input and the audio quality as it relates to timbre of the target drum domain. The ﬁrst evaluation measures the rhythmic similarities between the source audio and the corresponding drum translations, and the second provides a numerical analysis of the quality of the synthesised audio. Additionally, a semi- and fully-automatic audio effect has been proposed, in which the user may assist the system by manually labelling source audio segments or use a state-of-the-art automatic drum transcription system prior to drum translation.},
	language = {en},
	author = {Tomczak, Maciek and Drysdale, Jake and Hockman, Jason and Two, Author and Three, Author and Four, Author},
	year = {2019},
	file = {Tomczak et al. - 2019 - Drum translation for timbral and rhythmic transfor.pdf:/Users/eleanorrow/Zotero/storage/N2SDM5RY/Tomczak et al. - 2019 - Drum translation for timbral and rhythmic transfor.pdf:application/pdf},
}

@article{trainorDevelopingCraftReflexive2021,
	title = {Developing the craft: reflexive accounts of doing reflexive thematic analysis},
	volume = {13},
	issn = {2159-676X, 2159-6778},
	shorttitle = {Developing the craft},
	url = {https://www.tandfonline.com/doi/full/10.1080/2159676X.2020.1840423},
	doi = {10.1080/2159676X.2020.1840423},
	abstract = {Thematic analysis (TA) is unique in that it does not come with a prede­ termined theoretical framework, leaving the researcher accountable to articulate methodological decisions made. As a community of qualitative scholars, we need to clearly articulate and define the theoretical founda­ tions, assumptions, and parameters that guide our work and analysis. We also need to be transparent about our reflections during data analysis, sharing our tensions, struggles, and realizations. While the flexibility of TA can lead to poorly constructed and executed analysis, it also offers the ability to develop rich, detailed, and nuanced analysis. TA is not your ’simple go lucky‘ approach, rather the complexities, interaction, and crea­ tivity that reflexive TA offers is remarkable. While TA is one of the most commonly used methods to analyze qualitative data, there is considerable variability in how the method is understood and conducted. As a growing qualitative researcher, [Author A] was frustrated by the limited examples of the reflexive process of doing TA, and the lack of transparency of how the data analysis was carried out. She grappled with figuring outhowto conduct a high-quality TA. As an experienced qualitative researcher and a mentor to graduate students, [Author B] struggled to find ways to support and guide [Author A] to develop her craft. The experience brought her to reflect on her own use of TA and how her practice has evolved. In this manuscript, we use visual and written examples to show the active decisions made during analysis, struggles and rebounds, and how these aided us in understanding the process of reflexive TA.},
	language = {en},
	number = {5},
	urldate = {2023-12-31},
	journal = {Qualitative Research in Sport, Exercise and Health},
	author = {Trainor, Lisa R. and Bundon, Andrea},
	month = sep,
	year = {2021},
	keywords = {big Q qualitative, coding, epistemology, Reflexivity, theme},
	pages = {705--726},
	annote = {Publisher: Routledge \_eprint: https://doi.org/10.1080/2159676X.2020.1840423},
	file = {Trainor and Bundon - 2021 - Developing the craft reflexive accounts of doing .pdf:/Users/eleanorrow/Zotero/storage/F4ZJVAZN/Trainor and Bundon - 2021 - Developing the craft reflexive accounts of doing .pdf:application/pdf},
}

@misc{tsaiFindingFastTransformers2020,
	title = {Finding {Fast} {Transformers}: {One}-{Shot} {Neural} {Architecture} {Search} by {Component} {Composition}},
	shorttitle = {Finding {Fast} {Transformers}},
	url = {http://arxiv.org/abs/2008.06808},
	abstract = {Transformer-based models have achieved stateof-the-art results in many tasks in natural language processing. However, such models are usually slow at inference time, making deployment difﬁcult. In this paper, we develop an efﬁcient algorithm to search for fast models while maintaining model quality. We describe a novel approach to decompose the Transformer architecture into smaller components, and propose a sampling-based one-shot architecture search method to ﬁnd an optimal model for inference. The model search process is more efﬁcient than alternatives, adding only a small overhead to training time. By applying our methods to BERT-base architectures, we achieve 10\% to 30\% speedup for pre-trained BERT and 70\% speedup on top of a previous state-of-the-art distilled BERT model on Cloud TPU-v2 with a generally acceptable drop in performance.},
	language = {en},
	urldate = {2023-12-31},
	publisher = {arXiv},
	author = {Tsai, Henry and Ooi, Jayden and Ferng, Chun-Sung and Chung, Hyung Won and Riesa, Jason},
	month = aug,
	year = {2020},
	note = {arXiv:2008.06808 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {\_eprint: 2008.06808},
	file = {Tsai et al. - 2020 - Finding Fast Transformers One-Shot Neural Archite.pdf:/Users/eleanorrow/Zotero/storage/FMPT32W8/Tsai et al. - 2020 - Finding Fast Transformers One-Shot Neural Archite.pdf:application/pdf},
}

@inproceedings{uitdenbogerdMelodicMatchingTechniques1999a,
	address = {Orlando Florida USA},
	title = {Melodic matching techniques for large music databases},
	isbn = {978-1-58113-151-2},
	url = {https://dl.acm.org/doi/10.1145/319463.319470},
	doi = {10.1145/319463.319470},
	abstract = {With the growth in digital representations of music, and of music stored in these representations, it is increasingly attractive to search collections of music. One mode of search is by similarity, but, for music, similarity search presents several difficulties: in particular, for melodic query support, deciding what part of the music is likely to be perceived as the theme by a listener, and deciding whether two pieces of music with different sequences of notes represent the same theme. In this paper we propose a three-stage framework for matching pieces of music. We use the framework to compare a range of techniques for determining whether two pieces of music are similar, by experimentally testing their ability to retrieve different transcriptions of the same piece of music from a large collection of MIDI files. These experiments show that different comparison techniques differ widely in their effectiveness; and that, by instantiating the framework with appropriate music manipulation and comparison techniques, pieces of music that match a query can be identified in a large collection.},
	language = {en},
	urldate = {2023-12-31},
	booktitle = {Proceedings of the seventh {ACM} international conference on {Multimedia} ({Part} 1)},
	publisher = {ACM},
	author = {Uitdenbogerd, Alexandra and Zobel, Justin},
	month = oct,
	year = {1999},
	pages = {57--66},
	file = {Uitdenbogerd and Zobel - 1999 - Melodic matching techniques for large music databa.pdf:/Users/eleanorrow/Zotero/storage/8386E5JQ/Uitdenbogerd and Zobel - 1999 - Melodic matching techniques for large music databa.pdf:application/pdf},
}

@inproceedings{urbanoMIREX2012Symbolica,
	title = {{MIREX} 2012 {Symbolic} {Melodic} {Similarity}: {Hybrid} {Sequence} {Alignment} with {Geometric} {Representations}},
	abstract = {This short paper describes our ﬁve submissions to the 2012 edition of the MIREX Symbolic Melodic Similarity task. All ﬁve submissions rely on a geometric model that represents melodies as spline curves in the pitch-time plane. The similarity between two melodies is then computed with a sequence alignment algorithm between sequences of spline spans: the more similar the shape of the curves, the more similar the melodies they represent.},
	language = {en},
	author = {Urbano, Julián},
	pages = {4},
	file = {Urbano - MIREX 2012 Symbolic Melodic Similarity Hybrid Seq.pdf:/Users/eleanorrow/Zotero/storage/8G6ANULL/Urbano - MIREX 2012 Symbolic Melodic Similarity Hybrid Seq.pdf:application/pdf},
}

@misc{oordWaveNetGenerativeModel2016,
	title = {{WaveNet}: {A} {Generative} {Model} for {Raw} {Audio}},
	shorttitle = {{WaveNet}},
	url = {http://arxiv.org/abs/1609.03499},
	abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efﬁciently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-ofthe-art performance, with human listeners rating it as signiﬁcantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal ﬁdelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we ﬁnd that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	language = {en},
	urldate = {2023-12-31},
	publisher = {arXiv},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	month = sep,
	year = {2016},
	note = {arXiv:1609.03499 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, On paper},
	annote = {Pages: arXiv:1609.03499 Publication Title: arXiv e-prints},
	file = {Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:/Users/eleanorrow/Zotero/storage/GMP4HNHS/Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:application/pdf},
}

@misc{oordRepresentationLearningContrastive2019,
	title = {Representation {Learning} with {Contrastive} {Predictive} {Coding}},
	url = {http://arxiv.org/abs/1807.03748},
	abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artiﬁcial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
	language = {en},
	urldate = {2023-12-31},
	publisher = {arXiv},
	author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	month = jan,
	year = {2019},
	note = {arXiv:1807.03748 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Contrastive Predictive Coding, Statistics - Machine Learning},
	file = {Oord et al. - 2019 - Representation Learning with Contrastive Predictiv.pdf:/Users/eleanorrow/Zotero/storage/8IXXPY9X/Oord et al. - 2019 - Representation Learning with Contrastive Predictiv.pdf:application/pdf},
}

@misc{vaswaniAttentionAllYou2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	language = {en},
	urldate = {2023-12-31},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 15 pages, 5 figures},
	annote = {Comment: 15 pages, 5 figures},
	file = {Vaswani et al. - 2023 - Attention Is All You Need.pdf:/Users/eleanorrow/Zotero/storage/ZQTSRAB2/Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf},
}

@misc{wanOldPhotoRestoration2020,
	title = {Old {Photo} {Restoration} via {Deep} {Latent} {Space} {Translation}},
	url = {http://arxiv.org/abs/2009.07047},
	abstract = {We propose to restore old photos that suffer from severe degradation through a deep learning approach. Unlike conventional restoration tasks that can be solved through supervised learning, the degradation in real photos is complex and the domain gap between synthetic images and real old photos makes the network fail to generalize. Therefore, we propose a novel triplet domain translation network by leveraging real photos along with massive synthetic image pairs. Speciﬁcally, we train two variational autoencoders (VAEs) to respectively transform old photos and clean photos into two latent spaces. And the translation between these two latent spaces is learned with synthetic paired data. This translation generalizes well to real photos because the domain gap is closed in the compact latent space. Besides, to address multiple degradations mixed in one old photo, we design a global branch with a partial nonlocal block targeting to the structured defects, such as scratches and dust spots, and a local branch targeting to the unstructured defects, such as noises and blurriness. Two branches are fused in the latent space, leading to improved capability to restore old photos from multiple defects. Furthermore, we apply another face reﬁnement network to recover ﬁne details of faces in the old photos, thus ultimately generating photos with enhanced perceptual quality. With comprehensive experiments, the proposed pipeline demonstrates superior performance over state-of-the-art methods as well as existing commercial tools in terms of visual quality for old photos restoration.},
	language = {en},
	urldate = {2023-12-31},
	publisher = {arXiv},
	author = {Wan, Ziyu and Zhang, Bo and Chen, Dongdong and Zhang, Pan and Chen, Dong and Liao, Jing and Wen, Fang},
	month = sep,
	year = {2020},
	note = {arXiv:2009.07047 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Image, Image Generation, Image Restoration, Latent Space Translation, Mixed degradation, Models, VAE},
	annote = {Comment: 15 pages. arXiv admin note: substantial text overlap with arXiv:2004.09484},
	annote = {Comment: 15 pages. arXiv admin note: substantial text overlap with arXiv:2004.09484},
	file = {Wan et al. - 2020 - Old Photo Restoration via Deep Latent Space Transl.pdf:/Users/eleanorrow/Zotero/storage/SZWC9S4A/Wan et al. - 2020 - Old Photo Restoration via Deep Latent Space Transl.pdf:application/pdf},
}

@article{wangLiteratureReviewIndividual2017,
	title = {A literature review on individual creativity support systems},
	volume = {74},
	issn = {07475632},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0747563217302777},
	doi = {10.1016/j.chb.2017.04.035},
	abstract = {Individual creativity support systems have been developed to facilitate creative work. This article reviews the various design requirements and approaches proposed for supporting individual creative work, as well as relevant creativity theories. Current creativity support systems use many approaches in supporting the collection of relevant information and the creation of ideas or artifacts. However, the designs are typically based on just a few creativity theories. Based on various creativity theories, we propose a new integrated framework for individual creativity support systems. This framework enumerates aspects, components, and features of creativity support systems.},
	language = {en},
	urldate = {2023-12-31},
	journal = {Computers in Human Behavior},
	author = {Wang, Kai and Nickerson, Jeffrey V.},
	month = sep,
	year = {2017},
	keywords = {Creativity support systems, Creativity theory, Design, Innovation, On paper},
	pages = {139--151},
	annote = {ISBN: 0747-5632},
	file = {Wang and Nickerson - 2017 - A literature review on individual creativity suppo.pdf:/Users/eleanorrow/Zotero/storage/87LAAUID/Wang and Nickerson - 2017 - A literature review on individual creativity suppo.pdf:application/pdf},
}

@article{ward-steinmanComposingDoingIt2011,
	title = {On {Composing}: {Doing} {It}, {Teaching} {It}, {Living} {It}},
	volume = {19},
	issn = {10635734},
	shorttitle = {On {Composing}},
	url = {https://muse.jhu.edu/article/440754},
	doi = {10.2979/philmusieducrevi.19.1.5},
	abstract = {This paper is concerned with the craft and pedagogy of contemporary classical composition, starting with an examination of French pedagogy as I received it from Darius Milhaud and Nadia Boulanger in the late 1950s. I discuss their different points of view (briefly the global approach to composition vs. what might be termed the molecular), the composition process itself (what can and what cannot be taught), counterpoint, improvisation, writer's block, the current state of the art, our sister arts, and the changing audience for serious music, along with the increasing importance of improvisation and world music in composition and its pedagogy. I also discuss how many of these elements were implemented in the model Comprehensive Musicianship curriculum at San Diego State University that I designed and directed from 1967 until 2003.},
	language = {en},
	number = {1},
	urldate = {2023-12-31},
	journal = {Philosophy of Music Education Review},
	author = {{Ward-Steinman}},
	year = {2011},
	pages = {5--23},
	annote = {Publisher: Indiana University Press JSTOR: 10.2979/philmusieducrevi.19.1.5},
	file = {Ward-Steinman - 2011 - On Composing Doing It, Teaching It, Living It.pdf:/Users/eleanorrow/Zotero/storage/Z5IXZ9EC/Ward-Steinman - 2011 - On Composing Doing It, Teaching It, Living It.pdf:application/pdf},
}

@book{williamsSingerSongwriterHandbook2017a,
	title = {The {Singer}-{Songwriter} {Handbook}},
	isbn = {978-1-62892-029-1 978-1-5013-9659-5},
	url = {http://www.bloomsburycollections.com/book/the-singer-songwriter-handbook},
	language = {en},
	urldate = {2023-12-31},
	publisher = {Bloomsbury Academic},
	author = {Williams, Justin A. and Williams, Katherine},
	year = {2017},
	doi = {10.5040/9781501396595},
	file = {Williams and Williams - 2017 - The Singer-Songwriter Handbook.pdf:/Users/eleanorrow/Zotero/storage/XVB27IG8/Williams and Williams - 2017 - The Singer-Songwriter Handbook.pdf:application/pdf},
}

@misc{wuMuseMorphoseFullSongFineGrained2022,
	title = {{MuseMorphose}: {Full}-{Song} and {Fine}-{Grained} {Piano} {Music} {Style} {Transfer} with {One} {Transformer} {VAE}},
	shorttitle = {{MuseMorphose}},
	url = {http://arxiv.org/abs/2105.04090},
	abstract = {Transformers and variational autoencoders (VAE) have been extensively employed for symbolic (e.g., MIDI) domain music generation. While the former boast an impressive capability in modeling long sequences, the latter allow users to willingly exert control over different parts (e.g., bars) of the music to be generated. In this paper, we are interested in bringing the two together to construct a single model that exhibits both strengths. The task is split into two steps. First, we equip Transformer decoders with the ability to accept segment-level, time-varying conditions during sequence generation. Subsequently, we combine the developed and tested in-attention decoder with a Transformer encoder, and train the resulting MuseMorphose model with the VAE objective to achieve style transfer of long musical pieces, in which users can specify musical attributes including rhythmic intensity and polyphony (i.e., harmonic fullness) they desire, down to the bar level. Experiments show that MuseMorphose outperforms recurrent neural network (RNN) based baselines on numerous widely-used metrics for style transfer tasks.},
	language = {en},
	urldate = {2023-12-31},
	publisher = {arXiv},
	author = {Wu, Shih-Lun and Yang, Yi-Hsuan},
	month = dec,
	year = {2022},
	note = {arXiv:2105.04090 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Multimedia, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: Accepted for Publication at IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP). Online supplemental materials are attached to the end of this arXiv version},
	file = {Wu and Yang - 2022 - MuseMorphose Full-Song and Fine-Grained Piano Mus.pdf:/Users/eleanorrow/Zotero/storage/SJFFE45E/Wu and Yang - 2022 - MuseMorphose Full-Song and Fine-Grained Piano Mus.pdf:application/pdf},
}

@misc{wyseAudioSpectrogramRepresentations2017,
	title = {Audio {Spectrogram} {Representations} for {Processing} with {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1706.09559},
	abstract = {One of the decisions that arise when designing a neural network for any application is how the data should be represented in order to be presented to, and possibly generated by, a neural network. For audio, the choice is less obvious than it seems to be for visual images, and a variety of representations have been used for diﬀerent applications including the raw digitized sample stream, hand-crafted features, machine discovered features, MFCCs and variants that include deltas, and a variety of spectral representations. This paper reviews some of these representations and issues that arise, focusing particularly on spectrograms for generating audio using neural networks for style transfer.},
	language = {en},
	urldate = {2023-12-31},
	publisher = {arXiv},
	author = {Wyse, L.},
	month = jun,
	year = {2017},
	note = {arXiv:1706.09559 [cs]},
	keywords = {68Txx, C.1.3, Computer Science - Machine Learning, Computer Science - Multimedia, Computer Science - Neural and Evolutionary Computing, Computer Science - Sound, data representation, H.5.1, sound synthesis, spectrograms, style transfer},
	annote = {Comment: Proceedings of the First International Conference on Deep Learning and Music, Anchorage, US, May, 2017 (arXiv:1706.08675v1 [cs.NE])},
	annote = {\_eprint: 1706.09559},
	file = {Wyse - 2017 - Audio Spectrogram Representations for Processing w.pdf:/Users/eleanorrow/Zotero/storage/QMGK4FZA/Wyse - 2017 - Audio Spectrogram Representations for Processing w.pdf:application/pdf},
}

@article{yangEvaluationGenerativeModels2020,
	title = {On the evaluation of generative models in music},
	volume = {32},
	issn = {0941-0643, 1433-3058},
	url = {http://link.springer.com/10.1007/s00521-018-3849-7},
	doi = {10.1007/s00521-018-3849-7},
	abstract = {The modeling of artiﬁcial, human-level creativity is becoming more and more achievable. In recent years, neural networks have been successfully applied to different tasks such as image and music generation, demonstrating their great potential in realizing computational creativity. The fuzzy deﬁnition of creativity combined with varying goals of the evaluated generative systems, however, makes subjective evaluation seem to be the only viable methodology of choice. We review the evaluation of generative music systems and discuss the inherent challenges of their evaluation. Although subjective evaluation should always be the ultimate choice for the evaluation of creative results, researchers unfamiliar with rigorous subjective experiment design and without the necessary resources for the execution of a large-scale experiment face challenges in terms of reliability, validity, and replicability of the results. In numerous studies, this leads to the report of insigniﬁcant and possibly irrelevant results and the lack of comparability with similar and previous generative systems. Therefore, we propose a set of simple musically informed objective metrics enabling an objective and reproducible way of evaluating and comparing the output of music generative systems. We demonstrate the usefulness of the proposed metrics with several experiments on real-world data.},
	language = {en},
	number = {9},
	urldate = {2023-12-31},
	journal = {Neural Computing and Applications},
	author = {Yang, Li-Chia and Lerch, Alexander},
	month = may,
	year = {2020},
	pages = {4773--4784},
	file = {Yang and Lerch - 2020 - On the evaluation of generative models in music.pdf:/Users/eleanorrow/Zotero/storage/D9VNQ2L2/Yang and Lerch - 2020 - On the evaluation of generative models in music.pdf:application/pdf},
}

@misc{yangDiffusionModelsComprehensive2023,
	title = {Diffusion {Models}: {A} {Comprehensive} {Survey} of {Methods} and {Applications}},
	shorttitle = {Diffusion {Models}},
	url = {http://arxiv.org/abs/2209.00796},
	abstract = {Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language generation, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github: https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy.},
	language = {en},
	urldate = {2023-12-31},
	publisher = {arXiv},
	author = {Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
	month = oct,
	year = {2023},
	note = {arXiv:2209.00796 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {arXiv:2209.00796 [cs]},
	annote = {Comment: 39 pages, 9 figures, citing 289 papers, project: https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy},
	annote = {Comment: 49 pages, 16 figures, citing 342 (up-to-date) papers, project: https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy, accepted by ACM Computing Surveys in https://dl.acm.org/doi/10.1145/3626235},
	file = {Yang et al. - 2023 - Diffusion Models A Comprehensive Survey of Method.pdf:/Users/eleanorrow/Zotero/storage/6AAFZZPB/Yang et al. - 2023 - Diffusion Models A Comprehensive Survey of Method.pdf:application/pdf},
}

@article{yeMusicStyleTransfer2020a,
	title = {Music {Style} {Transfer} with {Vocals} {Based} on {CycleGAN}},
	volume = {1631},
	issn = {1742-6588, 1742-6596},
	url = {https://iopscience.iop.org/article/10.1088/1742-6596/1631/1/012039},
	doi = {10.1088/1742-6596/1631/1/012039},
	abstract = {In recent years, with the development of generative adversarial networks (GAN), the application of generative adversarial networks has gradually matured. An important application area for generating adversarial networks is called neural style transfer. In recent years, neural style transfer has played a major role in the field of image applications. However, it performed poorly in the music field. In addition, algorithms in the field of music style transfer have poor effect on the style transfer of music with vocals. Therefore, this paper extracts the CQT features and Mel spectrogram features of music, and then uses CycleGAN to transfer the styles of the CQT features and Mel spectrogram mapping pictures, and finally realizes the style transfer of music. On the classifier we trained, the average style transfer rate of music that meets our requirements reached 94.07\%.},
	language = {en},
	number = {1},
	urldate = {2023-12-31},
	journal = {Journal of Physics: Conference Series},
	author = {Ye, Hongliang and Zhu, Wanning},
	month = sep,
	year = {2020},
	pages = {012039},
	annote = {Publisher: IOP Publishing},
	file = {Ye and Zhu - 2020 - Music Style Transfer with Vocals Based on CycleGAN.pdf:/Users/eleanorrow/Zotero/storage/HCJIP26F/Ye and Zhu - 2020 - Music Style Transfer with Vocals Based on CycleGAN.pdf:application/pdf},
}

@article{zacharakisEvaluatingHumanComputerCocreative2021,
	title = {Evaluating {Human}-{Computer} {Co}-creative {Processes} in {Music}: {A} {Case} {Study} on the {CHAMELEON} {Melodic} {Harmonizer}},
	volume = {12},
	issn = {1664-1078},
	shorttitle = {Evaluating {Human}-{Computer} {Co}-creative {Processes} in {Music}},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2021.603752/full},
	doi = {10.3389/fpsyg.2021.603752},
	abstract = {CHAMELEON is a computational melodic harmonization assistant. It can harmonize a given melody according to a number of independent harmonic idioms or blends between idioms based on principles of conceptual blending theory. Thus, the system is capable of offering a wealth of possible solutions and viewpoints for melodic harmonization. This study investigates how human creativity may be inﬂuenced by the use of CHAMELEON in a melodic harmonization task. Professional and novice music composers participated in an experiment where they were asked to harmonize two similar melodies under two different conditions: one with and one without computational support. A control group harmonized both melodies without computational assistance. The inﬂuence of the system was examined both behaviorally, by comparing metrics of user-experience, and in terms of the properties of the artifacts (i.e., pitch class distribution and number of chord types characterizing each harmonization) that were created between the two experimental conditions. Results suggest that appreciation of the system was expertise-dependent (i.e., novices appreciated the computational support more than professionals). At the same time, users seemed to adopt more explorative strategies as a result of interaction with CHAMELEON based on the fact that the harmonizations created this way were more complex, diverse, and unexpected in comparison to the ones of the control group.},
	language = {en},
	number = {February},
	urldate = {2023-12-31},
	journal = {Frontiers in Psychology},
	author = {Zacharakis, Asterios and Kaliakatsos-Papakostas, Maximos and Kalaitzidou, Stamatia and Cambouropoulos, Emilios},
	month = feb,
	year = {2021},
	keywords = {conceptual blending, creativity evaluation, creativity support tools, melodic harmonization, musical harmony, subjective feedback},
	pages = {603752},
	file = {Zacharakis et al. - 2021 - Evaluating Human-Computer Co-creative Processes in.pdf:/Users/eleanorrow/Zotero/storage/RWE48ITV/Zacharakis et al. - 2021 - Evaluating Human-Computer Co-creative Processes in.pdf:application/pdf},
}

@article{zalkowMusicalStyleModification2016,
	title = {Musical {Style} {Modiﬁcation} as an {Optimization} {Problem}},
	abstract = {This paper concerns musical style modiﬁcation on symbolic level. It introduces a new, ﬂexible method for changing a given piece of music so that its style is modiﬁed to another one that previously has been learned from a corpus of musical pieces. Mainly this is an optimization task with the music’s note events being optimized for different objectives relating to that corpus. The method has been developed for the use case of pushing existing monophonic pieces of music closer to the style of the outstanding electric bass guitar player Jaco Pastorius.},
	language = {en},
	journal = {. INTRODUCTION},
	author = {Zalkow, Frank and Brand, Stephan and Graf, Bejamin},
	year = {2016},
	pages = {6},
	file = {Zalkow et al. - 2016 - Musical Style Modiﬁcation as an Optimization Probl.pdf:/Users/eleanorrow/Zotero/storage/IABEUE27/Zalkow et al. - 2016 - Musical Style Modiﬁcation as an Optimization Probl.pdf:application/pdf},
}

@inproceedings{zengMusicBERTSymbolicMusic2021,
	address = {Online},
	title = {{MusicBERT}: {Symbolic} {Music} {Understanding} with {Large}-{Scale} {Pre}-{Training}},
	shorttitle = {{MusicBERT}},
	url = {https://aclanthology.org/2021.findings-acl.70},
	doi = {10.18653/v1/2021.findings-acl.70},
	abstract = {Symbolic music understanding, which refers to the understanding of music from the symbolic data (e.g., MIDI format, but not audio), covers many music applications such as genre classiﬁcation, emotion classiﬁcation, and music pieces matching. While good music representations are beneﬁcial for these applications, the lack of training data hinders representation learning. Inspired by the success of pre-training models in natural language processing, in this paper, we develop MusicBERT, a large-scale pre-trained model for music understanding. To this end, we construct a large-scale symbolic music corpus that contains more than 1 million music songs. Since symbolic music contains more structural (e.g., bar, position) and diverse information (e.g., tempo, instrument, and pitch), simply adopting the pre-training techniques from NLP to symbolic music only brings marginal gains. Therefore, we design several mechanisms, including OctupleMIDI encoding and bar-level masking strategy, to enhance pre-training with symbolic music data. Experiments demonstrate the advantages of MusicBERT on four music understanding tasks, including melody completion, accompaniment suggestion, genre classiﬁcation, and style classiﬁcation. Ablation studies also verify the effectiveness of our designs of OctupleMIDI encoding and barlevel masking strategy in MusicBERT.},
	language = {en},
	urldate = {2023-12-31},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL}-{IJCNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Zeng, Mingliang and Tan, Xu and Wang, Rui and Ju, Zeqian and Qin, Tao and Liu, Tie-Yan},
	year = {2021},
	pages = {791--800},
	annote = {event-place: Online},
	file = {Zeng et al. - 2021 - MusicBERT Symbolic Music Understanding with Large.pdf:/Users/eleanorrow/Zotero/storage/98KPRCJ9/Zeng et al. - 2021 - MusicBERT Symbolic Music Understanding with Large.pdf:application/pdf},
}

@misc{zhangContrastiveLearningMedical2022,
	title = {Contrastive {Learning} of {Medical} {Visual} {Representations} from {Paired} {Images} and {Text}},
	url = {http://arxiv.org/abs/2010.00747},
	abstract = {Learning visual representations of medical images is core to medical image understanding but its progress has been held back by the small size of hand-labeled datasets. Existing work commonly relies on transferring weights from ImageNet pretraining, which is suboptimal due to drastically different image characteristics, or rule-based label extraction from the textual report data paired with medical images, which is inaccurate and hard to generalize. We propose an alternative unsupervised strategy to learn medical visual representations directly from the naturally occurring pairing of images and textual data. Our method of pretraining medical image encoders with the paired text data via a bidirectional contrastive objective between the two modalities is domain-agnostic, and requires no additional expert input. We test our method by transferring our pretrained weights to 4 medical image classiﬁcation tasks and 2 zero-shot retrieval tasks, and show that our method leads to image representations that considerably outperform strong baselines in most settings. Notably, in all 4 classiﬁcation tasks, our method requires only 10\% as much labeled training data as an ImageNet initialized counterpart to achieve better or comparable performance, demonstrating superior data efﬁciency.},
	language = {en},
	urldate = {2023-12-31},
	publisher = {arXiv},
	author = {Zhang, Yuhao and Jiang, Hang and Miura, Yasuhide and Manning, Christopher D. and Langlotz, Curtis P.},
	month = sep,
	year = {2022},
	note = {arXiv:2010.00747 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: First published in 2020. Accepted at Machine Learning for Healthcare (MLHC) 2022},
	file = {Zhang et al. - 2022 - Contrastive Learning of Medical Visual Representat.pdf:/Users/eleanorrow/Zotero/storage/5P85WPXY/Zhang et al. - 2022 - Contrastive Learning of Medical Visual Representat.pdf:application/pdf},
}

@article{zhangConsumerAICocreation2021,
	title = {Consumer and {AI} {Co}-creation: {When} and {Why} {Human} {Participation} {Improves} {AI} {Creation}.},
	issn = {1556-5068},
	shorttitle = {Consumer and {AI} {Co}-creation},
	url = {https://www.ssrn.com/abstract=3929070},
	doi = {10.2139/ssrn.3929070},
	abstract = {Firms are increasingly leveraging artificial intelligence (AI) to automatically create personalized products (e.g., custom photo products, home designs) for consumers. While AI automation substantially saves consumer effort, it may also reduce consumer engagement potentially leading to a high dropout rate. We conduct three studies to investigate whether and how firms should nudge consumers to participate in product co-creation with AI automation. Using a field experiment involving 128,153 consumers, we find that a simple nudge can significantly increase consumer participation in product co-creation with AI by 12\%. The nudged participation further increases immediate purchase by 22\% and post-experiment purchase by 2\%. Such purchase lifts are greater when consumers create more complex projects, have prior creation experience, or have lower opportunity costs of time. Our second experiment explores how and why firms should encourage consumer-AI co-creation. We learn that nudging attention (e.g., prompting a preview of the creation) rather than participation does not increase purchase and revisit intentions, and that mandating participation is less effective than nudging participation. Mediation tests show that the IKEA effect (‘I made it myself') plays a bigger role than preference fit in explaining the positive effects of nudging participation. Our third experiment further investigates the interplay between AI creation and nudging participation. We find that nudging participation increase purchase and revisit intentions only when AI creation is present. The consumer-AI co-creation brings the best of both worlds by leveraging AI to save upfront effort and prompting humans to engage in the creative process only if desired.},
	language = {en},
	number = {September},
	urldate = {2023-12-31},
	journal = {SSRN Electronic Journal},
	author = {Zhang, Mengxia and Sun, Tianshu and Luo, Lan and Golden, Joseph},
	year = {2021},
	file = {Zhang et al. - 2021 - Consumer and AI Co-creation When and Why Human Pa.pdf:/Users/eleanorrow/Zotero/storage/HE38CBYG/Zhang et al. - 2021 - Consumer and AI Co-creation When and Why Human Pa.pdf:application/pdf},
}

@misc{zhangWuYunExploringHierarchical2023,
	title = {{WuYun}: {Exploring} hierarchical skeleton-guided melody generation using knowledge-enhanced deep learning},
	shorttitle = {{WuYun}},
	url = {http://arxiv.org/abs/2301.04488},
	abstract = {Although deep learning has revolutionized music generation, existing methods for structured melody generation follow an end-to-end left-to-right note-by-note generative paradigm and treat each note equally. Here, we present WuYun, a knowledge-enhanced deep learning architecture for improving the structure of generated melodies, which ﬁrst generates the most structurally important notes to construct a melodic skeleton and subsequently inﬁlls it with dynamically decorative notes into a full-ﬂedged melody. Speciﬁcally, we use music domain knowledge to extract melodic skeletons and employ sequence learning to reconstruct them, which serve as additional knowledge to provide auxiliary guidance for the melody generation process. We demonstrate that WuYun can generate melodies with better long-term structure and musicality and outperforms other state-of-the-art methods by 0.51 on average on all subjective evaluation metrics. Our study provides a multidisciplinary lens to design melodic hierarchical structures and bridge the gap between data-driven and knowledge-based approaches for numerous music generation tasks.},
	language = {en},
	urldate = {2023-12-31},
	publisher = {arXiv},
	author = {Zhang, Kejun and Wu, Xinda and Zhang, Tieyao and Huang, Zhijie and Tan, Xu and Liang, Qihao and Wu, Songruoyao and Sun, Lingyun},
	month = mar,
	year = {2023},
	note = {arXiv:2301.04488 [cs, eess]},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {arXiv:2301.04488 [cs, eess]},
	file = {Zhang et al. - 2023 - WuYun Exploring hierarchical skeleton-guided melo.pdf:/Users/eleanorrow/Zotero/storage/GD9EW8JF/Zhang et al. - 2023 - WuYun Exploring hierarchical skeleton-guided melo.pdf:application/pdf},
}

@inproceedings{zhouVisionInfusedDeepAudio2019a,
	address = {Seoul, Korea (South)},
	title = {Vision-{Infused} {Deep} {Audio} {Inpainting}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9008233/},
	doi = {10.1109/ICCV.2019.00037},
	abstract = {Multi-modality perception is essential to develop interactive intelligence. In this work, we consider a new task of visual information-infused audio inpainting, i.e. synthesizing missing audio segments that correspond to their accompanying videos. We identify two key aspects for a successful inpainter: (1) It is desirable to operate on spectrograms instead of raw audios. Recent advances in deep semantic image inpainting could be leveraged to go beyond the limitations of traditional audio inpainting. (2) To synthesize visually indicated audio, a visual-audio joint feature space needs to be learned with synchronization of audio and video. To facilitate a large-scale study, we collect a new multi-modality instrument-playing dataset called MUSIC-Extra-Solo (MUSICES) by enriching MUSIC dataset [51]. Extensive experiments demonstrate that our framework is capable of inpainting realistic and varying audio segments with or without visual contexts. More importantly, our synthesized audio segments are coherent with their video counterparts, showing the effectiveness of our proposed Vision-Infused Audio Inpainter (VIAI). Code, models, dataset and video results are available at https://github.com/Hangz-nju-cuhk/ Vision-Infused-Audio-Inpainter-VIAI.},
	language = {en},
	urldate = {2023-12-31},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zhou, Hang and Liu, Ziwei and Xu, Xudong and Luo, Ping and Wang, Xiaogang},
	month = oct,
	year = {2019},
	keywords = {Audio, Inpainting, Models},
	pages = {283--292},
	annote = {ISBN: 9781728148038},
	file = {Zhou et al. - 2019 - Vision-Infused Deep Audio Inpainting.pdf:/Users/eleanorrow/Zotero/storage/CSQMDKLE/Zhou et al. - 2019 - Vision-Infused Deep Audio Inpainting.pdf:application/pdf},
}

@inproceedings{zhuExplainableAIDesigners2018,
	address = {Maastricht},
	title = {Explainable {AI} for {Designers}: {A} {Human}-{Centered} {Perspective} on {Mixed}-{Initiative} {Co}-{Creation}},
	volume = {2018-Augus},
	isbn = {978-1-5386-4359-4},
	shorttitle = {Explainable {AI} for {Designers}},
	url = {https://ieeexplore.ieee.org/document/8490433/},
	doi = {10.1109/CIG.2018.8490433},
	abstract = {Growing interest in eXplainable Artiﬁcial Intelligence (XAI) aims to make AI and machine learning more understandable to human users. However, most existing work focuses on new algorithms, and not on usability, practical interpretability and efﬁcacy on real users. In this vision paper, we propose a new research area of eXplainable AI for Designers (XAID), speciﬁcally for game designers. By focusing on a speciﬁc user group, their needs and tasks, we propose a human-centered approach for facilitating game designers to co-create with AI/ML techniques through XAID. We illustrate our initial XAID framework through three use cases, which require an understanding both of the innate properties of the AI techniques and users’ needs, and we identify key open challenges.},
	language = {en},
	urldate = {2023-12-31},
	booktitle = {2018 {IEEE} {Conference} on {Computational} {Intelligence} and {Games} ({CIG})},
	publisher = {IEEE},
	author = {Zhu, Jichen and Liapis, Antonios and Risi, Sebastian and Bidarra, Rafael and Youngblood, G. Michael},
	month = aug,
	year = {2018},
	keywords = {Explainable artificial intelligence, Game design, Human-computer interaction, Machine learning, Mixed-initiative co-creation},
	pages = {1--8},
	annote = {ISBN: 9781538643594 Publisher: IEEE},
	file = {Zhu et al. - 2018 - Explainable AI for Designers A Human-Centered Per.pdf:/Users/eleanorrow/Zotero/storage/7UNSQIUP/Zhu et al. - 2018 - Explainable AI for Designers A Human-Centered Per.pdf:application/pdf},
}

@article{zulicHowAICan2019,
	title = {How {AI} can {Change}/{Improve}/{Influence} {Music} {Composition}, {Performance} and {Education}: {Three} {Case} {Studies}},
	issn = {2637-1898},
	shorttitle = {How {AI} can {Change}/{Improve}/{Influence} {Music} {Composition}, {Performance} and {Education}},
	url = {https://insam-institute.com/how-ai-can-change-improve-influence-music-composition-performance-and-education-three-case-studies/},
	doi = {10.51191/issn.2637-1898.2019.2.2.100},
	abstract = {The use of artificial intelligence in science is happening more and more frequently, and often artificial intelligence can be seen in different approaches to creating music and art. In this paper, I will present some of the research that has been carried out, which involve the use of artificial intelligence in the field of composition, performance, and music education. The main focus in the field of composition will be on AIVA – the first virtual composer created with artificial intelligence, which is registered with an author’s rights society. In the field of performance, we’ll mostly talk about Yamaha's experiment where the world-renowned dancer Kaiji Moriyama controls a piano with his body movements, and in the context of education, this paper reviews some of the possibilities in a variety of artificial intelligence approaches to music education. Lastly, I will conclude the paper by presenting the direction of and possible future for the use of artificial intelligence in music.},
	language = {en},
	number = {2},
	urldate = {2023-12-31},
	journal = {INSAM Journal of Contemporary Music, Art and Technology},
	author = {Zulić, Harun},
	month = jul,
	year = {2019},
	pages = {100--114},
	file = {Zulić - 2019 - How AI can ChangeImproveInfluence Music Composit.pdf:/Users/eleanorrow/Zotero/storage/KTENBRPI/Zulić - 2019 - How AI can ChangeImproveInfluence Music Composit.pdf:application/pdf},
}

@inproceedings{gurevichExpressionItsDiscontents2007,
	address = {New York, NY, USA},
	series = {{NIME} '07},
	title = {Expression and {Its} {Discontents}: {Toward} an {Ecology} of {Musical} {Creation}},
	isbn = {978-1-4503-7837-6},
	shorttitle = {Expression and {Its} {Discontents}},
	url = {https://doi.org/10.1145/1279740.1279759},
	doi = {10.1145/1279740.1279759},
	abstract = {We describe the prevailing model of musical expression, which assumes a binary formulation of "the text" and "the act," along with its implied roles of composer and performer. We argue that this model not only excludes some contemporary aesthetic values but also limits the communicative ability of new music interfaces. As an alternative, an ecology of musical creation accounts for both a diversity of aesthetic goals and the complex interrelation of human and non-human agents. An ecological perspective on several approaches to musical creation with interactive technologies reveals an expanded, more inclusive view of artistic interaction that facilitates novel, compelling ways to use technology for music. This paper is fundamentally a call to consider the role of aesthetic values in the analysis of artistic processes and technologies.},
	language = {en},
	booktitle = {Proceedings of the 7th {International} {Conference} on {New} {Interfaces} for {Musical} {Expression}},
	publisher = {Association for Computing Machinery},
	author = {Gurevich, Michael and Treviño, Jeffrey},
	month = jun,
	year = {2007},
	keywords = {aesthetic goal, communication, construct, discipline, discourse, emotion, evaluation, experience, expression, expressivity, model, non-expressive, Participatory Design, transparency},
	pages = {106--111},
	file = {Gurevich and Treviño - 2007 - Expression and Its Discontents Toward an Ecology .pdf:/Users/eleanorrow/Zotero/storage/4JYMPQE4/Gurevich and Treviño - 2007 - Expression and Its Discontents Toward an Ecology .pdf:application/pdf},
}

@misc{ravikumarLOOKINGCREATIVESUPPORT2019,
	title = {Looking beyond creative support : {Augmenting} the perceived agency of music co-creation systems looking beyond creative support : {Augmenting} the perceived agency of music co-creation systems},
	author = {Ravikumar, Prashanth Thattai},
	year = {2019},
	file = {Ravikumar - 2019 - Looking beyond creative support  Augmenting the p.pdf:/Users/eleanorrow/Zotero/storage/LQAU3QQ6/Ravikumar - 2019 - Looking beyond creative support  Augmenting the p.pdf:application/pdf},
}

@article{zhangCOSMICConversationalInterface2021,
	title = {{COSMIC}: {A} {Conversational} {Interface} for {Human}-{AI} {Music} {Co}-{Creation}},
	doi = {10.21428/92fbeb44.110a7a32},
	author = {Zhang, Yixiao and Xia, Gus and Levy, Mark and Dixon, Simon},
	year = {2021},
	pages = {1--9},
	file = {Zhang et al. - COSMIC A Conversational Interface for Human-AI Mu.pdf:/Users/eleanorrow/Zotero/storage/ZEN3XXC9/Zhang et al. - COSMIC A Conversational Interface for Human-AI Mu.pdf:application/pdf},
}

@inproceedings{donahuePianoGenie2019,
	title = {Piano {Genie}},
	volume = {Part F1476},
	doi = {10.1145/3301275.3302288},
	abstract = {We present Piano Genie, an intelligent controller which allows non-musicians to improvise on the piano. With Piano Genie, a user performs on a simple interface with eight buttons, and their performance is decoded into the space of plausible piano music in real time. To learn a suitable mapping procedure for this problem, we train recurrent neural network autoencoders with discrete bottlenecks: an encoder learns an appropriate sequence of buttons corresponding to a piano piece, and a decoder learns to map this sequence back to the original piece. During performance, we substitute a user's input for the encoder output, and play the decoder's prediction each time the user presses a button. To improve the intuitiveness of Piano Genie's performance behavior, we impose musically meaningful constraints over the encoder's outputs.},
	booktitle = {International {Conference} on {Intelligent} {User} {Interfaces}, {Proceedings} {IUI}},
	author = {Donahue, Chris and Simon, Ian and Dieleman, Sander},
	year = {2019},
	keywords = {Augmented intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Computer Science - Sound, Discrete representation learning, Electrical Engineering and Systems Science - Audio and Speech Processing, Generative modeling, Music, Piano, Real-time, Statistics - Machine Learning, Web},
	pages = {160--164},
	annote = {Comment: Published as a conference paper at ACM IUI 2019},
	annote = {\_eprint: 1810.05246},
	file = {Donahue et al. - 2019 - Piano Genie.pdf:/Users/eleanorrow/Zotero/storage/MQ23FTHQ/Donahue et al. - 2019 - Piano Genie.pdf:application/pdf},
}

@inproceedings{katharopoulosTransformersAreRNNs2020,
	title = {Transformers are {RNNs}: {Fast} {Autoregressive} {Transformers} with {Linear} {Attention}},
	volume = {PartF16814},
	shorttitle = {Transformers are {RNNs}},
	abstract = {Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-Attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from O-N2 to O(N), where N is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.},
	booktitle = {37th {International} {Conference} on {Machine} {Learning}, {ICML} 2020},
	author = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Francois},
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {5112--5121},
	annote = {Comment: ICML 2020, project at https://linear-transformers.com/},
	annote = {ISBN: 9781713821120 \_eprint: 2006.16236},
	file = {Katharopoulos et al. - 2020 - Transformers are RNNs Fast Autoregressive Transfo.pdf:/Users/eleanorrow/Zotero/storage/2UQZWKIH/Katharopoulos et al. - 2020 - Transformers are RNNs Fast Autoregressive Transfo.pdf:application/pdf},
}

@article{sanhDistilBERTDistilledVersion2019,
	title = {{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
	issn = {2331-8422},
	url = {http://arxiv.org/abs/1910.01108},
	abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
	author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
	year = {2019},
	pages = {2--6},
	annote = {\_eprint: 1910.01108},
	file = {Sanh - DistilBERT, a distilled version of BERT smaller, .pdf:/Users/eleanorrow/Zotero/storage/SNSLBN8A/Sanh - DistilBERT, a distilled version of BERT smaller, .pdf:application/pdf},
}

@misc{liuLargeMarginSoftmaxLoss2016,
	title = {Large-{Margin} {Softmax} {Loss} for {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1612.02295},
	abstract = {Cross-entropy loss together with softmax is arguably one of the most common used supervision components in convolutional neural networks (CNNs). Despite its simplicity, popularity and excellent performance, the component does not explicitly encourage discriminative learning of features. In this paper, we propose a generalized large-margin softmax (L-Softmax) loss which explicitly encourages intra-class compactness and inter-class separability between learned features. Moreover, L-Softmax not only can adjust the desired margin but also can avoid overfitting. We also show that the L-Softmax loss can be optimized by typical stochastic gradient descent. Extensive experiments on four benchmark datasets demonstrate that the deeply-learned features with L-softmax loss become more discriminative, hence significantly boosting the performance on a variety of visual classification and verification tasks.},
	author = {Liu, Weiyang and Wen, Yandong and Yu, Zhiding and Yang, Meng},
	year = {2016},
	annote = {\_eprint: 1612.02295},
	file = {Liu et al. - Large-Margin Softmax Loss for Convolutional Neural.pdf:/Users/eleanorrow/Zotero/storage/8CJRX2KZ/Liu et al. - Large-Margin Softmax Loss for Convolutional Neural.pdf:application/pdf},
}

@inproceedings{liGenerativeFaceCompletion2017,
	title = {Generative {Face} {Completion}},
	volume = {2017-Janua},
	doi = {10.1109/CVPR.2017.624},
	abstract = {In this paper, we propose an effective face completion algorithm using a deep generative model. Different from well-studied background completion, the face completion task is more challenging as it often requires to generate semantically new pixels for the missing key components (e.g., eyes and mouths) that contain large appearance variations. Unlike existing nonparametric algorithms that search for patches to synthesize, our algorithm directly generates contents for missing regions based on a neural network. The model is trained with a combination of a reconstruction loss, two adversarial losses and a semantic parsing loss, which ensures pixel faithfulness and local-global contents consistency. With extensive experimental results, we demonstrate qualitatively and quantitatively that our model is able to deal with a large area of missing pixels in arbitrary shapes and generate realistic face completion results.},
	booktitle = {Proceedings - 30th {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}, {CVPR} 2017},
	author = {Li, Yijun and Liu, Sifei and Yang, Jimei and Yang, Ming Hsuan},
	month = apr,
	year = {2017},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {5892--5900},
	annote = {Comment: Accepted by CVPR 2017},
	annote = {ISBN: 9781538604571 \_eprint: 1704.05838},
	file = {Li et al. - 2017 - Generative Face Completion.pdf:/Users/eleanorrow/Zotero/storage/RYJQFWSG/Li et al. - 2017 - Generative Face Completion.pdf:application/pdf},
}

@inproceedings{parmarImageTransformer2018,
	title = {Image transformer},
	volume = {9},
	abstract = {Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the selfattention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks. While conceptually simple, our generative models significantly outperform the current state of the art in image generation on ImageNet, improving the best published negative log-likelihood on ImageNet from 3.83 to 3.77. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we find that images generated by our super-resolution model fool human observers three times more often than the previous state of the art.},
	booktitle = {35th {International} {Conference} on {Machine} {Learning}, {ICML} 2018},
	author = {Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, Lukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
	year = {2018},
	pages = {6453--6462},
	annote = {ISBN: 9781510867963 \_eprint: 1802.05751},
	file = {Parmar et al. - Image Transformer.pdf:/Users/eleanorrow/Zotero/storage/Q4A8P8IX/Parmar et al. - Image Transformer.pdf:application/pdf},
}

@article{comptonCasualCreators2015,
	title = {Casual creators},
	abstract = {Many creativity tools exist to support task-focused creativity, but in recent years we have seen a flourishing of autotelic creativity tools, which privilege the enjoyable experience of explorative creativity over task-completion. Because these tools are much smaller in scope, less commercially significant, and less”serious” than their larger siblings, they have been overlooked in academic research. This paper coins the term”Casual Creators” for these tools, and provide a definition to identify tools that belong to this category. We also identify the particular design considerations that arise from autotelic creativity, and propose a number of strong design patterns that serve those considerations, patterns which are demonstrated by case studies of software built with those patterns. We believe that once this field is identified and named, the currently-isolated practitioners who make these casual creators will be able to share knowledge, like these design patterns, and develop a community of practice.},
	number = {June},
	journal = {Proceedings of the 6th International Conference on Computational Creativity, ICCC 2015},
	author = {Compton, Kate and Mateas, Michael},
	year = {2015},
	pages = {228--235},
	annote = {ISBN: 9780842529709},
	file = {compton_et_al_2015_casual_creators.pdf:/Users/eleanorrow/Zotero/storage/PPNHIV28/compton_et_al_2015_casual_creators.pdf:application/pdf},
}

@inproceedings{robertsMagentaStudioAugmenting2019,
	title = {Magenta {Studio}: {Augmenting} {Creativity} with {Deep} {Learning} in {Ableton} {Live}},
	url = {http://webgl.org},
	abstract = {The field of Musical Metacreation (MuMe) has produced impressive results for both autonomous and interactive creativity, recently aided by modern deep learning frameworks. However, there are few examples of these systems crossing over to the "mainstream" of music creation and consumption. We tie together existing frameworks (Electron, TensorFlow.js, and Max For Live) to develop a system whose purpose is to bring the promise of interactive MuMe to the realm of professional music creators. Combining compelling applications of deep learning-based music generation with a focus on ease of installation and use in a popular DAW, we hope to expose more musicians and producers to the potential of using such systems in their creative workflows. Our suite of plug-ins for Ableton Live, named Magenta Studio, is available for download at http://g.co/magenta/studio along with its open source implementation.},
	booktitle = {International {Workshop} on {Musical} {Metacreation} ({MUME})},
	author = {Roberts, Adam and Engel, Jesse and Mann, Yotam and Gillick, Jon and Kayacik, Claire and Nørly, Signe and Dinculescu, Monica and Radebaugh, Carey and Hawthorne, Curtis and Eck, Douglas},
	year = {2019},
	keywords = {Ableton Live, Generative Music, Javascript, Models, Real-Time},
	file = {Roberts et al. - Magenta Studio Augmenting Creativity with Deep Le.pdf:/Users/eleanorrow/Zotero/storage/R6DW29PI/Roberts et al. - Magenta Studio Augmenting Creativity with Deep Le.pdf:application/pdf},
}

@article{marafiotiContextEncoderAudio2019,
	title = {A {Context} {Encoder} for {Audio} {Inpainting}},
	volume = {27},
	issn = {23299304},
	doi = {10.1109/TASLP.2019.2947232},
	abstract = {In this article, we study the ability of deep neural networks (DNNs) to restore missing audio content based on its context, i.e., inpaint audio gaps. We focus on a condition which has not received much attention yet: gaps in the range of tens of milliseconds. We propose a DNN structure that is provided with the signal surrounding the gap in the form of time-frequency (TF) coefficients. Two DNNs with either complex-valued TF coefficient output or magnitude TF coefficient output were studied by separately training them on inpainting two types of audio signals (music and musical instruments) having 64-ms long gaps. The magnitude DNN outperformed the complex-valued DNN in terms of signal-to-noise ratios and objective difference grades. Although, for instruments, a reference inpainting obtained through linear predictive coding performed better in both metrics, it performed worse than the magnitude DNN for music. This demonstrates the potential of the magnitude DNN, in particular for inpainting signals that are more complex than single instrument sounds.},
	number = {12},
	journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
	author = {Marafioti, Andres and Perraudin, Nathanael and Holighaus, Nicki and Majdak, Piotr},
	year = {2019},
	keywords = {Audio, frequency-domain analysis, Inpainting, machine learning, Models, Music, signal processing algorithms},
	pages = {2362--2372},
	annote = {\_eprint: 1810.12138},
}

@inproceedings{daiMusicStyleTransfer2018,
	title = {Music style transfer: {A} position paper},
	abstract = {Led by the success of neural style transfer on visual arts, there has been a rising trend very recently in the effort of music style transfer. However, "music style" is not yet a well-defined concept from a scientific point of view. The difficulty lies in the intrinsic multi-level and multi-modal character of music representation (which is very different from image representation). As a result, depending on their interpretation of "music style", current studies under the category of "music style transfer", are actually solving completely different problems that belong to a variety of sub-fields of Computer Music. Also, a vanilla end-to-end approach, which aims at dealing with all levels of music representation at once by directly adopting the method of image style transfer, leads to poor results. Thus, we vitally propose a more scientifically-viable definition of music style transfer by breaking it down into precise concepts of timbre style transfer, performance style transfer and composition style transfer, as well as to connect different aspects of music style transfer with existing well-established sub-fields of computer music studies. In addition, we discuss the current limitations of music style modeling and its future directions by drawing spirit from some deep generative models, especially the ones using unsu-pervised learning and disentanglement techniques.},
	booktitle = {The 6th {International} {Workshop} on {Musical} {Metacreation}},
	author = {Dai, Shuqi and Zhang, Zheng and Xia, Gus G.},
	year = {2018},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Music, Style Transfer},
	annote = {Comment: In Proceeding of International Workshop on Musical Metacreation (MUME), 2018, Salamanca, Spain},
	annote = {\_eprint: 1803.06841},
	file = {Dai et al. - 2018 - Music Style Transfer A Position Paper.pdf:/Users/eleanorrow/Zotero/storage/IQH2MWKY/Dai et al. - 2018 - Music Style Transfer A Position Paper.pdf:application/pdf},
}

@inproceedings{bazinNONOTOModelagnosticWeb2019,
	title = {{NONOTO}: {A} model-agnostic web interface for interactive music composition by inpainting},
	abstract = {Inpainting-based generative modeling allows for stimulating human-machine interactions by letting users perform stylistically coherent local editions to an object using a statistical model. We present NONOTO, a new interface for interactive music generation based on inpainting models. It is aimed both at researchers, by offering a simple and flexible API allowing them to connect their own models with the interface, and at musicians by providing industry-standard features such as audio playback, real-time MIDI output and straightforward synchronization with DAWs using Ableton Link.},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Computational} {Creativity}, {ICCC} 2019},
	author = {Bazin, Théis and Hadjeres, Gaëtan},
	year = {2019},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Generative models, Inpainting, Interactive music generation, Interfaces, MIDI, Models, Open-source software, Web technologies},
	pages = {89--91},
	annote = {Comment: 3 pages, 1 figure. Published as a conference paper at the 10th International Conference on Computational Creativity (ICCC 2019), UNC Charlotte, North Carolina},
	annote = {ISBN: 9789895416011 \_eprint: 1907.10380},
	file = {Bazin and Hadjeres - 2019 - NONOTO A Model-agnostic Web Interface for Interac.pdf:/Users/eleanorrow/Zotero/storage/DNEQ7RRW/Bazin and Hadjeres - 2019 - NONOTO A Model-agnostic Web Interface for Interac.pdf:application/pdf},
}

@misc{kotechaBach2BachGeneratingMusic2018,
	title = {{Bach2Bach}: {Generating} {Music} {Using} {A} {Deep} {Reinforcement} {Learning} {Approach}},
	url = {https://ui.adsabs.harvard.edu/abs/2018arXiv181201060K},
	abstract = {A model of music needs to have the ability to recall past details and have a clear, coherent understanding of musical structure. Detailed in the paper is a deep reinforcement learning architecture that predicts and generates polyphonic music aligned with musical rules. The probabilistic model presented is a Bi-axial LSTM trained with a pseudo- kernel reminiscent of a convolutional kernel. To encourage exploration and impose greater global coherence on the generated music, a deep reinforcement learning approach DQN is adopted. When analyzed quantitatively and qualitatively, this approach performs well in composing polyphonic music. {\textless}P /{\textgreater}},
	author = {Kotecha, Nikhil},
	year = {2018},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio, On paper, Processing, Statistics - Machine Learning},
	annote = {Pages: arXiv:1812.01060 Publication Title: arXiv e-prints},
	file = {kotecha_2018_bach2bach.pdf:/Users/eleanorrow/Zotero/storage/33C348PC/kotecha_2018_bach2bach.pdf:application/pdf},
}

@article{patiDisentanglementEnoughLatent2021,
	title = {Is {Disentanglement} enough? {On} {Latent} {Representations} for {Controllable} {Music} {Generation}},
	url = {http://arxiv.org/abs/2108.01450},
	abstract = {Improving controllability or the ability to manipulate one or more attributes of the generated data has become a topic of interest in the context of deep generative models of music. Recent attempts in this direction have relied on learning disentangled representations from data such that the underlying factors of variation are well separated. In this paper, we focus on the relationship between disentanglement and controllability by conducting a systematic study using different supervised disentanglement learning algorithms based on the Variational Auto-Encoder (VAE) architecture. Our experiments show that a high degree of disentanglement can be achieved by using different forms of supervision to train a strong discriminative encoder. However, in the absence of a strong generative decoder, disentanglement does not necessarily imply controllability. The structure of the latent space with respect to the VAE-decoder plays an important role in boosting the ability of a generative model to manipulate different attributes. To this end, we also propose methods and metrics to help evaluate the quality of a latent space with respect to the afforded degree of controllability.},
	author = {Pati, Ashis and Lerch, Alexander},
	year = {2021},
	annote = {\_eprint: 2108.01450},
	file = {Pati and Lerch - IS DISENTANGLEMENT ENOUGH ON LATENT REPRESENTATIO.pdf:/Users/eleanorrow/Zotero/storage/LZIS8GR3/Pati and Lerch - IS DISENTANGLEMENT ENOUGH ON LATENT REPRESENTATIO.pdf:application/pdf},
}

@article{mumfordParticipativeSystemsDesign1983,
	title = {Participative {Systems} {Design}: {Practice} and {Theory}},
	volume = {4},
	issn = {0142-2774},
	shorttitle = {Participative {Systems} {Design}},
	url = {https://www.jstor.org/stable/3000226},
	abstract = {This paper describes how a participative methodology to involve users at all levels in the design of new technical and work systems has been developed by Enid Mumford. Working with design groups in different companies has enabled her to test out different levels of participation from consultative to consensus, and to arrive at the conclusion that an attempt to involve all affected individuals and groups in decision-taking is most rewarding but difficult to manage. A requirement of participation in decisions for organizational change is that users shall have the skills and knowledge to play a significant role in the total design process and can take informed decisions at each design stage from defining the problem to operating the new system. A comprehensive methodology to give people the necessary skills has been developed, again as a result of working with design groups in different companies. The evolution and testing out of this methodology is described in this paper.},
	number = {1},
	urldate = {2022-03-02},
	journal = {Journal of Occupational Behaviour},
	author = {Mumford, Enid},
	year = {1983},
	keywords = {Participatory Design},
	pages = {47--57},
	annote = {Publisher: Wiley},
	file = {Mumford - 1983 - Participative Systems Design Practice and Theory.pdf:/Users/eleanorrow/Zotero/storage/EGJJBTLI/Mumford - 1983 - Participative Systems Design Practice and Theory.pdf:application/pdf},
}

@inproceedings{fiebrinkUnderstandingHumanComputerInteraction2010,
	title = {Toward {Understanding} {Human}-{Computer} {Interaction} {In} {Composing} {The} {Instrument}},
	abstract = {A weekly seminar consisting of seven composers and one computer scientist was convened for the purpose of exploring questions surrounding how technology can support aspects of the computer music composition process. The composers were introduced to an existing interactive software system for creating new musical interfaces and compositions, which they used throughout the seminar. The group engaged in a user-centered design process to critically evaluate and improve this software. Through documentation of the experience and analysis of composers’ responses to a questionnaire following the seminar, we achieved a richer understanding of how technology can support composers’ modes of working and goals in the process of computer music interface design and composition. This work also resulted in an improved compositional software system and progress toward several new musical compositions and instruments.},
	language = {en},
	booktitle = {{ICMC}},
	author = {Fiebrink, Rebecca and Trueman, Daniel and Britt, Cameron and Nagai, Michelle and Kaczmarek, Konrad and Early, Michael and Daniel, MR and Hege, Anne and Cook, Perry},
	year = {2010},
	pages = {8},
	file = {Fiebrink et al. - TOWARD UNDERSTANDING HUMAN-COMPUTER INTERACTION IN.pdf:/Users/eleanorrow/Zotero/storage/VTWLFGHJ/Fiebrink et al. - TOWARD UNDERSTANDING HUMAN-COMPUTER INTERACTION IN.pdf:application/pdf},
}

@book{bodenCreativeMindMyths2003,
	edition = {2nd edition},
	title = {The {Creative} {Mind}: {Myths} and {Mechanisms}},
	isbn = {0-415-31453-4},
	abstract = {How is it possible to think new thoughts? What is creativity and can science explain it? And just how did Coleridge dream up the creatures of The Ancient Mariner? When The Creative Mind: Myths and Mechanisms was first published, Margaret A. Boden's bold and provocative exploration of creativity broke new ground. Boden uses examples such as jazz improvisation, chess, story writing, physics, and the music of Mozart, together with computing models from the field of artificial intelligence to uncover the nature of human creativity in the arts. The second edition of The Creative Mind has been updated to include recent developments in artificial intelligence, with a new preface, introduction and conclusion by the author. It is an essential work for anyone interested in the creativity of the human mind. "synopsis" may belong to another edition of this title.},
	publisher = {Routledge},
	author = {Boden, Margaret},
	year = {2003},
	file = {boden_2003_the_creative_mind.pdf:/Users/eleanorrow/Zotero/storage/SXJMAB4Y/boden_2003_the_creative_mind.pdf:application/pdf},
}

@incollection{csikszentmihalyiFlowPsychologyOptimal1990,
	title = {Flow: {The} {Psychology} of {Optimal} {Experience}},
	shorttitle = {Flow},
	author = {Csikszentmihalyi, Mihaly},
	month = jan,
	year = {1990},
	file = {Csikszentmihalyi - 1990 - Flow The Psychology of Optimal Experience.pdf:/Users/eleanorrow/Zotero/storage/ERJCT7NC/Csikszentmihalyi - 1990 - Flow The Psychology of Optimal Experience.pdf:application/pdf},
}

@article{gaverDesigningHomoLudens2002,
	title = {Designing for {Homo} {Ludens}},
	volume = {12},
	journal = {I3 Magazine},
	author = {Gaver, William},
	month = jan,
	year = {2002},
	file = {Gaver - 2002 - Designing for Homo Ludens.pdf:/Users/eleanorrow/Zotero/storage/KSGNNFNJ/Gaver - 2002 - Designing for Homo Ludens.pdf:application/pdf},
}

@article{zappiDesignUseHackable2014,
	title = {Design and use of a hackable digital instrument},
	url = {https://qmro.qmul.ac.uk/xmlui/handle/123456789/7204},
	abstract = {This paper introduces the D-Box, a new digital musical instrument specifically designed to elicit unexpected creative uses and to support modification by the performer. Rather than taking a modular approach, the D-Box is a hackable instrument which allows for the discovery of novel working configurations through circuit bending techniques. Starting from the concept of appropriation, this paper describes the design, development and evaluation process lasting more than one year and made in collaboration with musicians and hackers.},
	language = {en},
	urldate = {2022-07-05},
	author = {Zappi, V. and Mcpherson, A. and Interfaces, International Conference on Live},
	year = {2014},
	annote = {Accepted: 2015-03-30T09:02:09Z},
	file = {zappi_et_al_2014_design_and_use_of_a_hackable_digital.pdf:/Users/eleanorrow/Zotero/storage/I33I5TCW/zappi_et_al_2014_design_and_use_of_a_hackable_digital.pdf:application/pdf},
}

@incollection{corbettIntelligentTutoringSystems1997,
	address = {Amsterdam},
	title = {Intelligent {Tutoring} {Systems}},
	isbn = {978-0-444-81862-1},
	url = {https://www.sciencedirect.com/science/article/pii/B9780444818621501035},
	abstract = {The goal of intelligent tutoring systems (ITSs) would be to engage the students in sustained reasoning activity and to interact with the student based on a deep understanding of the students' behavior. This chapter begins by providing an overview of intelligent tutoring systems. The chapter comments on competing research goals in the field, followed by descriptions of two successful systems: (1) the Pittsburgh urban math project algebra tutor project and (2) the SHERLOCK project. These two systems are being deployed in real-world educational environments with substantial success. This chapter briefly describes the underlying theory and implementation of each system, to motivate later discussions. A description of the standard components in an intelligent tutoring system is also presented along with a discussion of human-computer interaction assessment issues unique to educational software. This chapter also provides a prescription for ITS design and development methods. Several issues concerning ITS design principles are also addressed in the chapter.},
	language = {en},
	urldate = {2022-07-05},
	booktitle = {Handbook of {Human}-{Computer} {Interaction} ({Second} {Edition})},
	publisher = {North-Holland},
	author = {Corbett, Albert T. and Koedinger, Kenneth R. and Anderson, John R.},
	editor = {Helander, Marting G. and Landauer, Thomas K. and Prabhu, Prasad V.},
	month = jan,
	year = {1997},
	doi = {10.1016/B978-044481862-1.50103-5},
	pages = {849--874},
	file = {Corbett and Koedinger - Chapter 37 Intelligent Tutoring Systems.pdf:/Users/eleanorrow/Zotero/storage/F8LBYQVF/Corbett and Koedinger - Chapter 37 Intelligent Tutoring Systems.pdf:application/pdf},
}

@misc{bryan-kinnsExploringXAIArts2022,
	title = {Exploring {XAI} for the {Arts}: {Explaining} {Latent} {Space} in {Generative} {Music}},
	shorttitle = {Exploring {XAI} for the {Arts}},
	url = {https://qmro.qmul.ac.uk/xmlui/handle/123456789/77565},
	abstract = {Explainable AI has the potential to support more interactive and ﬂuid co-creative AI systems which can creatively collaborate with people. To do this, creative AI models need to be amenable to debugging by offering eXplainable AI (XAI) features which are inspectable, understandable, and modiﬁable. However, currently there is very little XAI for the arts. In this work, we demonstrate how a latent variable model for music generation can be made more explainable; speciﬁcally we extend MeasureVAE which generates measures of music. We increase the explainability of the model by: i) using latent space regularisation to force some speciﬁc dimensions of the latent space to map to meaningful musical attributes, ii) providing a user interface feedback loop to allow people to adjust dimensions of the latent space and observe the results of these changes in real-time, iii) providing a visualisation of the musical attributes in the latent space to help people understand and predict the effect of changes to latent space dimensions. We suggest that in doing so we bridge the gap between the latent space and the generated musical outcomes in a meaningful way which makes the model and its outputs more explainable and more debuggable.},
	language = {en},
	urldate = {2022-07-05},
	author = {Bryan-Kinns, N. and Banar, B. and Ford, C. and Reed, C. and Zhang, Y. and Colton, S. and Armitage, J. and Workshop, NeurIPS 2021-eXplainable AI Approaches for Debugging {and} Diagnosis},
	month = dec,
	year = {2022},
	annote = {Accepted: 2022-03-25T10:15:27Z},
	file = {Bryan-Kinns et al. - Exploring XAI for the Arts Explaining Latent Spac.pdf:/Users/eleanorrow/Zotero/storage/LAVFA7NY/Bryan-Kinns et al. - Exploring XAI for the Arts Explaining Latent Spac.pdf:application/pdf},
}

@article{pearceBodenCreativeMind,
	title = {Boden and {Beyond}: {The} {Creative} {Mind} and its {Reception} in the {Academic} {Community}.},
	language = {en},
	author = {Pearce, Marcus},
	pages = {21},
	file = {Pearce - Boden and Beyond The Creative Mind and its Recept.pdf:/Users/eleanorrow/Zotero/storage/9MG4QT85/Pearce - Boden and Beyond The Creative Mind and its Recept.pdf:application/pdf},
}

@inproceedings{bertin-mahieuxMillionSongDataset2011,
	title = {The {Million} {Song} {Dataset}},
	booktitle = {In {Proceedings} of the 12th {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR} 2011)},
	author = {Bertin-Mahieux, Thierry and Ellis, Daniel P.W. and Whitman, Brian and Lamere, Paul},
	year = {2011},
	annote = {tex.owner: thierry tex.timestamp: 2010.03.07},
	annote = {tex.owner: thierry tex.timestamp: 2010.03.07},
}

@misc{dodmanDorico2016,
	title = {Dorico},
	abstract = {Scorewriting software},
	publisher = {Steinberg},
	author = {Dodman, Andrew and Eastwood, Michael and Fuhrmann, Stefan and Kéri, András and Larcombe, James and Walmsley, Paul and Westlake, Graham},
	year = {2016},
}

@misc{finnSibelius1993,
	title = {Sibelius},
	publisher = {Avid},
	author = {Finn, Ben and Finn, Jonathan},
	year = {1993},
}

@article{rouncefieldFieldworkEthnographyEthnomethodology,
	title = {Fieldwork, {Ethnography} and {Ethnomethodology}},
	abstract = {Driven by the ’failure’ of systems that manifestly did not meet the needs of their users, ﬁeldwork is an approach to the study of work where an observer engages directly with work in its own environment, with a view to understanding the ’real’ processes, activities and interactions of the people involved. Ethnography is an observational approach that examines work as it is practised in a naturalistic setting and ethnomethodology is an approach to analysis that gives precedence to the actors their ways of structuring work rather than attempting to analyse this using some theoretical framework.},
	language = {en},
	author = {Rounceﬁeld, Mark},
	pages = {5},
	file = {Rounceﬁeld - Fieldwork, Ethnography and Ethnomethodology.pdf:/Users/eleanorrow/Zotero/storage/7SGW337X/Rounceﬁeld - Fieldwork, Ethnography and Ethnomethodology.pdf:application/pdf},
}

@article{luChordGANSymbolicMusic2021,
	title = {{ChordGAN}: {Symbolic} {Music} {Style} {Transfer} with {Chroma} {Feature} {Extraction}},
	abstract = {We propose ChordGAN, a generative adversarial network that transfers the style elements of music genres. ChordGAN seeks to learn the rendering of harmonic structures into notes by embedding chroma feature extraction within the training process. In notated music, the chroma representation approximates chord notation as it only takes into account the pitch class of musical notes, representing multiple notes collectively as a density of pitches over a short time period. Chroma is used in this work to distinguish critical style features from content features and improve the consistency of transfer. ChordGAN uses conditional GAN architecture and appropriate loss functions, paralleling image-to-image translation algorithms. In the paper, pop, jazz, and classical datasets were used for training and transfer purposes. To evaluate the success of the transfer, two metrics were used: Tonnetz distance, to measure harmonic similarity, and a separate genre classiﬁer, to measure the transfer style ﬁdelity. Given its success under these metrics, ChordGAN can be utilized as a tool for musicians to study compositional techniques for diﬀerent styles using same chords and automatically generate music from lead sheets.},
	language = {en},
	author = {Lu, Conan and Dubnov, Shlomo},
	year = {2021},
	pages = {9},
	file = {Lu and Dubnov - ChordGAN Symbolic Music Style Transfer with Chrom.pdf:/Users/eleanorrow/Zotero/storage/2P8KSUKH/Lu and Dubnov - ChordGAN Symbolic Music Style Transfer with Chrom.pdf:application/pdf},
}

@inproceedings{robertsHierarchicalLatentVector2018,
	title = {A {Hierarchical} {Latent} {Vector} {Model} for {Learning} {Long}-{Term} {Structure} in {Music}},
	url = {https://proceedings.mlr.press/v80/roberts18a.html},
	abstract = {The Variational Autoencoder (VAE) has proven to be an effective model for producing semantically meaningful latent representations for natural data. However, it has thus far seen limited application to sequential data, and, as we demonstrate, existing recurrent VAE models have difficulty modeling sequences with long-term structure. To address this issue, we propose the use of a hierarchical decoder, which first outputs embeddings for subsequences of the input and then uses these embeddings to generate each subsequence independently. This structure encourages the model to utilize its latent code, thereby avoiding the "posterior collapse" problem which remains an issue for recurrent VAEs. We apply this architecture to modeling sequences of musical notes and find that it exhibits dramatically better sampling, interpolation, and reconstruction performance than a "flat" baseline model. An implementation of our "MusicVAE" is available online at https://goo.gl/magenta/musicvae-code.},
	language = {en},
	urldate = {2022-11-24},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Roberts, Adam and Engel, Jesse and Raffel, Colin and Hawthorne, Curtis and Eck, Douglas},
	month = jul,
	year = {2018},
	pages = {4364--4373},
	annote = {ISSN: 2640-3498},
	file = {Roberts et al. - A Hierarchical Latent Vector Model for Learning Lo.pdf:/Users/eleanorrow/Zotero/storage/ACSC67W3/Roberts et al. - A Hierarchical Latent Vector Model for Learning Lo.pdf:application/pdf;Roberts et al. - A Hierarchical Latent Vector Model for Learning Lo.pdf:/Users/eleanorrow/Zotero/storage/9WS4XKRV/Roberts et al. - A Hierarchical Latent Vector Model for Learning Lo.pdf:application/pdf},
}

@misc{boulanger-lewandowskiModelingTemporalDependencies2012a,
	title = {Modeling {Temporal} {Dependencies} in {High}-{Dimensional} {Sequences}: {Application} to {Polyphonic} {Music} {Generation} and {Transcription}},
	url = {https://ui.adsabs.harvard.edu/abs/2012arXiv1206.6392B},
	abstract = {We investigate the problem of modeling symbolic sequences of polyphonic music in a completely general piano-roll representation. We introduce a probabilistic model based on distribution estimators conditioned on a recurrent neural network that is able to discover temporal dependencies in high-dimensional sequences. Our approach outperforms many traditional models of polyphonic music on a variety of realistic datasets. We show how our musical language model can serve as a symbolic prior to improve the accuracy of polyphonic transcription. \${\textless}\$P /\${\textgreater}\$},
	author = {Boulanger-Lewandowski, Nicolas and Bengio, Yoshua and Vincent, Pascal},
	year = {2012},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, On paper, Statistics - Machine Learning},
	annote = {Pages: arXiv:1206.6392 Publication Title: arXiv e-prints},
	file = {Boulanger-Lewandowski et al. - Modeling Temporal Dependencies in High-Dimensional.pdf:/Users/eleanorrow/Zotero/storage/ACD8AH35/Boulanger-Lewandowski et al. - Modeling Temporal Dependencies in High-Dimensional.pdf:application/pdf},
}

@inproceedings{naskarTextEmbellishmentUsing2019a,
	title = {Text {Embellishment} {Using} {Attention} {Based} {Encoder}-{Decoder} {Model}},
	abstract = {Text embellishment is a natural language generation problem that aims to enhance the lexical and syntactic complexity of a text. i.e., for a given sentence, the goal is to generate a sentence that is lexically and syntactically complex while retaining the same semantic information and meaning. In contrast to text simpliﬁcation (Wang et al., 2016), text embellishment is considered to be a more complex problem as it requires linguistic expertise, and therefore are difﬁcult to be shared across different platforms and domain. In this paper, we have explored this problem through the light of neural machine translation and text simpliﬁcation. Instead of using a standard sequential encoder-decoder network, we propose to improve text embellishment with the Transformer model. The proposed model yields superior performance in terms of lexical and syntactic embellishment and demonstrates broad applicability and effectiveness. We also introduce a language and domain agnostic evaluation set up speciﬁcally for the task of embellishment that can be used to test different embellishment algorithms.},
	booktitle = {Proceedings of the 4th {Workshop} on {Computational} {Creativity} in {Language} {Generation}},
	author = {Naskar, Subhajit and Saha, Soumya and Mukherjee, Sreeparna},
	year = {2019},
	pages = {28--38},
	file = {Naskar et al. - Text Embellishment using Attention Based Encoder-D.pdf:/Users/eleanorrow/Zotero/storage/A4HNEGUV/Naskar et al. - Text Embellishment using Attention Based Encoder-D.pdf:application/pdf},
}

@misc{herremansTensionRibbonsQuantifying2016a,
	title = {Tension {Ribbons}: {Quantifying} and {Visualising} {Tonal} {Tension}},
	shorttitle = {Tension {Ribbons}},
	abstract = {Tension is a complex multidimensional concept that is not easily quantified. This research proposes three methods for quantifying aspects of tonal tension based on the spiral array, a model for tonality. The cloud diameter measures the dispersion of clusters of notes in tonal space; the cloud momentum measures the movement of pitch sets in the spiral array; finally, tensile strain measures the distance between the local and global tonal context. The three methods are implemented in a system that displays the results as tension ribbons over the music score to allow for ease of interpretation. All three methods are extensively tested on data ranging from small snippets to phrases with the Tris-tan chord and larger sections from Beethoven and Schubert piano sonatas. They are further compared to results from an existing empirical experiment.},
	author = {Herremans, Dorien and Chew, Elaine},
	month = jun,
	year = {2016},
	keywords = {metric, tonal tension},
	file = {Herremans and Chew - Tension ribbons Quantifying and visualising tonal.pdf:/Users/eleanorrow/Zotero/storage/SAA7KKN7/Herremans and Chew - Tension ribbons Quantifying and visualising tonal.pdf:application/pdf},
}

@article{hoeflerSparsityDeepLearning2021a,
	title = {Sparsity in {Deep} {Learning}: {Pruning} and {Growth} for {Efficient} {Inference} and {Training} in {Neural} {Networks}},
	volume = {22},
	issn = {15337928},
	shorttitle = {Sparsity in {Deep} {Learning}},
	abstract = {The growing energy and performance costs of deep learning have driven the community to reduce the size of neural networks by selectively pruning components. Similarly to their biological counterparts, sparse networks generalize just as well, sometimes even better than, the original dense networks. Sparsity promises to reduce the memory footprint of regular networks to fit mobile devices, as well as shorten training time for ever growing networks. In this paper, we survey prior work on sparsity in deep learning and provide an extensive tutorial of sparsification for both inference and training. We describe approaches to remove and add elements of neural networks, different training strategies to achieve model sparsity, and mechanisms to exploit sparsity in practice. Our work distills ideas from more than 300 research papers and provides guidance to practitioners who wish to utilize sparsity today, as well as to researchers whose goal is to push the frontier forward. We include the necessary background on mathematical methods in sparsification, describe phenomena such as early structure adaptation, the intricate relations between sparsity and the training process, and show techniques for achieving acceleration on real hardware. We also define a metric of pruned parameter efficiency that could serve as a baseline for comparison of different sparse networks. We close by speculating on how sparsity can improve future workloads and outline major open problems in the field.},
	journal = {Journal of Machine Learning Research},
	author = {Hoefler, Torsten and Alistarh, Dan and Ben-Nun, Tal and Dryden, Nikoli and Peste, Alexandra},
	year = {2021},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Hardware Architecture, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Deep learning, Generalization, Low memory, Performance, Sparsity},
	annote = {Comment: 90 pages, 26 figures},
	file = {Hoefler et al. - 2021 - Sparsity in Deep Learning Pruning and growth for .pdf:/Users/eleanorrow/Zotero/storage/9R47XJFN/Hoefler et al. - 2021 - Sparsity in Deep Learning Pruning and growth for .pdf:application/pdf},
}

@inproceedings{fedorovSpArSeSparseArchitecture2019a,
	title = {{SpArSe}: {Sparse} {Architecture} {Search} for {CNNs} on {Resource}-{Constrained} {Microcontrollers}},
	volume = {32},
	abstract = {The vast majority of processors in the world are actually microcontroller units (MCUs), which find widespread use performing simple control tasks in applications ranging from automobiles to medical devices and office equipment. The Internet of Things (IoT) promises to inject machine learning into many of these every-day objects via tiny, cheap MCUs. However, these resource-impoverished hardware platforms severely limit the complexity of machine learning models that can be deployed. For example, although convolutional neural networks (CNNs) achieve state-of-the-art results on many visual recognition tasks, CNN inference on MCUs is challenging due to severe memory limitations. To circumvent the memory challenge associated with CNNs, various alternatives have been proposed that do fit within the memory budget of an MCU, albeit at the cost of prediction accuracy. This paper challenges the idea that CNNs are not suitable for deployment on MCUs. We demonstrate that it is possible to automatically design CNNs which generalize well, while also being small enough to fit onto memory-limited MCUs. Our Sparse Architecture Search method combines neural architecture search with pruning in a single, unified approach, which learns superior models on four popular IoT datasets. The CNNs we find are more accurate and up to 7.4× smaller than previous approaches, while meeting the strict MCU working memory constraint.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Fedorov, Igor and Adams, Ryan P. and Mattina, Matthew and Whatmough, Paul N.},
	year = {2019},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	pages = {1--26},
	file = {Fedorov et al. - 2019 - SpArSe Sparse Architecture Search for CNNs on Res.pdf:/Users/eleanorrow/Zotero/storage/KLDQ52H2/Fedorov et al. - 2019 - SpArSe Sparse Architecture Search for CNNs on Res.pdf:application/pdf},
}

@article{amabileSocialPsychologyCreativity1982a,
	title = {Social {Psychology} of {Creativity}: {A} {Consensual} {Assessment} {Technique}},
	volume = {43},
	issn = {00223514},
	doi = {10.1037/0022-3514.43.5.997},
	abstract = {States that both the popular creativity tests, such as the Torrance Tests of Creative Thinking, and the subjective assessment techniques used in some previous creativity studies are ill-suited to social psychological studies of creativity. A consensual definition of creativity is presented, and as a refinement of previous subjective methods, a reliable subjective assessment technique based on that definition is described. The results of 8 studies testing the methodology in elementary school and undergraduate populations in both artistic and verbal domains are presented, and the advantages and limitations of this technique are discussed. The present methodology can be useful for the development of a social psychology of creativity because of the nature of the tasks employed and the creativity assessments obtained. Creativity assessment is discussed in terms of the divergent aims and methods of personality psychology and social psychology. (46 ref) (PsycINFO Database Record (c) 2006 APA, all rights reserved). © 1982 American Psychological Association.},
	number = {5},
	journal = {Journal of Personality and Social Psychology},
	author = {Amabile, Teresa M.},
	year = {1982},
	keywords = {college students, consensual social psychological definition of crea, Creativity, development of subjective measurement technique, e},
	pages = {997--1013},
	file = {Amabile - 1982 - Social Psychology of Creativity A Consensual Asse.pdf:/Users/eleanorrow/Zotero/storage/QAH4C4BQ/Amabile - 1982 - Social Psychology of Creativity A Consensual Asse.pdf:application/pdf},
}

@inproceedings{yehSemanticImageInpainting2017a,
	title = {Semantic {Image} {Inpainting} with {Deep} {Generative} {Models}},
	volume = {2017-Janua},
	doi = {10.1109/CVPR.2017.728},
	abstract = {Semantic image inpainting is a challenging task where large missing regions have to be filled based on the available visual data. Existing methods which extract information from only a single image generally produce unsatisfactory results due to the lack of high level context. In this paper, we propose a novel method for semantic image inpainting, which generates the missing content by conditioning on the available data. Given a trained generative model, we search for the closest encoding of the corrupted image in the latent image manifold using our context and prior losses. This encoding is then passed through the generative model to infer the missing content. In our method, inference is possible irrespective of how the missing content is structured, while the state-of-the-art learning based method requires specific information about the holes in the training phase. Experiments on three datasets show that our method successfully predicts information in large missing regions and achieves pixel-level photorealism, significantly outperforming the state-of-the-art methods.},
	booktitle = {Proceedings - 30th {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}, {CVPR} 2017},
	author = {Yeh, Raymond A. and Chen, Chen and Yian Lim, Teck and Schwing, Alexander G. and Hasegawa-Johnson, Mark and Do, Minh N.},
	year = {2017},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {6882--6890},
	annote = {ISBN: 9781538604571},
	file = {Yeh et al. - 2017 - Semantic Image Inpainting with Deep Generative Mod.pdf:/Users/eleanorrow/Zotero/storage/D8MD4FNV/Yeh et al. - 2017 - Semantic Image Inpainting with Deep Generative Mod.pdf:application/pdf},
}

@inproceedings{shawSelfAttentionRelativePosition2018,
	title = {Self-{Attention} with {Relative} {Position} {Representations}},
	volume = {2},
	doi = {10.18653/v1/n18-2074},
	abstract = {Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graphlabeled inputs.},
	booktitle = {{NAACL} {HLT} 2018 - 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} - {Proceedings} of the {Conference}},
	author = {Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
	year = {2018},
	keywords = {Computer Science - Computation and Language},
	pages = {464--468},
	annote = {Comment: NAACL 2018},
	annote = {ISBN: 9781948087292},
	file = {Shaw et al. - 2018 - Self-Attention with Relative Position Representati.pdf:/Users/eleanorrow/Zotero/storage/IXHBPKGZ/Shaw et al. - 2018 - Self-Attention with Relative Position Representati.pdf:application/pdf},
}

@misc{yuScalingAutoregressiveModelsa,
	title = {Scaling {Autoregressive} {Models} for {Content}-{Rich} {Text}-to-{Image} {Generation}},
	author = {Yu, Jiahui and Xu, Yuanzhong and Koh, Jing Yu and Luong, Thang and Baid, Gunjan and Wang, Zirui and Vasudevan, Vijay and Ku, Alexander},
	file = {Yu et al. - Scaling Autoregressive Models for Content-Rich Tex.pdf:/Users/eleanorrow/Zotero/storage/B8T7HKGB/Yu et al. - Scaling Autoregressive Models for Content-Rich Tex.pdf:application/pdf},
}

@inproceedings{mullerSaarlandMusicData2011a,
	title = {Saarland {Music} {Data} ({SMD})},
	url = {2011_MuellerKonzBoglerArifi_SaarlandMusicData_ISMIR-LateBreaking.pdf},
	abstract = {Saarland Music Data provides audio recordings along with perfectly synchronized MIDI files for various piano pieces. The pieces were performed by students of the Hochschule für Musik Saar on a hybrid acoustic/digital piano (Yamaha Disklavier). The Disklavier allows for capturing key and pedal movements of the piano while playing. This information, which can be stored in a MIDI file, yields an accurate annotation of the corresponding audio recording in form of a symbolic description of all played musical note events. The SMD MIDI-Audio pairs constitute a valuable dataset for various music analysis tasks such as music transcription, performance analysis, music synchronization, audio alignment, or source separation.},
	author = {Müller, Meinard and Konz, Verena and Bogler, Wolfgang and Arifi-Müller, Vlora},
	year = {2011},
	file = {Bogler - Hochschule fu¨r Musik Saar.pdf:/Users/eleanorrow/Zotero/storage/35DG5WTS/Bogler - Hochschule fu¨r Musik Saar.pdf:application/pdf},
}

@article{hoppeQgraphBoundedQLearningStabilizing2020,
	title = {Qgraph-{Bounded} q-{Learning}: {Stabilizing} {Model}-{Free} off-{Policy} {Deep} {Reinforcement} {Learning}},
	issn = {23318422},
	abstract = {In state of the art model-free off-policy deep reinforcement learning, a replay memory is used to store past experience and derive all network updates. Even if both state and action spaces are continuous, the replay memory only holds a finite number of transitions. We represent these transitions in a data graph and link its structure to soft divergence. By selecting a subgraph with a favorable structure, we construct a simplified Markov Decision Process for which exact Q-values can be computed efficiently as more data comes in. The subgraph and its associated Q-values can be represented as a QGRAPH. We show that the Q-value for each transition in the simplified MDP is a lower bound of the Q-value for the same transition in the original continuous Q-learning problem. By using these lower bounds in temporal difference learning, our method QG-DDPG is less prone to soft divergence and exhibits increased sample efficiency while being more robust to hyperparameters. QGRAPHs also retain information from transitions that have already been overwritten in the replay memory, which can decrease the algorithm's sensitivity to the replay memory capacity.},
	journal = {arXiv},
	author = {Hoppe, Sabrina and Toussaint, Marc},
	year = {2020},
	keywords = {Markov, Models},
}

@article{mullerParticipatoryDesign1993a,
	title = {Participatory {Design}},
	volume = {36},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/153571.255960},
	doi = {10.1145/153571.255960},
	number = {6},
	urldate = {2022-01-08},
	journal = {Communications of the ACM},
	author = {Muller, Michael J. and Kuhn, Sarah},
	month = jun,
	year = {1993},
	keywords = {Participatory Design, Theory},
	pages = {24--28},
	file = {Muller and Kuhn - Participatory design.pdf:/Users/eleanorrow/Zotero/storage/LN7TU629/Muller and Kuhn - Participatory design.pdf:application/pdf},
}

@article{droit-voletMusicEmotionTime2013a,
	title = {Music, {Emotion}, and {Time} {Perception}: {The} {Influence} of {Subjective} {Emotional} {Valence} and {Arousal}?},
	volume = {4},
	issn = {1664-1078},
	shorttitle = {Music, {Emotion}, and {Time} {Perception}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3713348/},
	doi = {10.3389/fpsyg.2013.00417},
	abstract = {The present study used a temporal bisection task with short ({\textless}2 s) and long ({\textgreater}2 s) stimulus durations to investigate the effect on time estimation of several musical parameters associated with emotional changes in affective valence and arousal. In order to manipulate the positive and negative valence of music, Experiments 1 and 2 contrasted the effect of musical structure with pieces played normally and backwards, which were judged to be pleasant and unpleasant, respectively. This effect of valence was combined with a subjective arousal effect by changing the tempo of the musical pieces (fast vs. slow) (Experiment 1) or their instrumentation (orchestral vs. piano pieces). The musical pieces were indeed judged more arousing with a fast than with a slow tempo and with an orchestral than with a piano timbre. In Experiment 3, affective valence was also tested by contrasting the effect of tonal (pleasant) vs. atonal (unpleasant) versions of the same musical pieces. The results showed that the effect of tempo in music, associated with a subjective arousal effect, was the major factor that produced time distortions with time being judged longer for fast than for slow tempi. When the tempo was held constant, no signiﬁcant effect of timbre on the time judgment was found although the orchestral music was judged to be more arousing than the piano music. Nevertheless, emotional valence did modulate the tempo effect on time perception, the pleasant music being judged shorter than the unpleasant music.},
	urldate = {2022-07-29},
	journal = {Frontiers in Psychology},
	author = {Droit-Volet, Sylvie and Ramos, Danilo and Bueno, José L. O. and Bigand, Emmanuel},
	month = jul,
	year = {2013},
	note = {tex.pmcid: PMC3713348},
	pages = {417},
	annote = {\_eprint: 23882233 \_eprinttype: pmid},
	file = {Droit-Volet et al. - 2013 - Music, emotion, and time perception the influence.pdf:/Users/eleanorrow/Zotero/storage/VR7BWWFD/Droit-Volet et al. - 2013 - Music, emotion, and time perception the influence.pdf:application/pdf},
}

@inproceedings{chenMusicSketchnetControllable2020a,
	title = {Music {Sketchnet}: {Controllable} {Music} {Generation} via {Factorized} {Representations} of {Pitch} and {Rhythm}},
	url = {https://github.com/RetroCirce/Music-SketchNet.},
	abstract = {Drawing an analogy with automatic image completion systems, we propose Music SketchNet, a neural network framework that allows users to specify partial musical ideas guiding automatic music generation. We focus on generating the missing measures in incomplete mono-phonic musical pieces, conditioned on surrounding context , and optionally guided by user-specified pitch and rhythm snippets. First, we introduce SketchVAE, a novel variational autoencoder that explicitly factorizes rhythm and pitch contour to form the basis of our proposed model. Then we introduce two discriminative architec-tures, SketchInpainter and SketchConnector, that in conjunction perform the guided music completion, filling in representations for the missing measures conditioned on surrounding context and user-specified snippets. We evaluate SketchNet on a standard dataset of Irish folk music and compare with models from recent works. When used for music completion, our approach outperforms the state-of-the-art both in terms of objective metrics and subjective listening tests. Finally, we demonstrate that our model can successfully incorporate user-specified snippets during the generation process.},
	booktitle = {Proceedings of the 21st {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Chen, Ke and Wang, Cheng-I and Berg-Kirkpatrick, Taylor and Dubnov, Shlomo},
	year = {2020},
	keywords = {Computer Science - Machine Learning, Computer Science - Multimedia, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning, VAE},
	annote = {Comment: 8 pages, 8 figures, Proceedings of the 21st International Society for Music Information Retrieval Conference, ISMIR 2020},
	annote = {event-place: Montréal, Candada},
	file = {Chen et al. - 2020 - Music SketchNet Controllable Music Generation via.pdf:/Users/eleanorrow/Zotero/storage/XK3SLUAC/Chen et al. - 2020 - Music SketchNet Controllable Music Generation via.pdf:application/pdf},
}

@misc{tanMusicFaderNetsControllable2020a,
	title = {Music {FaderNets}: {Controllable} {Music} {Generation} {Based} {On} {High}-{Level} {Features} via {Low}-{Level} {Feature} {Modelling}},
	shorttitle = {Music {FaderNets}},
	abstract = {High-level musical qualities (such as emotion) are often abstract, subjective, and hard to quantify. Given these difficulties, it is not easy to learn good feature representations with supervised learning techniques, either because of the insufficiency of labels, or the subjectiveness (and hence large variance) in human-annotated labels. In this paper, we present a framework that can learn high-level feature representations with a limited amount of data, by first modelling their corresponding quantifiable low-level attributes. We refer to our proposed framework as Music FaderNets, which is inspired by the fact that low-level attributes can be continuously manipulated by separate "sliding faders" through feature disentanglement and latent regularization techniques. High-level features are then inferred from the low-level representations through semi-supervised clustering using Gaussian Mixture Variational Autoencoders (GM-VAEs). Using arousal as an example of a high-level feature, we show that the "faders" of our model are disentangled and change linearly w.r.t. the modelled low-level attributes of the generated output music. Furthermore, we demonstrate that the model successfully learns the intrinsic relationship between arousal and its corresponding low-level attributes (rhythm and note density), with only 1\% of the training set being labelled. Finally, using the learnt high-level feature representations, we explore the application of our framework in style transfer tasks across different arousal states. The effectiveness of this approach is verified through a subjective listening test.},
	author = {Tan, Hao and Herremans, Dorien},
	month = aug,
	year = {2020},
	annote = {low level attributes, which are straightforward to quantify: rhythm pitch harmony high-level: emotion style genre this paper infers high-level attributes from low-level attributes Look up music fadernets dataset!},
	file = {Tan and Herremans - MUSIC FADERNETS CONTROLLABLE MUSIC GENERATION BAS.pdf:/Users/eleanorrow/Zotero/storage/9F9ZPDA8/Tan and Herremans - MUSIC FADERNETS CONTROLLABLE MUSIC GENERATION BAS.pdf:application/pdf},
}

@inproceedings{dongMuseganMultitrackSequential2018a,
	title = {Musegan: {Multi}-track {Sequential} {Generative} {Adversarial} {Networks} for {Symbolic} {Music} {Generation} and {Accompaniment}},
	abstract = {Generating music has a few notable differences from generating images and videos. First, music is an art of time, necessitating a temporal model. Second, music is usually composed of multiple instruments/tracks with their own temporal dynamics, but collectively they unfold over time interdependently. Lastly, musical notes are often grouped into chords, arpeggios or melodies in polyphonic music, and thereby introducing a chronological ordering of notes is not naturally suitable. In this paper, we propose three models for symbolic multi-track music generation under the framework of generative adversarial networks (GANs). The three models, which differ in the underlying assumptions and accordingly the network architectures, are referred to as the jamming model, the composer model and the hybrid model. We trained the proposed models on a dataset of over one hundred thousand bars of rock music and applied them to generate piano-rolls of five tracks: bass, drums, guitar, piano and strings. A few intra-track and inter-track objective metrics are also proposed to evaluate the generative results, in addition to a subjective user study. We show that our models can generate coherent music of four bars right from scratch (i.e. without human inputs). We also extend our models to human-AI cooperative music generation: given a specific track composed by human, we can generate four additional tracks to accompany it. All code, the dataset and the rendered audio samples are available at https://salu133445.github.io/musegan/.},
	booktitle = {32nd {AAAI} {Conference} on {Artificial} {Intelligence}, {AAAI} 2018},
	author = {Dong, Hao Wen and Hsiao, Wen Yi and Yang, Li Chia and Yang, Yi Hsuan},
	year = {2018},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	pages = {34--41},
	annote = {Comment: to appear at AAAI 2018},
	annote = {Comment: to appear at AAAI 2018},
	annote = {Comment: to appear at AAAI 2018},
	annote = {ISBN: 9781577358008},
	file = {Dong et al. - 2017 - MuseGAN Multi-track Sequential Generative Adversa.pdf:/Users/eleanorrow/Zotero/storage/WYTQ3CPV/Dong et al. - 2017 - MuseGAN Multi-track Sequential Generative Adversa.pdf:application/pdf},
}

@article{giraldoModelingEmbellishmentTiming2012a,
	title = {Modeling {Embellishment}, {Timing} and {Energy} {Expressive} {Transformations} in {Jazz} {Guitar}},
	url = {http://mtg.upf.es/system/files/publications/MML12_Giraldo_Ramirez.pdf},
	abstract = {Professional musicians manipulate sound properties such as timing, energy, pitch and timbre in order to add expression to their performances. However, there is little quantitative information about how and in which context this manipulation occurs. This is particularly true in Jazz music where learning to play expressively is mostly acquired intuitively. We propose to develop a machine learning approach to investigate expressive music performance in Jazz guitar music. We extract symbolic features from audio performances and apply machine learning techniques to induce expressive computational models for embellishment, timing, and energy transformations. Finally, we apply concatenative synthesis techniques in order to generate expressive performances of new scores using the learnt computational models.},
	language = {en},
	journal = {Proc. of the 5th International Workshop on Machine {\textbackslash}textbackslash ldots},
	author = {Giraldo, S and Ramirez, R},
	year = {2012},
	keywords = {Audio, Embellishment, Models, Overpainting},
	file = {Giraldo and Ramirez - Modeling Embellishment, Timing and Energy Expressi.pdf:/Users/eleanorrow/Zotero/storage/BEIVWV95/Giraldo and Ramirez - Modeling Embellishment, Timing and Energy Expressi.pdf:application/pdf},
}

@inproceedings{yangMidinetConvolutionalGenerative2017a,
	title = {Midinet: {A} {Convolutional} {Generative} {Adversarial} {Network} for {Symbolic}-{Domain} {Music} {Generation}},
	abstract = {Most existing neural network models for music generation use recurrent neural networks. However, the recent WaveNet model proposed by DeepMind shows that convolutional neural networks (CNNs) can also generate realistic musical waveforms in the audio domain. Following this light, we investigate using CNNs for generating melody (a series of MIDI notes) one bar after another in the symbolic domain. In addition to the generator, we use a discriminator to learn the distributions of melodies, making it a generative adversarial network (GAN). Moreover, we propose a novel conditional mechanism to exploit available prior knowledge, so that the model can generate melodies either from scratch, by following a chord sequence, or by conditioning on the melody of previous bars (e.g. a priming melody), among other possibilities. The resulting model, named MidiNet, can be expanded to generate music with multiple MIDI channels (i.e. tracks). We conduct a user study to compare the melody of eight-bar long generated by MidiNet and by Google's MelodyRNN models, each time using the same priming melody. Result shows that MidiNet performs comparably with MelodyRNN models in being realistic and pleasant to listen to, yet MidiNet's melodies are reported to be much more interesting.},
	booktitle = {Proceedings of the 18th {International} {Society} for {Music} {Information} {Retrieval} {Conference}, {ISMIR} 2017},
	author = {Yang, Li Chia and Chou, Szu Yu and Yang, Yi Hsuan},
	year = {2017},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Sound},
	pages = {324--331},
	annote = {Comment: 8 pages, Accepted to ISMIR (International Society of Music Information Retrieval) Conference 2017},
	annote = {ISBN: 9789811151798},
	file = {Yang et al. - 2017 - MidiNet A Convolutional Generative Adversarial Ne.pdf:/Users/eleanorrow/Zotero/storage/SGIUB2VN/Yang et al. - 2017 - MidiNet A Convolutional Generative Adversarial Ne.pdf:application/pdf},
}

@misc{dinculescuMidiMePersonalizingMusicVAE2019a,
	title = {{MidiMe}: {Personalizing} a {MusicVAE} {Model} with {User} {Data}},
	abstract = {We introduce an approach to quickly train a small personalized model to control a larger pretrained latent variable model. To show its application for creative interactions, we implement the model in TensorFlow.js as a standalone application, so that training happens in real-time, in a browser, closest to the user.},
	language = {en},
	author = {Dinculescu, Monica and Engel, Jesse and Roberts, Adam},
	year = {2019},
	keywords = {Generative Music, Magenta, MidiMe, Models, MusicVAE, VAE},
	file = {Dinculescu et al. - MidiMe Personalizing a MusicVAE model with user d.pdf:/Users/eleanorrow/Zotero/storage/GCJKYVCR/Dinculescu et al. - MidiMe Personalizing a MusicVAE model with user d.pdf:application/pdf},
}

@inproceedings{brunnerMiDIVAEModelingDynamics2018a,
	title = {{MiDI}-{VAE}: {Modeling} {Dynamics} and {Instrumentation} of {Music} with {Applications} to {Style} {Transfer}},
	doi = {10.5281/zenodo.1492525},
	abstract = {We introduce MIDI-VAE, a neural network model based on Variational Autoencoders that is capable of handling polyphonic music with multiple instrument tracks, as well as modeling the dynamics of music by incorporating note durations and velocities. We show that MIDI-VAE can perform style transfer on symbolic music by automatically changing pitches, dynamics and instruments of a music piece from, e.g., a Classical to a Jazz style. We evaluate the efficacy of the style transfer by training separate style validation classifiers. Our model can also interpolate between short pieces of music, produce medleys and create mixtures of entire songs. The interpolations smoothly change pitches, dynamics and instrumentation to create a harmonic bridge between two music pieces. To the best of our knowledge, this work represents the first successful attempt at applying neural style transfer to complete musical compositions.},
	booktitle = {Proceedings of the 19th {International} {Society} for {Music} {Information} {Retrieval} {Conference}, {ISMIR} 2018},
	author = {Brunner, Gino and Konrad, Andres and Wang, Yuyi and Wattenhofer, Roger},
	year = {2018},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Generative Music, H.5.5, I.2.1, I.2.4, I.2.6, MIDI, MIDI-VAE, Models, Statistics - Machine Learning, VAE},
	pages = {747--754},
	annote = {Comment: Paper accepted at the 19th International Society for Music Information Retrieval Conference, ISMIR 2018, Paris, France},
	annote = {ISBN: 9782954035123},
	file = {Brunner et al. - 2018 - MIDI-VAE Modeling Dynamics and Instrumentation of.pdf:/Users/eleanorrow/Zotero/storage/IJGIZAEZ/Brunner et al. - 2018 - MIDI-VAE Modeling Dynamics and Instrumentation of.pdf:application/pdf},
}

@inproceedings{nashManhattanEnduserProgramming2014a,
	title = {Manhattan: {End}-user {Programming} for {Music}},
	abstract = {This paper explores the concept of end-user programming languages in music composition, and introduces the Manhattan system, which integrates formulas with a grid-based style of music sequencer. Following the paradigm of spreadsheets, an established model of end-user programming, Manhattan is designed to bridge the gap between traditional music editing methods (such as MIDI sequencing and typesetting) and generative and algorithmic music – seeking both to reduce the learning threshold of programming and support flexible integration of static and dynamic musical elements in a single work.},
	author = {Nash, Chris},
	year = {2014},
	file = {Nash - Manhattan End-User Programming for Music.pdf:/Users/eleanorrow/Zotero/storage/8TJLS76X/Nash - Manhattan End-User Programming for Music.pdf:application/pdf},
}

@article{raffelLearningBasedMethodsComparinga,
	title = {Learning-{Based} {Methods} for {Comparing} {Sequences}, with {Applications} to {Audio}-to-{MIDI} {Alignment} and {Matching}},
	author = {Raffel, Colin},
	pages = {222},
	file = {Raffel - Learning-Based Methods for Comparing Sequences, wi.pdf:/Users/eleanorrow/Zotero/storage/64XFNIUB/Raffel - Learning-Based Methods for Comparing Sequences, wi.pdf:application/pdf},
}

@misc{radfordLearningTransferableVisual2019a,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	url = {https://github.com/openai/CLIP},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmark-ing on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.},
	journal = {OpenAI},
	author = {Radford, Alec and Wook, Jong and Chris, Kim and Aditya, Hallacy and Gabriel, Ramesh and Sandhini, Goh and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	year = {2019},
	keywords = {Automatic Searching, CLIP, Collaboration, Contrastive Predictive Coding, Generative Art, OpenAI, Prompt Engineering, Search Algorithms},
	file = {Radford et al. - Learning Transferable Visual Models From Natural L.pdf:/Users/eleanorrow/Zotero/storage/66NHF74V/Radford et al. - Learning Transferable Visual Models From Natural L.pdf:application/pdf},
}

@inproceedings{patiLearningTraverseLatent2019a,
	title = {Learning {To} {Traverse} {Latent} {Spaces} {For} {Musical} {Score} {Inpainting}},
	isbn = {1907.01164v1},
	abstract = {Music Inpainting is the task of filling in missing or lost information in a piece of music. We investigate this task from an interactive music creation perspective. To this end, a novel deep learning-based approach for musical score inpainting is proposed. The designed model takes both past and future musical context into account and is capable of suggesting ways to connect them in a musically meaningful manner. To achieve this, we leverage the representational power of the latent space of a Variational Auto-Encoder and train a Recurrent Neural Network which learns to traverse this latent space conditioned on the past and future musical contexts. Consequently, the designed model is capable of generating several measures of music to connect two musical excerpts. The capabilities and performance of the model are showcased by comparison with competitive baselines using several objective and subjective evaluation methods. The results show that the model generates meaningful in-paintings and can be used in interactive music creation applications. Overall, the method demonstrates the merit of learning complex trajectories in the latent spaces of deep generative models.},
	booktitle = {Proceedings of the 20th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Pati, Ashis and Lerch, Alexander and Hadjeres, Gaëtan},
	year = {2019},
	keywords = {Inpainting, Music, RNN, VAE},
	annote = {event-place: Delft},
	file = {Pati et al. - LEARNING TO TRAVERSE LATENT SPACES FOR MUSICAL SCO.pdf:/Users/eleanorrow/Zotero/storage/FS36EDCN/Pati et al. - LEARNING TO TRAVERSE LATENT SPACES FOR MUSICAL SCO.pdf:application/pdf},
}

@inproceedings{robertsLearningLatentRepresentations2018a,
	title = {Learning {Latent} {Representations} of {Music} to {Generate} {Interactive} {Musical} {Palettes}},
	abstract = {Advances in machine learning have the potential to radically reshape interactions between humans and computers. Deep learning makes it possible to discover powerful representations that are capable of capturing the latent structure of highdimensional data such as music. By creating interactive latent space “palettes” of musical sequences and timbres, we demonstrate interfaces for musical creation made possible by machine learning. We introduce an interface to the intuitive, low-dimensional control spaces for high-dimensional note sequences, allowing users to explore a compositional space of melodies or drum beats in a simple 2-D grid. Furthermore, users can deﬁne 1-D trajectories in the 2-D space for autonomous, continuous morphing during improvisation. Similarly for timbre, our interface to a learned latent space of audio provides an intuitive and smooth search space for morphing between the timbres of different instruments. We remove technical and computational barriers by embedding pre-trained networks into a browser-based GPU-accelerated framework, making the systems accessible to a wide range of users while maintaining potential for creative ﬂexibility and personalization.},
	booktitle = {{IUI} {Workshops}},
	author = {Roberts, Adam and Engel, Jesse H and Oore, Sageev and Eck, Douglas},
	year = {2018},
	file = {Roberts et al. - Learning Latent Representations of Music to Genera.pdf:/Users/eleanorrow/Zotero/storage/ETJHU5YQ/Roberts et al. - Learning Latent Representations of Music to Genera.pdf:application/pdf},
}

@article{gulrajaniImprovedTrainingWasserstein2017a,
	title = {Improved {Training} of {Wasserstein} {GANs}},
	volume = {2017-Decem},
	issn = {10495258},
	abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only poor samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models with continuous generators. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.},
	language = {en},
	journal = {Advances in Neural Information Processing Systems},
	author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
	year = {2017},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {5768--5778},
	annote = {Comment: NIPS camera-ready},
	file = {Gulrajani et al. - 2017 - Improved Training of Wasserstein GANs.pdf:/Users/eleanorrow/Zotero/storage/S5J55JMA/Gulrajani et al. - 2017 - Improved Training of Wasserstein GANs.pdf:application/pdf},
}

@article{sohnImprovedDeepMetric2016a,
	title = {Improved {Deep} {Metric} {Learning} with {Multi}-{Class} {N}-pair {Loss} {Objective}},
	issn = {10495258},
	abstract = {Deep metric learning has gained much popularity in recent years, following the success of deep learning. However, existing frameworks of deep metric learning based on contrastive loss and triplet loss often suffer from slow convergence, partially because they employ only one negative example while not interacting with the other negative classes in each update. In this paper, we propose to address this problem with a new metric learning objective called multi-class N-pair loss. The proposed objective function firstly generalizes triplet loss by allowing joint comparison among more than one negative examples - more specifically, N-1 negative examples - and secondly reduces the computational burden of evaluating deep embedding vectors via an efficient batch construction strategy using only N pairs of examples, instead of (N+1)×N. We demonstrate the superiority of our proposed loss to the triplet loss as well as other competing loss functions for a variety of tasks on several visual recognition benchmark, including fine-grained object recognition and verification, image clustering and retrieval, and face verification and identification.},
	number = {Nips},
	journal = {Advances in Neural Information Processing Systems},
	author = {Sohn, Kihyuk},
	year = {2016},
	pages = {1857--1865},
	file = {Sohn - Improved Deep Metric Learning with Multi-class N-p.pdf:/Users/eleanorrow/Zotero/storage/T5YP6NDK/Sohn - Improved Deep Metric Learning with Multi-class N-p.pdf:application/pdf},
}

@article{tangImprovedAlgorithmsMusic2009a,
	title = {Improved {Algorithms} of {Music} {Information} {Retrieval} {Based} on {Audio} {Fingerprint}},
	doi = {10.1109/IITAW.2009.110},
	abstract = {There are two important research topics in the field of Music Information Retrieval (MIR). One is how to improve the robustness of features and the other is how to speed up the retrieval process. This paper improved the algorithms which proposed by Shazam company in these two aspects. We improve the robustness of the system by a new audio finger-printing extraction using computer graphics, and the system can recognize the recordings which get in complex environment accurately. On the other hand, we propose a recursive search algorithm based on the confidence measure to improve the retrieval speed. Quantitative analysis of the opposite experiment verifies the improvement in the retrieval speed and accuracy. © 2009 IEEE.},
	journal = {3rd International Symposium on Intelligent Information Technology Application Workshops, IITAW 2009},
	author = {Tang, Jie and Liu, Gang and Guo, Jun},
	year = {2009},
	keywords = {Audio fingerprinting, Computer vision, Recursive retrieval},
	pages = {367--371},
	annote = {ISBN: 9780769538600},
}

@inproceedings{isolaImagetoImageTranslationConditional2017,
	title = {Image-to-{Image} {Translation} with {Conditional} {Adversarial} {Networks}},
	volume = {2017-Janua},
	doi = {10.1109/CVPR.2017.632},
	abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pix2pix software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
	booktitle = {Proceedings - 30th {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}, {CVPR} 2017},
	author = {Isola, Phillip and Zhu, Jun Yan and Zhou, Tinghui and Efros, Alexei A.},
	year = {2017},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {5967--5976},
	annote = {Comment: Website: https://phillipi.github.io/pix2pix/, CVPR 2017},
	annote = {ISBN: 9781538604571},
	file = {Isola et al. - 2018 - Image-to-Image Translation with Conditional Advers.pdf:/Users/eleanorrow/Zotero/storage/TRSM2EEP/Isola et al. - 2018 - Image-to-Image Translation with Conditional Advers.pdf:application/pdf},
}

@inproceedings{yangHighResolutionImageInpainting2017,
	title = {High-{Resolution} {Image} {Inpainting} {Using} {Multi}-{Scale} {Neural} {Patch} {Synthesis}},
	volume = {2017-Janua},
	doi = {10.1109/CVPR.2017.434},
	abstract = {Recent advances in deep learning have shown exciting promise in filling large holes in natural images with semantically plausible and context aware details, impacting fundamental image manipulation tasks such as object removal. While these learning-based methods are significantly more effective in capturing high-level features than prior techniques, they can only handle very lowresolution inputs due to memory limitations and difficulty in training. Even for slightly larger images, the inpainted regions would appear blurry and unpleasant boundaries become visible. We propose a multi-scale neural patch synthesis approach based on joint optimization of image content and texture constraints, which not only preserves contextual structures but also produces high-frequency details by matching and adapting patches with the most similar mid-layer feature correlations of a deep classification network. We evaluate our method on the ImageNet and Paris Streetview datasets and achieved state-of-theart inpainting accuracy. We show our approach produces sharper and more coherent results than prior methods, especially for high-resolution images.},
	booktitle = {Proceedings - 30th {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}, {CVPR} 2017},
	author = {Yang, Chao and Lu, Xin and Lin, Zhe and Shechtman, Eli and Wang, Oliver and Li, Hao},
	year = {2017},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {4076--4084},
	annote = {ISBN: 9781538604571},
	file = {Yang et al. - 2017 - High-Resolution Image Inpainting using Multi-Scale.pdf:/Users/eleanorrow/Zotero/storage/DDNMB28C/Yang et al. - 2017 - High-Resolution Image Inpainting using Multi-Scale.pdf:application/pdf},
}

@misc{songGenerativeModelingEstimating2019a,
	title = {Generative {Modeling} by {Estimating} {Gradients} of the {Data} {Distribution}},
	abstract = {We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.},
	author = {Song, Yang and Ermon, Stefano},
	year = {2019},
	file = {Song and Ermon - Generative Modeling by Estimating Gradients of the.pdf:/Users/eleanorrow/Zotero/storage/S2W4J78Y/Song and Ermon - Generative Modeling by Estimating Gradients of the.pdf:application/pdf},
}

@misc{kotechaGeneratingMusicUsing2018a,
	title = {Generating {Music} {Using} an {LSTM} {Network}},
	url = {https://ui.adsabs.harvard.edu/abs/2018arXiv180407300K},
	abstract = {A model of music needs to have the ability to recall past details and have a clear, coherent understanding of musical structure. Detailed in the paper is a neural network architecture that predicts and generates polyphonic music aligned with musical rules. The probabilistic model presented is a Bi-axial LSTM trained with a kernel reminiscent of a convolutional kernel. When analyzed quantitatively and qualitatively, this approach performs well in composing polyphonic music. Link to the code is provided. \${\textless}\$P /\${\textgreater}\$},
	author = {Kotecha, Nikhil and Young, Paul},
	year = {2018},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio, On paper},
	annote = {Pages: arXiv:1804.07300 Publication Title: arXiv e-prints},
	file = {kotecha_et_al_2018_generating_music_using_an_lstm_network.pdf:/Users/eleanorrow/Zotero/storage/EXF3MVDF/kotecha_et_al_2018_generating_music_using_an_lstm_network.pdf:application/pdf},
}

@article{wooFutureTrendsHumanmachine2020a,
	title = {Future {Trends} in {I}\&{M}: {Human}-machine {Co}-{Creation} in the {Rise} of {AI}},
	volume = {23},
	issn = {1941-0123},
	shorttitle = {Future {Trends} in {I}\&{M}},
	doi = {10.1109/MIM.2020.9062691},
	abstract = {The emergence of Artificial Intelligence (AI) is creating new dimensions and redefining the concept and meaning of work in industrial settings. Documented success has been reported where AI is transforming industrial scenes such as scaling large operation processes, speed of execution, flexibility of processes where rigid manufacturing by dumb robots is replaced with smart individualized production following real-time customer choices, decision-making in which a huge amount of data can be quickly available at the fingertips of workers on the factory floor or even prevent problems before they happen, and personalization where AI uses data to deliver personalized user experience. According to the market research firm Trac tica, the global AI software market is expected to experience massive growth in the coming years, with revenues increasing from around US \$9.5 billion in 2018 to an expected US \$118.6 billion by 2025.},
	number = {2},
	journal = {IEEE Instrumentation \& Measurement Magazine},
	author = {Woo, Wai Lok},
	month = apr,
	year = {2020},
	keywords = {Artificial intelligence, Man-machine systems, Market research, Robots, Task analysis},
	pages = {71--73},
	annote = {Conference Name: IEEE Instrumentation \& Measurement Magazine},
	file = {woo_2020_future_trends_in_i&m.pdf:/Users/eleanorrow/Zotero/storage/BX6RCPWG/woo_2020_future_trends_in_i&m.pdf:application/pdf},
}

@article{gelineckIdeaRealizationUnderstanding2009a,
	title = {From {Idea} to {Realization} - {Understanding} the {Compositional} {Processes} of {Electronic} {Musicians}},
	abstract = {This paper presents a study of the compositional process of creating electronic music. 18 electronic musicians were interviewed with focus on discussing their compositional approach, how ideas were realized, and how musical tools were utilized throughout the process. Results show that the process changes significantly from the beginning of the compositional process to the end. Freedom and control are not always kyeword for designing successful musical tools. Participants reported that many creative ideas arise by not being fully in control, not being able to predict the outcome, or restricting or deliberately creating challenges for ones-self.},
	language = {en},
	journal = {Proceedings of Audio Mostly 2009 - A Conference on Interaction with Sound},
	author = {Gelineck, Steven and Serafin, Stefania},
	year = {2009},
	pages = {111--115},
	file = {Gelineck and Seraﬁn - From Idea to Realization - Understanding the Compo.pdf:/Users/eleanorrow/Zotero/storage/FDJFQSE3/Gelineck and Seraﬁn - From Idea to Realization - Understanding the Compo.pdf:application/pdf},
}

@inproceedings{marringtonExperiencingMusicalComposition2010a,
	title = {Experiencing {Musical} {Composition} in the {DAW}: {The} {Software} {Interface} as {Mediator} of the {Musical} {Idea}},
	abstract = {My paper discusses the effect of the DAW environment upon student attitudes to musical composition with reference to pedagogical research that I have conducted over the past two years at Leeds College of Music. I focus in particular upon nature of the graphical interfaces provided by certain DAW platforms, considering their relationship with the ‘traditional’ media they are often modeled upon, and their impact upon the conceptualization of musical ideas. Much of the discussion is focused upon the musical thought processes that users of DAWs bring a priori to their chosen platform and how contact with the software both modifies these ideas and impacts upon creative flow. The issues arising from the paper have interesting implications for ideologies of composition teaching per se and aim to raise debate in regard to the special challenge presented by new technologies to received ideas in this area.},
	booktitle = {Proceedings of the 6th {Art} of {Record} {Production} {Conference}},
	author = {Marrington, Mark},
	year = {2010},
	pages = {1},
	file = {Marrington - 2010 - Experiencing musical composition in the DAW the s.pdf:/Users/eleanorrow/Zotero/storage/JS43SGZQ/Marrington - 2010 - Experiencing musical composition in the DAW the s.pdf:application/pdf},
}

@inproceedings{ponsEndtoEndLearningMusic2018,
	title = {End-to-{End} {Learning} for {Music} {Audio} {Tagging} at {Scale}},
	abstract = {The lack of data tends to limit the outcomes of deep learning research, particularly when dealing with end-to-end learning stacks processing raw data such as waveforms. In this study, 1.2M tracks annotated with musical labels are available to train our end-to-end models. This large amount of data allows us to unrestrictedly explore two different design paradigms for music auto-tagging: assumption-free models – using waveforms as input with very small convolutional filters; and models that rely on domain knowledge – log-mel spectrograms with a convolutional neural network designed to learn timbral and temporal features. Our work focuses on studying how these two types of deep architectures perform when datasets of variable size are available for training: the MagnaTagATune (25k songs), the Million Song Dataset (240k songs), and a private dataset of 1.2M songs. Our experiments suggest that music domain assumptions are relevant when not enough training data are available, thus showing how waveform-based models outperform spectrogram-based ones in large-scale data scenarios.},
	language = {en},
	booktitle = {Proceedings of the 19th {International} {Society} for {Music} {Information} {Retrieval} {Conference}, {ISMIR} 2018},
	author = {Pons, Jordi and Nieto, Oriol and Prockup, Matthew and Schmidt, Erik and Ehmann, Andreas and Serra, Xavier},
	year = {2018},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {637--644},
	annote = {Comment: Presented at the Workshop on Machine Learning for Audio Signal Processing (ML4Audio) at NIPS 2017, and in proceedings of the 19th International Society for Music Information Retrieval Conference (ISMIR2018). Code: https://github.com/jordipons/music-audio-tagging-at-scale-models. Demo: http://www.jordipons.me/apps/music-audio-tagging-at-scale-demo/},
	annote = {ISBN: 9782954035123},
	file = {Pons et al. - END-TO-END LEARNING FOR MUSIC AUDIO TAGGING AT SCA.pdf:/Users/eleanorrow/Zotero/storage/TD9ZYS6T/Pons et al. - END-TO-END LEARNING FOR MUSIC AUDIO TAGGING AT SCA.pdf:application/pdf},
}

@article{hawthorneEnablingFactorizedPiano2019a,
	title = {Enabling {Factorized} {Piano} {Music} {Modeling} and {Generation} with the {MAESTRO} {Dataset}},
	url = {https://openreview.net/forum?id=r1lYRjC9F7},
	abstract = {Generating musical audio directly with neural networks is notoriously difﬁcult because it requires coherently modeling structure at many different timescales. Fortunately, most music is also highly structured and can be represented as discrete note events played on musical instruments. Herein, we show that by using notes as an intermediate representation, we can train a suite of models capable of transcribing, composing, and synthesizing audio waveforms with coherent musical structure on timescales spanning six orders of magnitude (∼0.1 ms to ∼100 s), a process we call Wave2Midi2Wave. This large advance in the state of the art is enabled by our release of the new MAESTRO (MIDI and Audio Edited for Synchronous TRacks and Organization) dataset, composed of over 172 hours of virtuosic piano performances captured with ﬁne alignment (≈3 ms) between note labels and audio waveforms. The networks and the dataset together present a promising approach toward creating new expressive and interpretable neural models of music.},
	language = {en},
	journal = {In International Conference on Learning Representations},
	author = {Hawthorne, Curtis and Stasyuk, Andriy and Roberts, Adam and Simon, Ian and Huang, Cheng-Zhi Anna and Dieleman, Sander and Elsen, Erich and Engel, Jesse and Eck, Douglas},
	year = {2019},
	file = {Hawthorne et al. - 2019 - ENABLING FACTORIZED PIANO MUSIC MODELING AND GENER.pdf:/Users/eleanorrow/Zotero/storage/HK7NYPPW/Hawthorne et al. - 2019 - ENABLING FACTORIZED PIANO MUSIC MODELING AND GENER.pdf:application/pdf},
}

@misc{fyansEcologicalConsiderationsParticipatory2012,
	title = {Ecological {Considerations} for {Participatory} {Design} of {DMIs}},
	url = {https://zenodo.org/record/1178257},
	abstract = {A study is presented examining the participatory design of digital musical interactions. The study takes into consideration the entire ecology of digital musical interactions including the designer, performer and spectator. A new instrument is developed through iterative participatory design involving a group of performers. Across the study the evolution of creative practice and skill development in an emerging community of practice is examined and a spectator study addresses the cognition of performance and the perception of skill with the instrument. Observations are presented regarding the cognition of a novel interaction and evolving notions of skill. The design process of digital musical interactions is reflected on focusing on involvement of the spectator in design contexts.},
	language = {en},
	urldate = {2022-03-03},
	publisher = {Zenodo},
	author = {Fyans, A. Cavan and Marquez-Borbon, Adnan and Stapleton, Paul and Gurevich, Michael},
	month = jun,
	year = {2012},
	doi = {10.5281/zenodo.1178257},
	keywords = {Participatory Design},
	annote = {Pages: x-x Place: Ann Arbor, Michigan Publication Title: Proceedings of the International Conference on New Interfaces for Musical Expression},
	file = {Fyans et al. - Ecological considerations for participatory design.pdf:/Users/eleanorrow/Zotero/storage/YTGY8Y8U/Fyans et al. - Ecological considerations for participatory design.pdf:application/pdf},
}

@inproceedings{hadjeresDeepbachSteerableModel2017a,
	title = {Deepbach: {A} {Steerable} {Model} for {Bach} {Chorales} {Generation}},
	isbn = {2640-3498},
	abstract = {This paper introduces DeepBach, a graphical model aimed at modeling polyphonic music and speciﬁcally hymn-like pieces. We claim that, after being trained on the chorale harmonizations by Johann Sebastian Bach, our model is capable of generating highly convincing chorales in the style of Bach. DeepBach’s strength comes from the use of pseudo-Gibbs sampling coupled with an adapted representation of musical data. This is in contrast with many automatic music composition approaches which tend to compose music sequentially. Our model is also steerable in the sense that a user can constrain the generation by imposing positional constraints such as notes, rhythms or cadences in the generated score. We also provide a plugin on top of the MuseScore music editor making the interaction with DeepBach easy to use.},
	language = {en},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}, {ICML}, 2017},
	publisher = {PMLR},
	author = {Hadjeres, Gaëtan and Pachet, François and Nielsen, Frank},
	year = {2017},
	keywords = {RNN},
	pages = {1362--1371},
	file = {Hadjeres et al. - DeepBach a Steerable Model for Bach Chorales Gene.pdf:/Users/eleanorrow/Zotero/storage/KX75N4PR/Hadjeres et al. - DeepBach a Steerable Model for Bach Chorales Gene.pdf:application/pdf;Hadjeres et al. - DeepBach a Steerable Model for Bach Chorales Gene.pdf:/Users/eleanorrow/Zotero/storage/VF8FJQNJ/Hadjeres et al. - DeepBach a Steerable Model for Bach Chorales Gene.pdf:application/pdf},
}

@inproceedings{barkatiConsideringMusicProduction2013a,
	address = {Luxembourg Luxembourg},
	series = {{MEDES} '13},
	title = {Considering {Music} {Production} and {Culture} {Management} as an {Emerging} {Digital} {Ecosystem}},
	isbn = {978-1-4503-2004-7},
	url = {https://dl.acm.org/doi/10.1145/2536146.2536151},
	doi = {10.1145/2536146.2536151},
	abstract = {Digital music production processes trace a great amount of processes and objects. The important flow of these traces calls for a system to support their interpretation. We have studied and developed such a system in the digital music production context – within the Gamelan research project – towards musical object and process reconstitution. We present the results we obtained from combining trace engineering, knowledge modeling and knowledge engineering, based on the differential elaboration of a strongly-committed ontology, standard formats and common knowledge management tools. We conclude by discussing some hypotheses about trace-based knowledge management, digital music preservation and reconstitution, opening on to some considerations about artistic style and digital humanities aspects.},
	language = {en},
	urldate = {2022-08-09},
	booktitle = {Proceedings of the {Fifth} {International} {Conference} on {Management} of {Emergent} {Digital} {EcoSystems}},
	publisher = {Association for Computing Machinery},
	author = {Barkati, Karim and Rousseaux, Francis},
	month = oct,
	year = {2013},
	keywords = {digital humanities, digital music production, digital studio, knowledge management, reconstitution and preservation, style and creativity, trace engineering},
	pages = {45--53},
	annote = {event-place: New York, NY, USA},
	file = {Barkati and Rousseaux - 2013 - Considering music production and culture managemen.pdf:/Users/eleanorrow/Zotero/storage/PPXHAUEL/Barkati and Rousseaux - 2013 - Considering music production and culture managemen.pdf:application/pdf},
}

@article{ooreThisTimeFeeling2020a,
	title = {This {Time} with {Feeling}: {Learning} {Expressive} {Musical} {Performance}},
	volume = {32},
	abstract = {Music generation has generally been focused on either creating scores or interpreting them. We discuss diﬀerences between these two problems and propose that, in fact, it may be valuable to work in the space of direct performance generation: jointly predicting the notes and also their expressive timing and dynamics. We consider the signiﬁcance and qualities of the data set needed for this. Having identiﬁed both a problem domain and characteristics of an appropriate data set, we show an LSTM-based recurrent network model that subjectively performs quite well on this task. Critically, we provide generated examples. We also include feedback from professional composers and musicians about some of these examples.},
	number = {4},
	journal = {Neural Computing and Applications},
	author = {Oore, Sageev and Simon, Ian and Dieleman, Sander and Eck, Douglas and Simonyan, Karen},
	year = {2020},
	pages = {955--967},
	annote = {ISBN: 1433-3058},
	file = {Oore et al. - 2020 - This Time with Feeling Learning Expressive Musica.pdf:/Users/eleanorrow/Zotero/storage/93JQG8W5/Oore et al. - 2020 - This Time with Feeling Learning Expressive Musica.pdf:application/pdf},
}

@article{amabileSocialPsychologyCreativity1983a,
	title = {The {Social} {Psychology} of {Creativity}: {A} {Componential} {Conceptualization}},
	volume = {45},
	issn = {1939-1315},
	shorttitle = {The {Social} {Psychology} of {Creativity}},
	doi = {10.1037/0022-3514.45.2.357},
	abstract = {Considers the definition and assessment of creativity and presents a componential framework for conceptualizing this faculty. Including domain-relevant skills, creativity-relevant skills, and task motivation as a set of necessary and sufficient components of creativity, the framework describes the way in which cognitive abilities, personality characteristics, and social factors might contribute to stages of the creative process. The discussion emphasizes the previously neglected social factors and highlights the contributions that a social psychology of creativity can make to a comprehensive view of creative performance. (99 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	language = {en},
	number = {2},
	journal = {Journal of Personality and Social Psychology},
	author = {Amabile, Teresa M.},
	year = {1983},
	keywords = {Creativity, Social Psychology},
	pages = {357--376},
	annote = {Place: US Publisher: American Psychological Association},
	file = {Amabile - The Social Psychology of Creativity A Componentia.pdf:/Users/eleanorrow/Zotero/storage/HY3UAV36/Amabile - The Social Psychology of Creativity A Componentia.pdf:application/pdf},
}

@incollection{poulosConceptualFoundationsAutoethnography2021a,
	address = {Washington},
	title = {Conceptual {Foundations} of {Autoethnography}.},
	isbn = {978-1-4338-3454-7 978-1-4338-3470-7},
	url = {http://content.apa.org/books/17204-001},
	urldate = {2022-07-04},
	booktitle = {Essentials of {Autoethnography}.},
	publisher = {American Psychological Association},
	author = {Poulos, Christopher N.},
	year = {2021},
	doi = {10.1037/0000222-001},
	pages = {3--17},
	file = {Poulos - 2021 - Conceptual foundations of autoethnography..pdf:/Users/eleanorrow/Zotero/storage/BAH2CIJL/Poulos - 2021 - Conceptual foundations of autoethnography..pdf:application/pdf},
}

@article{ellisAutoethnographyOverview2011a,
	title = {Autoethnography: {An} {Overview}},
	volume = {12},
	issn = {1438-5627},
	shorttitle = {Autoethnography},
	url = {https://www.qualitative-research.net/index.php/fqs/article/view/1589},
	doi = {10.17169/fqs-12.1.1589},
	abstract = {Autoethnography is an approach to research and writing that seeks to describe and systematically analyze personal experience in order to understand cultural experience. This approach challenges canonical ways of doing research and representing others and treats research as a political, socially-just and socially-conscious act. A researcher uses tenets of autobiography and ethnography to do and write autoethnography. Thus, as a method, autoethnography is both process and product. URN: http://nbn-resolving.de/urn:nbn:de:0114-fqs1101108},
	number = {1},
	urldate = {2022-07-04},
	journal = {Forum Qualitative Sozialforschung / Forum: Qualitative Social Research},
	author = {Ellis, Carolyn and Adams, Tony E. and Bochner, Arthur P.},
	year = {2011},
	keywords = {autoethnography, co-constructed narratives, ethnography, interactive interviews, narrative, narrative ethnographies, personal narrative, relational ethics},
	file = {ellis_et_al_2011_autoethnography.pdf:/Users/eleanorrow/Zotero/storage/LUD9X6PR/ellis_et_al_2011_autoethnography.pdf:application/pdf},
}

@article{xianZeroShotLearningaComprehensive2019,
	title = {Zero-{Shot} {Learning}-a {Comprehensive} {Evaluation} of the {Good}, the {Bad} and the {Ugly}},
	volume = {41},
	issn = {19393539},
	doi = {10.1109/TPAMI.2018.2857768},
	abstract = {Due to the importance of zero-shot learning, i.e., classifying images where there is a lack of labeled training data, the number of proposed approaches has recently increased steadily. We argue that it is time to take a step back and to analyze the status quo of the area. The purpose of this paper is three-fold. First, given the fact that there is no agreed upon zero-shot learning benchmark, we first define a new benchmark by unifying both the evaluation protocols and data splits of publicly available datasets used for this task. This is an important contribution as published results are often not comparable and sometimes even flawed due to, e.g., pre-training on zero-shot test classes. Moreover, we propose a new zero-shot learning dataset, the Animals with Attributes 2 (AWA2) dataset which we make publicly available both in terms of image features and the images themselves. Second, we compare and analyze a significant number of the state-of-the-art methods in depth, both in the classic zero-shot setting but also in the more realistic generalized zero-shot setting. Finally, we discuss in detail the limitations of the current status of the area which can be taken as a basis for advancing it.},
	number = {9},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Xian, Yongqin and Lampert, Christoph H. and Schiele, Bernt and Akata, Zeynep},
	year = {2019},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Generalized zero-shot learning, Image classification, Transductive learning, Weakly-supervised learning},
	pages = {2251--2265},
	annote = {Comment: Accepted by TPAMI in July, 2018. We introduce Proposed Split Version 2.0 (Please download it from our project webpage). arXiv admin note: substantial text overlap with arXiv:1703.04394},
	annote = {\_eprint: 30028691 \_eprinttype: pmid},
	file = {Xian et al. - 2020 - Zero-Shot Learning -- A Comprehensive Evaluation o.pdf:/Users/eleanorrow/Zotero/storage/DUNJ6RIM/Xian et al. - 2020 - Zero-Shot Learning -- A Comprehensive Evaluation o.pdf:application/pdf},
}

@article{knottsSurveyUptakeMusic2020a,
	title = {A {Survey} on the {Uptake} of {Music} {AI} {Software}},
	issn = {2220-4806},
	url = {pdfs/nime2020_paper95.pdf},
	abstract = {The recent proliferation of commercial software claiming ground in the field of music AI has provided opportunity to engage with AI in music making without the need to use libraries aimed at those with programming skills. Pre-packaged music AI software has the potential to broaden access to machine learning tools but it is unclear how widely these softwares are used by music technologists or how engagement affects attitudes towards AI in music making. To interrogate these questions we undertook a survey in October 2019, gaining 117 responses. The survey collected statistical information on the use of pre-packaged and self-written music AI software. Respondents reported a range of musical outputs including producing recordings, live performance and generative work across many genres of music making. The survey also gauged general attitudes towards AI in music and provided an open field for general comments. The responses to the survey suggested a forward-looking attitude to music AI with participants often pointing to the future potential of AI tools, rather than present utility. Optimism was partially related to programming skill with those with more experience showing higher skepticism towards the current state and future potential of AI.},
	journal = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	author = {Knotts, Shelly and Collins, Nick},
	year = {2020},
	keywords = {A Peer Reviewed article presented at the Internati},
	pages = {594--600},
	file = {Knotts and Collins - A survey on the uptake of Music AI Software.pdf:/Users/eleanorrow/Zotero/storage/IKLZTEVX/Knotts and Collins - A survey on the uptake of Music AI Software.pdf:application/pdf},
}

@inproceedings{radfordUnsupervisedRepresentationLearning2016a,
	title = {Unsupervised {Representation} {Learning} with {Deep} {Convolutional} {Generative} {Adversarial} {Networks}},
	abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	booktitle = {4th {International} {Conference} on {Learning} {Representations}, {ICLR} 2016 - {Conference} {Track} {Proceedings}},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	year = {2016},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	pages = {1--16},
	annote = {Comment: Under review as a conference paper at ICLR 2016},
	file = {Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf:/Users/eleanorrow/Zotero/storage/T9K2TE97/Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf:application/pdf},
}

@inproceedings{luoUnsupervisedDisentanglementPitch2020a,
	title = {Unsupervised {Disentanglement} of {Pitch} and {Timbre} for {Isolated} {Musical} {Instrument} {Sounds}},
	abstract = {Disentangling factors of variation aims to uncover latent variables that underlie the process of data generation. In this paper, we propose a framework that achieves unsupervised pitch and timbre disentanglement for isolated musical instrument sounds without relying on data annotations or pre-trained neural networks. Our framework, based on variational auto-encoders, takes as input a spectral frame, and encodes pitch and timbre as categorical and continuous variables, respectively. The input is then reconstructed by combining those variables. Under an unsupervised training setting, a major challenge is that encoders are tasked to capture factors of interest with distinct latent representations, without access to the corresponding ground-truth labels. We therefore introduce auxiliary tasks and objectives which leverage pitch shifting as a strategy to create surrogate labels, thereby encouraging the disentanglement of pitch and timbre. Through an ablation study we analyze the impact of the proposed objectives. The evaluation shows the efficacy of the proposed framework for learning disentangled representations, and verifies its applicability to unsupervised pitch classification and conditional spectral synthesis.},
	booktitle = {{ISMIR} 2020},
	author = {Luo, Yin-Jyun and Cheuk, Kin Wai and Nakano, Tomoyasu and Goto, Masataka and Herremans, Dorien},
	year = {2020},
	keywords = {Domain knowledge, Machine learning/Artificial intelligence for music},
	pages = {1--8},
	file = {Luo et al. - 2020 - UNSUPERVISED DISENTANGLEMENT OF PITCH AND TIMBRE F.pdf:/Users/eleanorrow/Zotero/storage/WRWFNH4I/Luo et al. - 2020 - UNSUPERVISED DISENTANGLEMENT OF PITCH AND TIMBRE F.pdf:application/pdf},
}

@article{donahueLakhNESIMPROVINGMULTIINSTRUMENTAL2019,
	title = {{LakhNES}: {IMPROVING} {MULTI}-{INSTRUMENTAL} {MUSIC} {GENERATION} {WITH} {CROSS}-{DOMAIN} {PRE}-{TRAINING}},
	abstract = {We are interested in the task of generating multiinstrumental music scores. The Transformer architecture has recently shown great promise for the task of piano score generation—here we adapt it to the multiinstrumental setting. Transformers are complex, highdimensional language models which are capable of capturing long-term structure in sequence data, but require large amounts of data to ﬁt. Their success on piano score generation is partially explained by the large volumes of symbolic data readily available for that domain. We leverage the recently-introduced NES-MDB dataset of four-instrument scores from an early video game sound synthesis chip (the NES), which we ﬁnd to be well-suited to training with the Transformer architecture. To further improve the performance of our model, we propose a pre-training technique to leverage the information in a large collection of heterogeneous music, namely the Lakh MIDI dataset. Despite differences between the two corpora, we ﬁnd that this transfer learning procedure improves both quantitative and qualitative performance for our primary task.},
	language = {en},
	author = {Donahue, Chris and Mao, Huanru Henry and Li, Yiting Ethan and Cottrell, Garrison W and McAuley, Julian},
	year = {2019},
	file = {Donahue et al. - 2019 - LakhNES IMPROVING MULTI-INSTRUMENTAL MUSIC GENERA.pdf:/Users/eleanorrow/Zotero/storage/VWYTJ7UG/Donahue et al. - 2019 - LakhNES IMPROVING MULTI-INSTRUMENTAL MUSIC GENERA.pdf:application/pdf},
}

@article{sturmTakingModelsBack2017,
	title = {Taking the {Models} back to {Music} {Practice}: {Evaluating} {Generative} {Transcription} {Models} built using {Deep} {Learning}},
	volume = {2},
	issn = {2399-7656},
	shorttitle = {Taking the {Models} back to {Music} {Practice}},
	url = {https://www.jcms.org.uk/article/id/517/},
	doi = {10.5920/JCMS.2017.09},
	abstract = {We extend our evaluation of generative models of music transcriptions that were first presented in Sturm, Santos, Ben-Tal, and Korshunova (2016). We evaluate the models in five different ways: 1) at the population level, comparing statistics of 30,000 generated transcriptions with those of over 23,000 training transcriptions; 2) at the practice level, examining the ways in which specific generated transcriptions are successful as music compositions; 3) as a “nefarious tester”, seeking the music knowledge limits of the models; 4) in the context of assisted music composition, using the models to create music within the conventions of the training data; and finally, 5) taking the models to real-world music practitioners. Our work attempts to demonstrate new approaches to evaluating the application of machine learning methods to modelling and making music, and the importance of taking the results back to the realm of music practice to judge their usefulness. Our datasets and software are open and available at https://github.com/IraKorshunova/folk-rnn.},
	language = {None},
	number = {1},
	urldate = {2023-07-11},
	journal = {Journal of Creative Music Systems},
	author = {Sturm, Bob L. and Ben-Tal, Oded},
	month = sep,
	year = {2017},
	keywords = {evaluation, music, transcription},
	annote = {Number: 1 Publisher: Huddersfield University Press},
	file = {L. Sturm and Ben-Tal - 2017 - Taking the Models back to Music Practice Evaluati.pdf:/Users/eleanorrow/Zotero/storage/TV2K4MK8/L. Sturm and Ben-Tal - 2017 - Taking the Models back to Music Practice Evaluati.pdf:application/pdf},
}

@article{briotDeepLearningMusic2020,
	title = {Deep learning for music generation: challenges and directions},
	volume = {32},
	issn = {1433-3058},
	url = {https://doi.org/10.1007/s00521-018-3813-6},
	doi = {10.1007/s00521-018-3813-6},
	abstract = {In addition to traditional tasks such as prediction, classification and translation, deep learning is receiving growing attention as an approach for music generation, as witnessed by recent research groups such as Magenta at Google and CTRL (Creator Technology Research Lab) at Spotify. The motivation is in using the capacity of deep learning architectures and training techniques to automatically learn musical styles from arbitrary musical corpora and then to generate samples from the estimated distribution.However, a direct application of deep learning to generate content rapidly reaches limits as the generated content tends to mimic the training set without exhibiting true creativity. Moreover, deep learning architectures do not offer direct ways for controlling generation (e.g., imposing some tonality or other arbitrary constraints). Furthermore, deep learning architectures alone are autistic automata which generate music autonomously without human user interaction, far from the objective of interactively assisting musicians to compose and refine music. Issues such as control, structure, creativity and interactivity are the focus of our analysis. In this paper, we select some limitations of a direct application of deep learning to music generation and analyze why the issues are not fulfilled and how to address them by possible approaches. Various examples of recent systems are cited as examples of promising directions.},
	number = {4},
	journal = {Neural Computing and Applications},
	author = {Briot, Jean-Pierre and Pachet, François},
	month = feb,
	year = {2020},
	pages = {981--993},
	file = {Submitted Version:/Users/eleanorrow/Zotero/storage/VKSGNLTI/Briot and Pachet - 2020 - Deep learning for music generation challenges and.pdf:application/pdf},
}

@inproceedings{mauchApproximateNoteTranscription2010,
	title = {Approximate note transcription for the improved identification of difficult chords},
	abstract = {The automatic detection and transcription of musical chords from audio is an established music computing task. The choice of chord proﬁles and higher-level time-series modelling have received a lot of attention, resulting in methods with an overall performance of more than 70\% in the MIREX Chord Detection task 2009. Research on the front end of chord transcription algorithms has often concentrated on ﬁnding good chord templates to ﬁt the chroma features. In this paper we reverse this approach and seek to ﬁnd chroma features that are more suitable for usage in a musically-motivated model. We do so by performing a prior approximate transcription using an existing technique to solve non-negative least squares problems (NNLS). The resulting NNLS chroma features are tested by using them as an input to an existing state-of-the-art high-level model for chord transcription. We achieve very good results of 80\% accuracy using the song collection and metric of the 2009 MIREX Chord Detection tasks. This is a signiﬁcant increase over the top result (74\%) in MIREX 2009. The nature of some chords makes their identiﬁcation particularly susceptible to confusion between fundamental frequency and partials. We show that the recognition of these diffcult chords in particular is substantially improved by the prior approximate transcription using NNLS.},
	language = {en},
	booktitle = {Proceedings of the 11th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Mauch, Matthias and Dixon, Simon},
	year = {2010},
	file = {Mauch and Dixon - APPROXIMATE NOTE TRANSCRIPTION FOR THE IMPROVED ID.pdf:/Users/eleanorrow/Zotero/storage/EYC6LF3M/Mauch and Dixon - APPROXIMATE NOTE TRANSCRIPTION FOR THE IMPROVED ID.pdf:application/pdf},
}

@article{kongGiantMIDIPianoLargeScaleMIDI2022,
	title = {{GiantMIDI}-{Piano}: {A} {Large}-{Scale} {MIDI} {Dataset} for {Classical} {Piano} {Music}},
	doi = {10.5334/tismir.80},
	abstract = {Symbolic music datasets are important for music information retrieval and musical analysis. However, there is a lack of large-scale symbolic datasets for classical piano music. In this article, we describe the creation of the GiantMIDI-Piano (GP) dataset containing 38,700,838 transcribed notes and 10,855 unique solo piano works composed by 2,786 composers. We extract the names of music works and the names of composers from the International Music Score Library Project (IMSLP). We search and download their corresponding audio recordings from the Internet. We further create a curated subset containing 7,236 works composed by 1,787 composers where the titles of downloaded audio recordings contain the surnames of composers. We apply a convolutional neural network to detect solo piano works. Then, we transcribe those solo piano recordings into Musical Instrument Digital Interface (MIDI) files using a high-resolution piano transcription system. Each transcribed MIDI file contains the onset, offset, pitch, and velocity attributes of piano notes and pedals. GiantMIDI-Piano includes 90\% live performance MIDI files and 10\% sequence input MIDI files. We analyse the statistics of GiantMIDI-Piano and show pitch class, interval, trichord, and tetrachord frequencies of six composers from different eras to show that GiantMIDI-Piano can be used for musical analysis. We evaluate the quality of GiantMIDI-Piano in terms of solo piano detection F1 scores, metadata accuracy, and transcription error rates. We release the source code for acquiring the GiantMIDI-Piano dataset at https://github.com/bytedance/GiantMIDI-Piano.},
	journal = {Transactions of the International Society for Music Information Retrieval},
	author = {Kong, Qiuqiang and Li, Bochen and Chen, Jitong and Wang, Yuxuan},
	year = {2022},
	keywords = {Computer Science - Information Retrieval, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, en\_US},
	annote = {Comment: 11 pages, 13 figures},
	file = {Kong et al. - 2022 - GiantMIDI-Piano A large-scale MIDI dataset for cl.pdf:/Users/eleanorrow/Zotero/storage/Q4P4J6SF/Kong et al. - 2022 - GiantMIDI-Piano A large-scale MIDI dataset for cl.pdf:application/pdf},
}

@misc{patiDMelodiesMusicDataset2020,
	title = {{dMelodies}: {A} {Music} {Dataset} for {Disentanglement} {Learning}},
	shorttitle = {{dMelodies}},
	url = {http://arxiv.org/abs/2007.15067},
	abstract = {Representation learning focused on disentangling the underlying factors of variation in given data has become an important area of research in machine learning. However, most of the studies in this area have relied on datasets from the computer vision domain and thus, have not been readily extended to music. In this paper, we present a new symbolic music dataset that will help researchers working on disentanglement problems demonstrate the efficacy of their algorithms on diverse domains. This will also provide a means for evaluating algorithms specifically designed for music. To this end, we create a dataset comprising of 2-bar monophonic melodies where each melody is the result of a unique combination of nine latent factors that span ordinal, categorical, and binary types. The dataset is large enough (approx. 1.3 million data points) to train and test deep networks for disentanglement learning. In addition, we present benchmarking experiments using popular unsupervised disentanglement algorithms on this dataset and compare the results with those obtained on an image-based dataset.},
	language = {en},
	urldate = {2023-02-21},
	publisher = {arXiv},
	author = {Pati, Ashis and Gururani, Siddharth and Lerch, Alexander},
	month = jul,
	year = {2020},
	doi = {10.48550/arXiv.2007.15067},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, disentanglement learning, Electrical Engineering and Systems Science - Audio and Speech Processing, melody, monophonic, polyphony},
	annote = {arXiv:2007.15067 [cs, eess]},
	annote = {Comment: To be published in: Proceedings of 21st International Society for Music Information Retrieval Conference (ISMIR), Montr{\textbackslash}'eal, Canada, 2020},
	file = {Pati et al. - dMELODIES A MUSIC DATASET FOR DISENTANGLEMENT LEA.pdf:/Users/eleanorrow/Zotero/storage/NZTTC5ES/Pati et al. - dMELODIES A MUSIC DATASET FOR DISENTANGLEMENT LEA.pdf:application/pdf;Pati et al. - dMELODIES A MUSIC DATASET FOR DISENTANGLEMENT LEA.pdf:/Users/eleanorrow/Zotero/storage/N9T5A9XZ/Pati et al. - dMELODIES A MUSIC DATASET FOR DISENTANGLEMENT LEA.pdf:application/pdf},
}

@misc{huangMidiFindFastEffective2013,
	title = {{MidiFind}: {Fast} and {Eﬀective} {Similarity} {Searching} in {Large} {MIDI} databases},
	abstract = {While there are perhaps millions of MIDI ﬁles available over the Internet, it is diﬃcult to ﬁnd performances of a particular piece because well labeled metadata and indexes are unavailable. We address the particular problem of ﬁnding performances of compositions for piano, which is diﬀerent from often-studied problems of Query-by-Humming and Music Fingerprinting. Our MidiFind system is designed to search a million MIDI ﬁles with high precision and recall. By using a hybrid search strategy, it runs more than 1000 times faster than naive competitors, and by using a combination of bag-of-words and enhanced Levenshtein distance methods for similarity, our system achieves a precision of 99.5\% and recall of 89.8\%.},
	language = {en},
	author = {Huang, Tongbo and Xia, Guangyu and Ma, Yifei and Dannenberg, Roger and Faloutsos, Christos},
	year = {2013},
	file = {Huang et al. - 2013 - MidiFind Fast and Eﬀective Similarity Searching i.pdf:/Users/eleanorrow/Zotero/storage/W2Y3P3MA/Huang et al. - 2013 - MidiFind Fast and Eﬀective Similarity Searching i.pdf:application/pdf},
}

@article{huProbabilisticModelMelodic1996,
	title = {A {Probabilistic} {Model} of {Melodic} {Similarity}},
	url = {https://kilthub.cmu.edu/articles/journal_contribution/A_Probabilistic_Model_of_Melodic_Similarity/6591227/1},
	doi = {10.1184/R1/6591227.v1},
	abstract = {Melodic similarity is an important concept for music databases, musicological studies, and interactive music systems. Dynamic programming is commonly used to compare melodies, often with a distance function based on pitch differences measured in semitones. This approach computes an “edit distance” as a measure of melodic dissimilarity. The problem can also be viewed in probabilistic terms: What is the probability that a melody is a “mutation” of another melody, given a table of mutation probabilities? We explain this approach and demonstrate how it can be used to search a database of melodies. Our experiments show that the probabilistic model performs better than a typical “edit distance” comparison.},
	language = {en},
	urldate = {2023-02-21},
	author = {Hu, Ning and Dannenberg, Roger B. and Lewis, Ann L.},
	month = nov,
	year = {1996},
	annote = {Publisher: Carnegie Mellon University},
	file = {Hu et al. - A Probabilistic Model of Melodic Similarity.pdf:/Users/eleanorrow/Zotero/storage/ULPEJHU2/Hu et al. - A Probabilistic Model of Melodic Similarity.pdf:application/pdf},
}

@inproceedings{walshawMultilevelMelodicMatching2015,
	title = {Multilevel melodic matching},
	url = {https://gala.gre.ac.uk/id/eprint/14023/},
	abstract = {This paper describes a multilevel algorithm for matching tunes when performing inexact searches in symbolic mu-sical data. The basis of the algorithm is straightforward: initially each tune in the search database is normalised and quantised and then recursively coarsened, typically by removing weaker off-beats, until the tune is reduced to a skeleton representation with just one note per bar. The same process is applied to the search query and melodic matching between query and data can then take place at every level. The algorithm implemented here uses the longest common substring algorithm at each level, but in principle a variety of similarity measures could be used. The multilevel framework allows inexact matches to occur by identifying similarities at course levels and is also exploited with the use of early termination heuristics at coarser levels, both to reduce computational complexity and to enhance the matching qualitatively. Experimenta-tion demonstrates the effectiveness of the approach for inexact melodic searches within a corpus of tunes.},
	language = {en},
	urldate = {2023-02-21},
	author = {Walshaw, Chris},
	year = {2015},
	note = {Type: Conference Proceedings},
	annote = {Conference Name: Proc. Folk Music Analysis ISBN: 9791095209003 Meeting Name: Proc. Folk Music Analysis Num Pages: 142 Pages: 130-137 Place: Paris Publisher: Association Dirac},
	file = {walshaw_2015_multilevel_melodic_matching.pdf:/Users/eleanorrow/Zotero/storage/3CP6XBBR/walshaw_2015_multilevel_melodic_matching.pdf:application/pdf},
}

@misc{karsdorpLEARNINGSIMILARITYMETRICS2019,
	title = {Learning similarity metrics for melody retrieval},
	abstract = {Similarity measures are indispensable in music information retrieval. In recent years, various proposals have been made for measuring melodic similarity in symbolically encoded scores. Many of these approaches are ultimately based on a dynamic programming approach such as sequence alignment or edit distance, which has various drawbacks. First, the similarity scores are not necessarily metrics and are not directly comparable. Second, the algorithms are mostly ﬁrst-order and of quadratic timecomplexity, and ﬁnally, the features and weights need to be deﬁned precisely. We propose an alternative approach which employs deep neural networks for end-to-end similarity metric learning. We contrast and compare different recurrent neural architectures (LSTM and GRU) for representing symbolic melodies as continuous vectors, and demonstrate how duplet and triplet loss functions can be employed to learn compact distributional representations of symbolic music in an induced melody space. This approach is contrasted with an alignment-based approach. We present results for the Meertens Tune Collections, which consists of a large number of vocal and instrumental monophonic pieces from Dutch musical sources, spanning ﬁve centuries, and demonstrate the robustness of the learned similarity metrics.},
	language = {en},
	author = {Karsdorp, Folgert and van Kranenburg, Peter and Manjavacas, Enrique},
	year = {2019},
	file = {Karsdorp et al. - 2019 - LEARNING SIMILARITY METRICS FOR MELODY RETRIEVAL.pdf:/Users/eleanorrow/Zotero/storage/FC3R46FR/Karsdorp et al. - 2019 - LEARNING SIMILARITY METRICS FOR MELODY RETRIEVAL.pdf:application/pdf},
}

@misc{tanprasertMIDISheetMusicAlignment2019,
	title = {{MIDI}-{Sheet} {Music} {Alignment} {Using} {Bootleg} {Score} {Synthesis}},
	url = {https://zenodo.org/record/3527748},
	abstract = {MIDI-sheet music alignment is the task of finding correspondences between a MIDI representation of a piece and its corresponding sheet music images. Rather than using optical music recognition to bridge the gap between sheet music and MIDI, we explore an alternative approach: projecting the MIDI data into pixel space and performing alignment in the image domain. Our method converts the MIDI data into a crude representation of the score that only contains rectangular floating notehead blobs, a process we call bootleg score synthesis. Furthermore, we project sheet music images into the same bootleg space by applying a deep watershed notehead detector and filling in the bounding boxes around each detected notehead. Finally, we align the bootleg representations using a simple variant of dynamic time warping. On a dataset of 68 real scanned piano scores from IMSLP and corresponding MIDI performances, our method achieves a 97.3\% accuracy at an error tolerance of one second, outperforming several baseline systems that employ optical music recognition.},
	urldate = {2023-02-21},
	author = {Tanprasert, Thitaree and Jenrungrot, Teerapat and Müller, Meinard and Tsai, Timothy},
	month = nov,
	year = {2019},
	doi = {10.5281/zenodo.3527748},
	note = {Place: Delft, The Netherlands},
	annote = {Pages: 91-98 Publication Title: Proceedings of the 20th International Society for Music Information Retrieval Conference Publisher: ISMIR},
	file = {Tanprasert et al. - 2019 - MIDI-Sheet Music Alignment Using Bootleg Score Syn.pdf:/Users/eleanorrow/Zotero/storage/FSC52EG8/Tanprasert et al. - 2019 - MIDI-Sheet Music Alignment Using Bootleg Score Syn.pdf:application/pdf},
}

@article{fosterFILOSAXDATASETANNOTATED2021,
	title = {{FILOSAX}: {A} {DATASET} {OF} {ANNOTATED} {JAZZ} {SAXOPHONE} {RECORDINGS}},
	abstract = {The Filosax dataset is a large collection of specially commissioned recordings of jazz saxophonists playing with commercially available backing tracks. Five participants each recorded themselves playing the melody, interpreting a transcribed solo and improvising on 48 tracks, giving a total of around 24 hours of audio data. The solos are annotated both as individual note events with physical timing, and as sheet music with a metrical interpretation of the timing. In this paper, we outline the criteria used for choosing and sourcing the repertoire, the recording process and the semi-automatic transcription pipeline. We demonstrate the use of the dataset to analyse musical phenomena such as swing timing and dynamics of typical musical ﬁgures, as well as for training a source activity detection system and predicting expressive characteristics. Other potential applications include the modelling of jazz improvisation, performer identiﬁcation, automatic music transcription, source separation and music generation.},
	language = {en},
	author = {Foster, Dave and Dixon, Simon},
	year = {2021},
	file = {Foster and Dixon - 2021 - FILOSAX A DATASET OF ANNOTATED JAZZ SAXOPHONE REC.pdf:/Users/eleanorrow/Zotero/storage/ZPRNGZKM/Foster and Dixon - 2021 - FILOSAX A DATASET OF ANNOTATED JAZZ SAXOPHONE REC.pdf:application/pdf},
}

@misc{emiyaMAPSPianoDatabase,
	title = {{MAPS} - {A} piano database for multipitch estimation and automatic transcription of music},
	abstract = {MAPS – standing for MIDI Aligned Piano Sounds – is a database of MIDI-annotated piano recordings. MAPS has been designed in order to be released in the music information retrieval research community, especially for the development and the evaluation of algorithms for single-pitch or multipitch estimation and automatic transcription of music. It is composed by isolated notes, random-pitch chords, usual musical chords and pieces of music. The database provides a large amount of sounds obtained in various recording conditions.},
	language = {en},
	author = {Emiya, Valentin and Bertin, Nancy and David, Bertrand and Badeau, Roland},
	file = {Emiya et al. - MAPS - A piano database for multipitch estimation .pdf:/Users/eleanorrow/Zotero/storage/RTBEUZDH/Emiya et al. - MAPS - A piano database for multipitch estimation .pdf:application/pdf},
}

@inproceedings{engelNeuralAudioSynthesis2017,
	title = {Neural {Audio} {Synthesis} of {Musical} {Notes} with {WaveNet} {Autoencoders}},
	url = {https://proceedings.mlr.press/v70/engel17a.html},
	abstract = {Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.},
	language = {en},
	urldate = {2023-02-21},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Engel, Jesse and Resnick, Cinjon and Roberts, Adam and Dieleman, Sander and Norouzi, Mohammad and Eck, Douglas and Simonyan, Karen},
	month = jul,
	year = {2017},
	pages = {1068--1077},
	annote = {ISSN: 2640-3498},
	file = {Engel et al. - Neural Audio Synthesis of Musical Notes  with Wave.pdf:/Users/eleanorrow/Zotero/storage/YI9LPQ8A/Engel et al. - Neural Audio Synthesis of Musical Notes  with Wave.pdf:application/pdf},
}

@phdthesis{prpaAttendingInnerSelf2020,
	title = {Attending to inner self: {Designing} and unfolding breath-based {VR} experiences through micro-phenomenology},
	shorttitle = {Attending to inner self},
	url = {https://summit.sfu.ca/item/20693},
	abstract = {This thesis contributes to human-computer interaction (HCI) research with a focus on the design of virtual reality (VR) applications that support and elicit the experience of breath awareness. Within HCI, advocating for technology-supported well-being has resulted in a large body of interactive systems informed by the quantified self paradigm. While these technologies elicit positive health outcomes, they also sometimes reduce access to a greater range of experiences that promote self-regulation and well-being. A growing interest in HCI is moving beyond the quantified self to designing technologies “as experiences” based upon embodied and first-person reflective practices. In this research, we are specifically interested in the experiences that arise through technologies that elicit breath awareness. However, in reviewing prior HCI research in designing for breath awareness, we have found that the differing epistemological commitments and theoretical frameworks determine very different sets of systems’ values, expectations and methods. This is an under-explored design space within HCI that necessitates a deeper understanding of disambiguation of how epistemological commitments shape not only our systems, but our experiences and how we consider methodologies that support the rich and meaningful explication of those experiences. While we contribute primarily to HCI, our work is positioned in the broader intersection of art, science, and technology. We structure our research around two main foci. First focus is on the design and evaluation of VR applications built upon first-person practices of eliciting breath awareness. We engage in disambiguating theoretical underpinnings of the systems that perceptually extend breath awareness to understand how epistemological commitments of different theoretical frameworks inform system design to support breath awareness. Then, we present the iterative process of design and evaluation of two breath-based VR systems: Pulse Breath Water and Respire. Second focus is on methodological strategies that clarify not only fine-grained descriptions of the experience but its very own structure. We have applied micro-phenomenology in HCI to design and evaluate two immersive VR systems for eliciting breath awareness. We contribute to understanding how micro-phenomenology can be used in the context of VR systems for articulating the nuances, complexity, and diversity of a user's experience beyond surface descriptions.},
	language = {en},
	urldate = {2022-12-08},
	author = {Prpa, Mirjana},
	month = aug,
	year = {2020},
	note = {tex.copyright: Copyright is held by the author.},
	annote = {Publisher: Simon Fraser University},
	file = {Prpa - Attending to Inner Self Designing and Unfolding B.pdf:/Users/eleanorrow/Zotero/storage/9Z8PYQJP/Prpa - Attending to Inner Self Designing and Unfolding B.pdf:application/pdf},
}

@article{pachetContinuatorMusicalInteraction2003,
	title = {The {Continuator}: {Musical} {Interaction} {With} {Style}},
	volume = {32},
	shorttitle = {The {Continuator}},
	doi = {10.1076/jnmr.32.3.333.16861},
	journal = {Journal of New Music Research},
	author = {Pachet, Francois},
	year = {2003},
	keywords = {Human-AI Partnerships},
	pages = {333--341},
	file = {pachet_2003_the_continuator.pdf:/Users/eleanorrow/Zotero/storage/HC2BD9EU/pachet_2003_the_continuator.pdf:application/pdf},
}

@misc{frielerTWOWEBAPPLICATIONS2018,
	title = {Two web applications for exploring melodic patterns in jazz solos},
	abstract = {This paper presents two novel user interfaces for investigating the pattern content in monophonic jazz solos and exempliﬁes how these interfaces could be used for research on jazz improvisation. In jazz improvisation, patterns are of particular interest for the analysis of improvisation styles, the oral transmission of musical language, the practice of improvisation, and the psychology of creative processes. The ongoing project “Dig That Lick” is devoted to addressing these questions with the help of a large database of jazz solo transcriptions generated by automated melody extraction algorithms. To expose these transcriptions to jazz researchers, two prototypes of user interfaces were designed that work currently with the 456 manually transcribed jazz solos of the Weimar Jazz Database. The ﬁrst one is a Shiny application that allows exploring a set of 653 of the most common patterns by eminent players. The second one is a web interface for a general two-staged pattern search in the Weimar Jazz Database featuring regular expressions. These applications aim on the one hand at an expert audience of jazz researchers to facilitate generating and testing hypotheses about patterns in jazz improvisation, and on the other hand at a wider audience of jazz teachers, students, and fans.},
	language = {en},
	author = {Frieler, Klaus and Höger, Frank and Pﬂeiderer, Martin and Dixon, Simon},
	year = {2018},
	file = {Frieler et al. - 2018 - TWO WEB APPLICATIONS FOR EXPLORING MELODIC PATTERN.pdf:/Users/eleanorrow/Zotero/storage/N2BBDU37/Frieler et al. - 2018 - TWO WEB APPLICATIONS FOR EXPLORING MELODIC PATTERN.pdf:application/pdf},
}

@inproceedings{couturierDatasetSymbolicTexture2022,
	address = {Bengaluru, India},
	title = {A {Dataset} of {Symbolic} {Texture} {Annotations} in {Mozart} {Piano} {Sonatas}},
	url = {https://hal.archives-ouvertes.fr/hal-03860195},
	abstract = {Musical scores are generally analyzed under different aspects, notably melody, harmony, rhythm, but also through their texture, although this last concept is arguably more delicate to formalize. Symbolic texture depicts how sounding components are organized in the score. It outlines the density of elements, their heterogeneity, role and interactions. In this paper, we release a set of manual annotations for each bar of 9 movements among early piano sonatas by W. A. Mozart, totaling 1164 labels that follow a syntax dedicated to piano score texture. A quantitative analysis of the annotations highlights some characteristic textural features in the corpus. In addition, we present and release the implementation of low-level descriptors of symbolic texture, that are preliminary experimented for textural elements prediction. The annotations and the descriptors offer promising applications in computer-assisted music analysis and composition.},
	urldate = {2022-12-06},
	booktitle = {23rd {Int}. {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR} 2022)},
	author = {Couturier, Louis and Bigo, Louis and Levé, Florence},
	month = dec,
	year = {2022},
	keywords = {dataset, musical texture, piano, symbolic music, symbolic musical features},
	file = {Couturier et al. - A Dataset of Symbolic Texture Annotations in Mozar.pdf:/Users/eleanorrow/Zotero/storage/DUZ6ZF7J/Couturier et al. - A Dataset of Symbolic Texture Annotations in Mozar.pdf:application/pdf},
}

@inproceedings{prpaArticulatingExperienceReflections2020a,
	address = {New York, NY, USA},
	series = {{CHI} '20},
	title = {Articulating {Experience}: {Reflections} from {Experts} {Applying} {Micro}-{Phenomenology} to {Design} {Research} in {HCI}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {Articulating {Experience}},
	url = {https://doi.org/10.1145/3313831.3376664},
	doi = {10.1145/3313831.3376664},
	abstract = {Third wave HCI initiated a slow transformation in the methods of UX research: from widely used quantitative approaches to more recently employed qualitative techniques. Articulating the nuances, complexity, and diversity of a user's experience beyond surface descriptions remains a challenge within design. One qualitative method — micro-phenomenology — has been used in HCI/Design research since 2001. Yet, no systematic understanding of micro-phenomenology has been presented, particularly from the perspective of HCI/Design researchers who actively use it in design contexts. We interviewed 5 HCI/Design experts who utilize micro-phenomenology and present their experiences with the method. We illustrate how this method has been applied by the selected experts through developing a practice, and present conditions under which the descriptions of the experience unfold, and the values that this method can provide to HCI/Design field. Our contribution highlights the value of micro-phenomenology in articulating the experience of designers and participants, developing vocabulary for multi-sensory experiences, and unfolding embodied tacit knowledge.},
	urldate = {2022-11-30},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Prpa, Mirjana and Fdili-Alaoui, Sarah and Schiphorst, Thecla and Pasquier, Philippe},
	month = apr,
	year = {2020},
	keywords = {empirical methods, micro-phenomenology, user experience},
	pages = {1--14},
	file = {Prpa et al. - 2020 - Articulating Experience Reflections from Experts .pdf:/Users/eleanorrow/Zotero/storage/E69NNHH6/Prpa et al. - 2020 - Articulating Experience Reflections from Experts .pdf:application/pdf},
}

@article{brownStyleReproductionComputational,
	title = {Is style reproduction a computational creativity task?},
	abstract = {Is style reproduction a valid computational creativity task? Does producing output ‘in the style of’ an existing creator contribute to computational creativity research? Where is the creativity in imitation or replication of an existing style, and where does style reproduction fall into what has been criticised as ‘pastiche’ rather than credible creative activity? This paper tackles these debates, which have been under-addressed in computational creativity literature. We review the presentaiton of past work in style reproduction, and consider the fit of such work into evolving definitions of computational creativity research. As part of this, we consider style reproduction itself as a creative task, both within and outside computational forms. We discuss various points of interest that emerge in the analysis, such as control in the creative process, intentionality and effort. Our work gives a more objective understanding of the level of creativity present in style generation, and specifically what value it brings to computational creativity research.},
	language = {en},
	author = {Brown, Daniel G and Jordanous, Anna},
	pages = {11},
	file = {Brown and Jordanous - Is style reproduction a computational creativity t.pdf:/Users/eleanorrow/Zotero/storage/UYUVHNUL/Brown and Jordanous - Is style reproduction a computational creativity t.pdf:application/pdf},
}

@article{ensCAEMSICrossDomainAnalytic,
	title = {{CAEMSI} : {A} {Cross}-{Domain} {Analytic} {Evaluation} {Methodology} for {Style} {Imitation}},
	abstract = {We propose CAEMSI, a cross-domain analytic evaluation methodology for Style Imitation (SI) systems, based on a set of statistical signiﬁcance tests that allow hypotheses comparing two corpora to be tested. Typically, SI systems are evaluated using human participants, however, this type of approach has several weaknesses. For humans to provide reliable assessments of an SI system, they must possess a sufﬁcient degree of domain knowledge, which can place signiﬁcant limitations on the pool of participants. Furthermore, both human bias against computer-generated artifacts, and the variability of participants’ assessments call the reliability of the results into question. Most importantly, the use of human participants places limitations on the number of generated artifacts and SI systems which can be feasibly evaluated. Directly motivated by these shortcomings, CAEMSI provides a robust and scalable approach to the evaluation problem. Normalized Compression Distance, a domain-independent distance metric, is used to measure the distance between individual artifacts within a corpus. The difference between corpora is measured using test statistics derived from these inter-artifact distances, and permutation testing is used to determine the signiﬁcance of the difference. We provide empirical evidence validating the statistical significance tests, using datasets from two distinct domains.},
	language = {en},
	author = {Ens, Jeff and Pasquier, Philippe},
	pages = {8},
	file = {Ens and Pasquier - CAEMSI  A Cross-Domain Analytic Evaluation Method.pdf:/Users/eleanorrow/Zotero/storage/9JFRBWML/Ens and Pasquier - CAEMSI  A Cross-Domain Analytic Evaluation Method.pdf:application/pdf},
}

@inproceedings{janssenComparisonSymbolicSimilarity2015,
	title = {A {Comparison} of {Symbolic} {Similarity} {Measures} for {Finding} {Occurrences} of {Melodic} {Segments}},
	abstract = {To ﬁnd occurrences of melodic segments, such as themes, phrases and motifs, in musical works, a well-performing similarity measure is needed to support human analysis of large music corpora. We evaluate the performance of a range of melodic similarity measures to ﬁnd occurrences of phrases in folk song melodies. We compare the similarity measures correlation distance, city-block distance, Euclidean distance and alignment, proposed for melody comparison in computational ethnomusicology; furthermore Implication-Realization structure alignment and B-spline alignment, forming successful approaches in symbolic melodic similarity; moreover, wavelet transform and the geometric approach Structure Induction, having performed well in musical pattern discovery. We evaluate the success of the different similarity measures through observing retrieval success in relation to human annotations. Our results show that local alignment and SIAM perform on an almost equal level to human annotators.},
	author = {Janssen, Berit and Van Kranenburg, Peter and Volk, Anja},
	month = oct,
	year = {2015},
	file = {Janssen et al. - A COMPARISON OF SYMBOLIC SIMILARITY MEASURES FOR F.pdf:/Users/eleanorrow/Zotero/storage/JMG59MS2/Janssen et al. - A COMPARISON OF SYMBOLIC SIMILARITY MEASURES FOR F.pdf:application/pdf},
}

@book{levinejazz,
	title = {The jazz piano book},
	url = {https://books.google.co.uk/books?id=a01bnQEACAAJ},
	publisher = {Sher Music Company},
	author = {Levine, M.},
}

@misc{rameshHierarchicalTextConditionalImage2022,
	title = {Hierarchical {Text}-{Conditional} {Image} {Generation} with {CLIP} {Latents}},
	url = {http://arxiv.org/abs/2204.06125},
	abstract = {Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, ﬁnding that the latter are computationally more efﬁcient and produce higher-quality samples.},
	language = {en},
	urldate = {2024-01-01},
	publisher = {arXiv},
	author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
	month = apr,
	year = {2022},
	note = {arXiv:2204.06125 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Ramesh et al. - 2022 - Hierarchical Text-Conditional Image Generation wit.pdf:/Users/eleanorrow/Zotero/storage/UW5M7W4Q/Ramesh et al. - 2022 - Hierarchical Text-Conditional Image Generation wit.pdf:application/pdf},
}

@incollection{blandfordSemiStructuredQualitativeStudies2013,
	title = {Semi-{Structured} {Qualitative} {Studies}},
	abstract = {Human-Computer Interaction (HCI) addresses problems of interaction design: delivering novel designs, evaluating existing designs, and understanding user needs for future designs. Qualitative methods have an essential role to play in this enterprise, particularly in understanding user needs and behav...},
	author = {Blandford, Ann},
	year = {2013},
	file = {Blandford - 2013 - Semi-Structured Qualitative Studies.pdf:/Users/eleanorrow/Zotero/storage/NH4MG55F/Blandford - 2013 - Semi-Structured Qualitative Studies.pdf:application/pdf},
}

@article{edwardsPiJAMAPianoJazz2023,
	title = {{PiJAMA}: {Piano} {Jazz} with {Automatic} {MIDI} {Annotations}},
	volume = {6},
	issn = {2514-3298},
	shorttitle = {{PiJAMA}},
	url = {http://transactions.ismir.net/articles/10.5334/tismir.162/},
	doi = {10.5334/tismir.162},
	abstract = {Recent advances in automatic piano transcription have enabled large scale analysis of piano music in the symbolic domain. However, the research has largely focused on classical piano music. We present PiJAMA (Piano Jazz with Automatic MIDI Annotations): a dataset of over 200 hours of solo jazz piano performances with automatically transcribed MIDI. In total there are 2,777 unique performances by 120 different pianists across 244 recorded albums. The dataset contains a mixture of studio recordings and live performances. We use automatic audio tagging to identify applause, spoken introductions, and other non-piano audio to facilitate downstream music information retrieval tasks. We explore descriptive statistics of the MIDI data, including pitch histograms and chromaticism. We then demonstrate two experimental benchmarks on the data: performer identification and generative modeling. The dataset, including a link to the associated source code is available at https://almostimplemented.github.io/PiJAMA/.},
	language = {en},
	number = {1},
	urldate = {2024-01-01},
	journal = {Transactions of the International Society for Music Information Retrieval},
	author = {Edwards, Drew and Dixon, Simon and Benetos, Emmanouil},
	month = sep,
	year = {2023},
	pages = {89--102},
	file = {Edwards et al. - 2023 - PiJAMA Piano Jazz with Automatic MIDI Annotations.pdf:/Users/eleanorrow/Zotero/storage/C7JCJ8VT/Edwards et al. - 2023 - PiJAMA Piano Jazz with Automatic MIDI Annotations.pdf:application/pdf},
}

@misc{rowJAZZVARDatasetVariations2023,
	title = {{JAZZVAR}: {A} {Dataset} of {Variations} found within {Solo} {Piano} {Performances} of {Jazz} {Standards} for {Music} {Overpainting}},
	copyright = {All rights reserved},
	shorttitle = {{JAZZVAR}},
	url = {http://arxiv.org/abs/2307.09670},
	abstract = {Jazz pianists often uniquely interpret jazz standards. Passages from these interpretations can be viewed as sections of variation. We manually extracted such variations from solo jazz piano performances. The JAZZVAR dataset is a collection of 502 pairs of Variation and Original MIDI segments. Each Variation in the dataset is accompanied by a corresponding Original segment containing the melody and chords from the original jazz standard. Our approach differs from many existing jazz datasets in the music information retrieval (MIR) community, which often focus on improvisation sections within jazz performances. In this paper, we outline the curation process for obtaining and sorting the repertoire, the pipeline for creating the Original and Variation pairs, and our analysis of the dataset. We also introduce a new generative music task, Music Overpainting, and present a baseline Transformer model trained on the JAZZVAR dataset for this task. Other potential applications of our dataset include expressive performance analysis and performer identification.},
	urldate = {2024-01-01},
	publisher = {arXiv},
	author = {Row, Eleanor and Tang, Jingjing and Fazekas, George},
	month = jul,
	year = {2023},
	note = {arXiv:2307.09670 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: Pre-print accepted for publication at CMMR2023, 12 pages, 4 figures},
	file = {Row et al. - 2023 - JAZZVAR A Dataset of Variations found within Solo.pdf:/Users/eleanorrow/Zotero/storage/WQILLIJU/Row et al. - 2023 - JAZZVAR A Dataset of Variations found within Solo.pdf:application/pdf},
}

@misc{minPolyffusionDiffusionModel2023,
	title = {Polyffusion: {A} {Diffusion} {Model} for {Polyphonic} {Score} {Generation} with {Internal} and {External} {Controls}},
	shorttitle = {Polyffusion},
	url = {http://arxiv.org/abs/2307.10304},
	abstract = {We propose Polyffusion, a diffusion model that generates polyphonic music scores by regarding music as image-like piano roll representations. The model is capable of controllable music generation with two paradigms: internal control and external control. Internal control refers to the process in which users pre-define a part of the music and then let the model infill the rest, similar to the task of masked music generation (or music inpainting). External control conditions the model with external yet related information, such as chord, texture, or other features, via the cross-attention mechanism. We show that by using internal and external controls, Polyffusion unifies a wide range of music creation tasks, including melody generation given accompaniment, accompaniment generation given melody, arbitrary music segment inpainting, and music arrangement given chords or textures. Experimental results show that our model significantly outperforms existing Transformer and sampling-based baselines, and using pre-trained disentangled representations as external conditions yields more effective controls.},
	urldate = {2024-01-01},
	publisher = {arXiv},
	author = {Min, Lejun and Jiang, Junyan and Xia, Gus and Zhao, Jingwei},
	month = jul,
	year = {2023},
	note = {arXiv:2307.10304 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: In Proceedings of the 24th Conference of the International Society for Music Information Retrieval (ISMIR 2023), Milan, Italy},
	file = {Min et al. - 2023 - Polyffusion A Diffusion Model for Polyphonic Scor.pdf:/Users/eleanorrow/Zotero/storage/V42T9PQE/Min et al. - 2023 - Polyffusion A Diffusion Model for Polyphonic Scor.pdf:application/pdf},
}

@inproceedings{sabnisTactileSymbolsContinuous2023,
	address = {Hamburg Germany},
	title = {Tactile {Symbols} with {Continuous} and {Motion}-{Coupled} {Vibration}: {An} {Exploration} of using {Embodied} {Experiences} for {Hermeneutic} {Design}},
	isbn = {978-1-4503-9421-5},
	shorttitle = {Tactile {Symbols} with {Continuous} and {Motion}-{Coupled} {Vibration}},
	url = {https://dl.acm.org/doi/10.1145/3544548.3581356},
	doi = {10.1145/3544548.3581356},
	language = {en},
	urldate = {2024-01-01},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Sabnis, Nihar and Wittchen, Dennis and Vega, Gabriela and Reed, Courtney N. and Strohmeier, Paul},
	month = apr,
	year = {2023},
	pages = {1--19},
	file = {Sabnis et al. - 2023 - Tactile Symbols with Continuous and Motion-Coupled.pdf:/Users/eleanorrow/Zotero/storage/364GNW7L/Sabnis et al. - 2023 - Tactile Symbols with Continuous and Motion-Coupled.pdf:application/pdf},
}

@phdthesis{thompsonTradingFoursMr2023,
	type = {Undergraduate {Thesis}},
	title = {Trading {Fours} with {Mr}. {PC}: {Toward} an {Improved} {Model} for {Jazz} {Synthesis}},
	abstract = {Machine learning (ML) is a demonstrably effective method for learning patternand rule-based systems, such as natural language or music. This thesis identifies jazz
improvisation as one such system and seeks to design an ML model for learning it. To
this end, Mr. PC is an ML model capable of synthesizing novel jazz improvisation in a
swing style. It was constructed using a state-of-the-art Transformer-XL architecture and
trained with the hereunto unexplored Filosax dataset. The Transformer-XL architecture is
a promising solution for inducing long-term structure in generated improvisation: a
persistent challenge in the music generation literature. The quality of the generated
improvisation is evaluated using both traditional music analysis techniques and
quantitative fractal analysis. Further, the output is compared to that of a smaller, simpler
model trained on the same data set to test the margin of improvement.},
	language = {en},
	author = {Thompson, Christopher M.},
	year = {2023},
	keywords = {Jazz Improvisation},
	file = {Thompson - 2023 - Trading Fours with Mr. PC Toward an Improved Mode.pdf:/Users/eleanorrow/Zotero/storage/28493RCY/Thompson - 2023 - Trading Fours with Mr. PC Toward an Improved Mode.pdf:application/pdf},
}

@inproceedings{mamanUnalignedSupervisionAutomatic2022,
	title = {Unaligned {Supervision} for {Automatic} {Music} {Transcription} in {The} {Wild}},
	url = {https://proceedings.mlr.press/v162/maman22a.html},
	abstract = {Multi-instrument Automatic Music Transcription (AMT), or the decoding of a musical recording into semantic musical content, is one of the holy grails of Music Information Retrieval. Current AMT approaches are restricted to piano and (some) guitar recordings, due to difficult data collection. In order to overcome data collection barriers, previous AMT approaches attempt to employ musical scores in the form of a digitized version of the same song or piece. The scores are typically aligned using audio features and strenuous human intervention to generate training labels. We introduce Note𝐸𝑀EM\_\{EM\}, a method for simultaneously training a transcriber and aligning the scores to their corresponding performances, in a fully-automated process. Using this unaligned supervision scheme, complemented by pseudo-labels and pitch shift augmentation, our method enables training on in-the-wild recordings with unprecedented accuracy and instrumental variety. Using only synthetic data and unaligned supervision, we report SOTA note-level accuracy of the MAPS dataset, and large favorable margins on cross-dataset evaluations. We also demonstrate robustness and ease of use; we report comparable results when training on a small, easily obtainable, self-collected dataset, and we propose alternative labeling to the MusicNet dataset, which we show to be more accurate. Our project page is available at https://benadar293.github.io.},
	language = {en},
	urldate = {2024-01-01},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Maman, Ben and Bermano, Amit H.},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {14918--14934},
	file = {Full Text PDF:/Users/eleanorrow/Zotero/storage/A2F3BL35/Maman and Bermano - 2022 - Unaligned Supervision for Automatic Music Transcri.pdf:application/pdf},
}

@misc{UnderstandingCLIPOpenAI2022,
	title = {Understanding {CLIP} by {OpenAI}},
	url = {https://cv-tricks.com/how-to/understanding-clip-by-openai/},
	abstract = {CLIP By OPEN-AI Introduction Nearly all state-of-the-art visual perception algorithms rely on the same formula:  (1) pretrain a convolutional network on a large, manually annotated image classification dataset (2) finetune the network on a smaller, task-specific dataset. This technique has been widely used for several years and has led to impressive improvements on numerous tasks.  …},
	language = {en-US},
	urldate = {2024-01-01},
	journal = {CV-Tricks.com},
	month = may,
	year = {2022},
	note = {Section: How-to},
}

@inproceedings{wigginsSIAESEAlgorithm2002,
	title = {{SIA}({M}){ESE}: {An} {Algorithm} for {Transposition} {Invariant}, {Polyphonic} {Content}-{Based} {Music} {Retrieval}},
	shorttitle = {{SIA}({M}){ESE}},
	url = {https://www.semanticscholar.org/paper/SIA(M)ESE%3A-An-Algorithm-for-Transposition-Music-Wiggins-Lemstr%C3%B6m/e2f5c8aa016b7b2859ac9d2177c9995700d84fac},
	abstract = {In this paper, we study transposition-invariant content-based music retrieval (TI-CBMR) in polyphonic music. The aim is to find transposition invariant occurrences of a given query pattern called a template, in a database of polyphonic music called a dataset. Between the musical events (represented by points) in the dataset that have been found to match points in the template, there may be any finite number of other intervening musical events. For this task, we introduce an algorithm, called SIA(M)ESE, which is based on the SIA pattern induction algorithm [11]. The algorithm is first introduced in abstract mathematical form, then we show how we have implemented it using sophisticated techniques and equipped it with appropriate heuristics. The resulting efficient algorithm has a worst case running time of O(mn log(mn)), where m and n are the size of the template and the dataset, respectively. Moreover, the algorithm is generalizable to any arbitrary, multidimensional translation invariant pattern matching problem, where the events considered can be represented by points in a multidimensional dataset.},
	urldate = {2024-01-01},
	author = {Wiggins, Geraint A. and Lemström, Kjell and Meredith, D.},
	year = {2002},
	annote = {[TLDR] An algorithm, called SIA(M)ESE, which is based on the SIA pattern induction algorithm, is introduced, which has a worst case running time of O(mn log(mn), where m and n are the size of the template and the dataset, respectively.},
	file = {Full Text PDF:/Users/eleanorrow/Zotero/storage/X7AAP2CL/Wiggins et al. - 2002 - SIA(M)ESE An Algorithm for Transposition Invarian.pdf:application/pdf},
}

@misc{toyamaAutomaticPianoTranscription2023a,
	title = {Automatic {Piano} {Transcription} with {Hierarchical} {Frequency}-{Time} {Transformer}},
	url = {http://arxiv.org/abs/2307.04305},
	abstract = {Taking long-term spectral and temporal dependencies into account is essential for automatic piano transcription. This is especially helpful when determining the precise onset and offset for each note in the polyphonic piano content. In this case, we may rely on the capability of self-attention mechanism in Transformers to capture these long-term dependencies in the frequency and time axes. In this work, we propose hFT-Transformer, which is an automatic music transcription method that uses a two-level hierarchical frequency-time Transformer architecture. The first hierarchy includes a convolutional block in the time axis, a Transformer encoder in the frequency axis, and a Transformer decoder that converts the dimension in the frequency axis. The output is then fed into the second hierarchy which consists of another Transformer encoder in the time axis. We evaluated our method with the widely used MAPS and MAESTRO v3.0.0 datasets, and it demonstrated state-of-the-art performance on all the F1-scores of the metrics among Frame, Note, Note with Offset, and Note with Offset and Velocity estimations.},
	language = {en},
	urldate = {2024-01-02},
	publisher = {arXiv},
	author = {Toyama, Keisuke and Akama, Taketo and Ikemiya, Yukara and Takida, Yuhta and Liao, Wei-Hsiang and Mitsufuji, Yuki},
	month = jul,
	year = {2023},
	note = {arXiv:2307.04305 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: 8 pages, 6 figures, to be published in ISMIR2023},
	file = {Full Text:/Users/eleanorrow/Zotero/storage/W4Z45I9C/Toyama et al. - 2023 - Automatic Piano Transcription with Hierarchical Fr.pdf:application/pdf;Toyama et al. - 2023 - Automatic Piano Transcription with Hierarchical Fr.pdf:/Users/eleanorrow/Zotero/storage/QARYF2PP/Toyama et al. - 2023 - Automatic Piano Transcription with Hierarchical Fr.pdf:application/pdf},
}

@inproceedings{shaninAnnotatingJazzRecordings2023a,
	address = {New Paltz, NY, USA},
	title = {Annotating {Jazz} {Recordings} {Using} {Lead} {Sheet} {Alignment} with {Deep} {Chroma} {Features}},
	isbn = {9798350323726},
	url = {https://ieeexplore.ieee.org/document/10248107/},
	doi = {10.1109/WASPAA58266.2023.10248107},
	abstract = {The automatic recognition of chords from jazz recordings remains largely an unsolved challenge, due to the broad harmonic vocabulary, the freedom of interpretation in performance, the rich variety of expressive techniques, and the limited availability of accurately labeled training data. In practice, many jazz recordings contain performances of popular compositions that are known as standards. We propose an approach that takes into consideration available prior information about chord changes of popular compositions known as lead sheets. Instead of estimating the exact chord symbol at each time point, we aim to identify the position in the lead sheet, thereby solving the audio-to-score alignment task (with a distinction that the score does not contain any melodic information, and the harmonic annotation is only approximate). This approach also solves the structural segmentation problem, as segment boundaries are available in the lead sheets. To achieve this goal we combine a multi-task convolutional recurrent neural architecture with an alignment algorithm based on a Hidden Markov Model. The proposed approach uses the iRealPro corpus of 1186 lead sheets and is evaluated on a test set of the 220 audio excerpts in the Weimar Jazz Database, showing that it outperforms previously published work.},
	language = {en},
	urldate = {2024-01-02},
	booktitle = {2023 {IEEE} {Workshop} on {Applications} of {Signal} {Processing} to {Audio} and {Acoustics} ({WASPAA})},
	publisher = {IEEE},
	author = {Shanin, Ivan and Dixon, Simon},
	month = oct,
	year = {2023},
	pages = {1--5},
	file = {Shanin and Dixon - 2023 - Annotating Jazz Recordings Using Lead Sheet Alignm.pdf:/Users/eleanorrow/Zotero/storage/AP4JSNUT/Shanin and Dixon - 2023 - Annotating Jazz Recordings Using Lead Sheet Alignm.pdf:application/pdf},
}

@misc{gardnerLLarkMultimodalFoundation2023,
	title = {{LLark}: {A} {Multimodal} {Foundation} {Model} for {Music}},
	shorttitle = {{LLark}},
	url = {http://arxiv.org/abs/2310.07160},
	abstract = {Music has a unique and complex structure which is challenging for both expert humans and existing AI systems to understand, and presents unique challenges relative to other forms of audio. We present LLARK, an instruction-tuned multimodal model for music understanding. We detail our process for dataset creation, which involves augmenting the annotations of diverse open-source music datasets and converting them to a unified instruction-tuning format. We propose a multimodal architecture for LLARK, integrating a pretrained generative model for music with a pretrained language model. In evaluations on three types of tasks (music understanding, captioning, and reasoning), we show that our model matches or outperforms existing baselines in zero-shot generalization for music understanding, and that humans show a high degree of agreement with the model’s responses in captioning and reasoning tasks. LLARK is trained entirely from open-source music data and models, and we make our training code available along with the release of this paper. Additional results and audio examples are at https://bit.ly/llark, and our source code is available at https: //github.com/spotify-research/llark .},
	language = {en},
	urldate = {2024-01-02},
	publisher = {arXiv},
	author = {Gardner, Josh and Durand, Simon and Stoller, Daniel and Bittner, Rachel M.},
	month = oct,
	year = {2023},
	note = {arXiv:2310.07160 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Gardner et al. - 2023 - LLark A Multimodal Foundation Model for Music.pdf:/Users/eleanorrow/Zotero/storage/BAR2IXRP/Gardner et al. - 2023 - LLark A Multimodal Foundation Model for Music.pdf:application/pdf},
}

@misc{keshavHowReadPaperN/A,
	title = {How to {Read} a {Paper}},
	url = {https://web.stanford.edu/class/ee384m/Handouts/HowtoReadPaper.pdf},
	urldate = {2024-01-02},
	author = {Keshav, S},
	file = {HowtoReadPaper.pdf:/Users/eleanorrow/Zotero/storage/2BH2JLGI/HowtoReadPaper.pdf:application/pdf},
}

@misc{plachourasMir_refRepresentationEvaluation2023,
	title = {mir\_ref: {A} {Representation} {Evaluation} {Framework} for {Music} {Information} {Retrieval} {Tasks}},
	shorttitle = {mir\_ref},
	url = {https://arxiv.org/abs/2312.05994v2},
	abstract = {Music Information Retrieval (MIR) research is increasingly leveraging representation learning to obtain more compact, powerful music audio representations for various downstream MIR tasks. However, current representation evaluation methods are fragmented due to discrepancies in audio and label preprocessing, downstream model and metric implementations, data availability, and computational resources, often leading to inconsistent and limited results. In this work, we introduce mir\_ref, an MIR Representation Evaluation Framework focused on seamless, transparent, local-first experiment orchestration to support representation development. It features implementations of a variety of components such as MIR datasets, tasks, embedding models, and tools for result analysis and visualization, while facilitating the implementation of custom components. To demonstrate its utility, we use it to conduct an extensive evaluation of several embedding models across various tasks and datasets, including evaluating their robustness to various audio perturbations and the ease of extracting relevant information from them.},
	language = {en},
	urldate = {2024-01-02},
	author = {Plachouras, Christos and Alonso-Jiménez, Pablo and Bogdanov, Dmitry},
	month = dec,
	year = {2023},
	file = {Full Text PDF:/Users/eleanorrow/Zotero/storage/ZGDRGFBM/Plachouras et al. - 2023 - mir_ref A Representation Evaluation Framework for.pdf:application/pdf},
}

@misc{millerAttentionOne,
	title = {Attention {Is} {Off} {By} {One}},
	url = {https://www.evanmiller.org/attention-is-off-by-one.html},
	abstract = {Let’s fix these pesky Transformer outliers using Softmax One and QuietAttention.},
	language = {en},
	urldate = {2024-01-02},
	author = {Miller, Evan},
}

@misc{multiplatform.aiAnticipatoryMusicTransformer2023,
	title = {The {Anticipatory} {Music} {Transformer}: {Empowering} {Composers} with {Creative} {Control}},
	shorttitle = {The {Anticipatory} {Music} {Transformer}},
	url = {https://medium.com/@multiplatform.ai/the-anticipatory-music-transformer-empowering-composers-with-creative-control-c4a5dba4f195},
	abstract = {- The Anticipatory Music Transformer is a groundbreaking AI tool developed by Stanford researchers.

- It offers composers greater control in symbolic music composition, allowing them to initiate and…},
	language = {en},
	urldate = {2024-01-02},
	journal = {Medium},
	author = {Multiplatform.AI},
	month = dec,
	year = {2023},
}

@misc{TonalHarmonyVs,
	title = {Tonal {Harmony} vs {Modal} {Harmony}},
	url = {https://www.thejazzpianosite.com/jazz-piano-lessons/modern-jazz-theory/tonal-harmony-vs-modal-harmony/},
	abstract = {Jazz pre-1950's was almost exclusively Tonal, but Modern Jazz is often Modal. This lesson covers the difference between Tonal Harmony vs Modal Harmony.},
	language = {en-US},
	urldate = {2024-01-02},
	journal = {TJPS},
}
